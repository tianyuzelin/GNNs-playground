{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53f32db5-33c5-4bfa-8e8f-87c27a93369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model is currently not optimized for GPU!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44582f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import argparse\n",
    "import scipy.sparse as sp\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75c034c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(Module):\n",
    "    # GCN Layer\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else: \n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8910a2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f621b0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "def normalize(mx):\n",
    "    # Row-normalize sparse matrix\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    # Convert a scipy sparse matrix to a torch sparse tensor\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1895fe4a-5031-49ee-8215-b667cc98a41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Dataset Attribute (Commented)\n",
    "cora_path = \"./dataset/cora/\"\n",
    "def test_dataset(path=cora_path, dataset=\"cora\"):\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset), dtype=np.dtype(str))\n",
    "    print(type(idx_features_labels)); print(idx_features_labels.shape); \n",
    "    # print(idx_features_labels)\n",
    "    \n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    print(type(edges_unordered)); print(edges_unordered.shape); \n",
    "    # print(edges_unordered)\n",
    "    # print(edges)\n",
    "    return None\n",
    "# test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4899fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path=cora_path, dataset=\"cora\"):\n",
    "    # Load citation network dataset (cora only for now)\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset), dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset), dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "    \n",
    "    print(\"Dataset Loaded!\")\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8156b5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n",
      "Dataset Loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62c2691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "epochs = 200\n",
    "weightDecay = 5e-4\n",
    "learningRate = 0.01\n",
    "hidden = 16\n",
    "dropout = 0.5\n",
    "\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learningRate, weight_decay=weightDecay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16c9b592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate validation set performance separately,\n",
    "    # deactivates dropout during validation run.\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "256a6311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "008610b6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9283 acc_train: 0.1214 loss_val: 1.9083 acc_val: 0.3267 time: 0.0539s\n",
      "Epoch: 0002 loss_train: 1.9178 acc_train: 0.2286 loss_val: 1.8943 acc_val: 0.3500 time: 0.0100s\n",
      "Epoch: 0003 loss_train: 1.9113 acc_train: 0.2714 loss_val: 1.8817 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0004 loss_train: 1.8866 acc_train: 0.3286 loss_val: 1.8697 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0005 loss_train: 1.8807 acc_train: 0.2857 loss_val: 1.8583 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0006 loss_train: 1.8679 acc_train: 0.2786 loss_val: 1.8474 acc_val: 0.3500 time: 0.0100s\n",
      "Epoch: 0007 loss_train: 1.8590 acc_train: 0.2929 loss_val: 1.8368 acc_val: 0.3500 time: 0.0090s\n",
      "Epoch: 0008 loss_train: 1.8497 acc_train: 0.2929 loss_val: 1.8266 acc_val: 0.3500 time: 0.0100s\n",
      "Epoch: 0009 loss_train: 1.8234 acc_train: 0.2929 loss_val: 1.8168 acc_val: 0.3500 time: 0.0090s\n",
      "Epoch: 0010 loss_train: 1.8284 acc_train: 0.2929 loss_val: 1.8076 acc_val: 0.3500 time: 0.0090s\n",
      "Epoch: 0011 loss_train: 1.8254 acc_train: 0.2929 loss_val: 1.7990 acc_val: 0.3500 time: 0.0090s\n",
      "Epoch: 0012 loss_train: 1.8010 acc_train: 0.3000 loss_val: 1.7906 acc_val: 0.3500 time: 0.0090s\n",
      "Epoch: 0013 loss_train: 1.7896 acc_train: 0.2929 loss_val: 1.7827 acc_val: 0.3500 time: 0.0100s\n",
      "Epoch: 0014 loss_train: 1.7722 acc_train: 0.2929 loss_val: 1.7753 acc_val: 0.3500 time: 0.0100s\n",
      "Epoch: 0015 loss_train: 1.7675 acc_train: 0.2929 loss_val: 1.7682 acc_val: 0.3500 time: 0.0090s\n",
      "Epoch: 0016 loss_train: 1.7543 acc_train: 0.2929 loss_val: 1.7614 acc_val: 0.3500 time: 0.0090s\n",
      "Epoch: 0017 loss_train: 1.7531 acc_train: 0.2929 loss_val: 1.7548 acc_val: 0.3500 time: 0.0100s\n",
      "Epoch: 0018 loss_train: 1.7562 acc_train: 0.2929 loss_val: 1.7485 acc_val: 0.3500 time: 0.0100s\n",
      "Epoch: 0019 loss_train: 1.7647 acc_train: 0.3000 loss_val: 1.7424 acc_val: 0.3500 time: 0.0100s\n",
      "Epoch: 0020 loss_train: 1.7501 acc_train: 0.3000 loss_val: 1.7366 acc_val: 0.3500 time: 0.0089s\n",
      "Epoch: 0021 loss_train: 1.7369 acc_train: 0.2929 loss_val: 1.7310 acc_val: 0.3500 time: 0.0120s\n",
      "Epoch: 0022 loss_train: 1.7188 acc_train: 0.3071 loss_val: 1.7253 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0023 loss_train: 1.7283 acc_train: 0.3000 loss_val: 1.7197 acc_val: 0.3500 time: 0.0090s\n",
      "Epoch: 0024 loss_train: 1.7159 acc_train: 0.3071 loss_val: 1.7139 acc_val: 0.3500 time: 0.0080s\n",
      "Epoch: 0025 loss_train: 1.7045 acc_train: 0.3000 loss_val: 1.7082 acc_val: 0.3500 time: 0.0100s\n",
      "Epoch: 0026 loss_train: 1.6904 acc_train: 0.3214 loss_val: 1.7021 acc_val: 0.3500 time: 0.0100s\n",
      "Epoch: 0027 loss_train: 1.6905 acc_train: 0.3143 loss_val: 1.6959 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0028 loss_train: 1.6888 acc_train: 0.2857 loss_val: 1.6894 acc_val: 0.3500 time: 0.0100s\n",
      "Epoch: 0029 loss_train: 1.6745 acc_train: 0.3500 loss_val: 1.6828 acc_val: 0.3500 time: 0.0090s\n",
      "Epoch: 0030 loss_train: 1.6328 acc_train: 0.3357 loss_val: 1.6759 acc_val: 0.3600 time: 0.0100s\n",
      "Epoch: 0031 loss_train: 1.6332 acc_train: 0.3714 loss_val: 1.6687 acc_val: 0.3633 time: 0.0100s\n",
      "Epoch: 0032 loss_train: 1.6438 acc_train: 0.3643 loss_val: 1.6608 acc_val: 0.3633 time: 0.0110s\n",
      "Epoch: 0033 loss_train: 1.6175 acc_train: 0.3857 loss_val: 1.6526 acc_val: 0.3600 time: 0.0100s\n",
      "Epoch: 0034 loss_train: 1.6124 acc_train: 0.3714 loss_val: 1.6441 acc_val: 0.3633 time: 0.0090s\n",
      "Epoch: 0035 loss_train: 1.5862 acc_train: 0.4143 loss_val: 1.6353 acc_val: 0.3733 time: 0.0090s\n",
      "Epoch: 0036 loss_train: 1.5820 acc_train: 0.4357 loss_val: 1.6262 acc_val: 0.3800 time: 0.0090s\n",
      "Epoch: 0037 loss_train: 1.5491 acc_train: 0.4571 loss_val: 1.6165 acc_val: 0.4000 time: 0.0100s\n",
      "Epoch: 0038 loss_train: 1.5533 acc_train: 0.4429 loss_val: 1.6064 acc_val: 0.4067 time: 0.0100s\n",
      "Epoch: 0039 loss_train: 1.5335 acc_train: 0.4286 loss_val: 1.5960 acc_val: 0.4167 time: 0.0100s\n",
      "Epoch: 0040 loss_train: 1.5236 acc_train: 0.4929 loss_val: 1.5850 acc_val: 0.4500 time: 0.0089s\n",
      "Epoch: 0041 loss_train: 1.4955 acc_train: 0.4786 loss_val: 1.5738 acc_val: 0.4600 time: 0.0110s\n",
      "Epoch: 0042 loss_train: 1.4878 acc_train: 0.5214 loss_val: 1.5621 acc_val: 0.4767 time: 0.0120s\n",
      "Epoch: 0043 loss_train: 1.4838 acc_train: 0.5071 loss_val: 1.5503 acc_val: 0.4833 time: 0.0090s\n",
      "Epoch: 0044 loss_train: 1.4609 acc_train: 0.5429 loss_val: 1.5382 acc_val: 0.5000 time: 0.0100s\n",
      "Epoch: 0045 loss_train: 1.4545 acc_train: 0.5500 loss_val: 1.5260 acc_val: 0.5133 time: 0.0100s\n",
      "Epoch: 0046 loss_train: 1.4105 acc_train: 0.5714 loss_val: 1.5139 acc_val: 0.5200 time: 0.0090s\n",
      "Epoch: 0047 loss_train: 1.4247 acc_train: 0.5357 loss_val: 1.5013 acc_val: 0.5300 time: 0.0100s\n",
      "Epoch: 0048 loss_train: 1.3959 acc_train: 0.5571 loss_val: 1.4886 acc_val: 0.5600 time: 0.0100s\n",
      "Epoch: 0049 loss_train: 1.3585 acc_train: 0.5929 loss_val: 1.4756 acc_val: 0.5667 time: 0.0090s\n",
      "Epoch: 0050 loss_train: 1.3455 acc_train: 0.6571 loss_val: 1.4621 acc_val: 0.5767 time: 0.0090s\n",
      "Epoch: 0051 loss_train: 1.3407 acc_train: 0.6214 loss_val: 1.4487 acc_val: 0.5767 time: 0.0090s\n",
      "Epoch: 0052 loss_train: 1.3611 acc_train: 0.5786 loss_val: 1.4351 acc_val: 0.5800 time: 0.0100s\n",
      "Epoch: 0053 loss_train: 1.3376 acc_train: 0.6143 loss_val: 1.4213 acc_val: 0.5933 time: 0.0090s\n",
      "Epoch: 0054 loss_train: 1.3330 acc_train: 0.6071 loss_val: 1.4073 acc_val: 0.5967 time: 0.0100s\n",
      "Epoch: 0055 loss_train: 1.2547 acc_train: 0.6429 loss_val: 1.3933 acc_val: 0.6067 time: 0.0100s\n",
      "Epoch: 0056 loss_train: 1.2676 acc_train: 0.6571 loss_val: 1.3794 acc_val: 0.6100 time: 0.0100s\n",
      "Epoch: 0057 loss_train: 1.2562 acc_train: 0.6714 loss_val: 1.3658 acc_val: 0.6100 time: 0.0100s\n",
      "Epoch: 0058 loss_train: 1.2192 acc_train: 0.6500 loss_val: 1.3524 acc_val: 0.6133 time: 0.0090s\n",
      "Epoch: 0059 loss_train: 1.2138 acc_train: 0.6786 loss_val: 1.3395 acc_val: 0.6367 time: 0.0100s\n",
      "Epoch: 0060 loss_train: 1.2169 acc_train: 0.6429 loss_val: 1.3268 acc_val: 0.6567 time: 0.0100s\n",
      "Epoch: 0061 loss_train: 1.1979 acc_train: 0.6857 loss_val: 1.3142 acc_val: 0.6667 time: 0.0100s\n",
      "Epoch: 0062 loss_train: 1.1639 acc_train: 0.6714 loss_val: 1.3015 acc_val: 0.6767 time: 0.0110s\n",
      "Epoch: 0063 loss_train: 1.1755 acc_train: 0.6857 loss_val: 1.2886 acc_val: 0.6833 time: 0.0110s\n",
      "Epoch: 0064 loss_train: 1.1512 acc_train: 0.7071 loss_val: 1.2759 acc_val: 0.6900 time: 0.0110s\n",
      "Epoch: 0065 loss_train: 1.1418 acc_train: 0.6786 loss_val: 1.2633 acc_val: 0.6933 time: 0.0120s\n",
      "Epoch: 0066 loss_train: 1.1470 acc_train: 0.7143 loss_val: 1.2509 acc_val: 0.6967 time: 0.0110s\n",
      "Epoch: 0067 loss_train: 1.0848 acc_train: 0.7357 loss_val: 1.2385 acc_val: 0.7000 time: 0.0100s\n",
      "Epoch: 0068 loss_train: 1.0402 acc_train: 0.7357 loss_val: 1.2270 acc_val: 0.7033 time: 0.0090s\n",
      "Epoch: 0069 loss_train: 1.0862 acc_train: 0.6929 loss_val: 1.2155 acc_val: 0.7033 time: 0.0100s\n",
      "Epoch: 0070 loss_train: 1.0475 acc_train: 0.7429 loss_val: 1.2039 acc_val: 0.7133 time: 0.0100s\n",
      "Epoch: 0071 loss_train: 1.0295 acc_train: 0.7786 loss_val: 1.1924 acc_val: 0.7133 time: 0.0100s\n",
      "Epoch: 0072 loss_train: 1.0162 acc_train: 0.7286 loss_val: 1.1816 acc_val: 0.7200 time: 0.0110s\n",
      "Epoch: 0073 loss_train: 0.9875 acc_train: 0.7643 loss_val: 1.1709 acc_val: 0.7267 time: 0.0100s\n",
      "Epoch: 0074 loss_train: 1.0467 acc_train: 0.7857 loss_val: 1.1601 acc_val: 0.7400 time: 0.0100s\n",
      "Epoch: 0075 loss_train: 1.0012 acc_train: 0.7714 loss_val: 1.1496 acc_val: 0.7433 time: 0.0110s\n",
      "Epoch: 0076 loss_train: 0.9720 acc_train: 0.8000 loss_val: 1.1393 acc_val: 0.7433 time: 0.0100s\n",
      "Epoch: 0077 loss_train: 0.9584 acc_train: 0.8000 loss_val: 1.1293 acc_val: 0.7433 time: 0.0090s\n",
      "Epoch: 0078 loss_train: 0.9197 acc_train: 0.8071 loss_val: 1.1198 acc_val: 0.7533 time: 0.0100s\n",
      "Epoch: 0079 loss_train: 0.9521 acc_train: 0.8000 loss_val: 1.1104 acc_val: 0.7567 time: 0.0100s\n",
      "Epoch: 0080 loss_train: 0.9568 acc_train: 0.7714 loss_val: 1.1012 acc_val: 0.7600 time: 0.0110s\n",
      "Epoch: 0081 loss_train: 0.8807 acc_train: 0.8071 loss_val: 1.0924 acc_val: 0.7767 time: 0.0090s\n",
      "Epoch: 0082 loss_train: 0.8688 acc_train: 0.8429 loss_val: 1.0830 acc_val: 0.7867 time: 0.0100s\n",
      "Epoch: 0083 loss_train: 0.8924 acc_train: 0.8071 loss_val: 1.0743 acc_val: 0.7933 time: 0.0100s\n",
      "Epoch: 0084 loss_train: 0.8975 acc_train: 0.8214 loss_val: 1.0661 acc_val: 0.7900 time: 0.0100s\n",
      "Epoch: 0085 loss_train: 0.8707 acc_train: 0.7929 loss_val: 1.0580 acc_val: 0.7933 time: 0.0100s\n",
      "Epoch: 0086 loss_train: 0.8644 acc_train: 0.8286 loss_val: 1.0503 acc_val: 0.7900 time: 0.0090s\n",
      "Epoch: 0087 loss_train: 0.8495 acc_train: 0.8357 loss_val: 1.0422 acc_val: 0.7933 time: 0.0120s\n",
      "Epoch: 0088 loss_train: 0.8336 acc_train: 0.8357 loss_val: 1.0338 acc_val: 0.7967 time: 0.0100s\n",
      "Epoch: 0089 loss_train: 0.8586 acc_train: 0.7786 loss_val: 1.0249 acc_val: 0.7967 time: 0.0090s\n",
      "Epoch: 0090 loss_train: 0.7955 acc_train: 0.8500 loss_val: 1.0165 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0091 loss_train: 0.8713 acc_train: 0.8286 loss_val: 1.0078 acc_val: 0.7967 time: 0.0090s\n",
      "Epoch: 0092 loss_train: 0.8161 acc_train: 0.8357 loss_val: 0.9996 acc_val: 0.7967 time: 0.0090s\n",
      "Epoch: 0093 loss_train: 0.8138 acc_train: 0.8714 loss_val: 0.9917 acc_val: 0.7967 time: 0.0090s\n",
      "Epoch: 0094 loss_train: 0.7822 acc_train: 0.8357 loss_val: 0.9848 acc_val: 0.7933 time: 0.0100s\n",
      "Epoch: 0095 loss_train: 0.8063 acc_train: 0.8214 loss_val: 0.9785 acc_val: 0.7933 time: 0.0130s\n",
      "Epoch: 0096 loss_train: 0.7894 acc_train: 0.8500 loss_val: 0.9723 acc_val: 0.7900 time: 0.0120s\n",
      "Epoch: 0097 loss_train: 0.7834 acc_train: 0.8643 loss_val: 0.9663 acc_val: 0.7967 time: 0.0100s\n",
      "Epoch: 0098 loss_train: 0.7821 acc_train: 0.8214 loss_val: 0.9610 acc_val: 0.8000 time: 0.0120s\n",
      "Epoch: 0099 loss_train: 0.7638 acc_train: 0.8429 loss_val: 0.9558 acc_val: 0.8033 time: 0.0120s\n",
      "Epoch: 0100 loss_train: 0.7452 acc_train: 0.8500 loss_val: 0.9509 acc_val: 0.8067 time: 0.0120s\n",
      "Epoch: 0101 loss_train: 0.7165 acc_train: 0.8643 loss_val: 0.9460 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0102 loss_train: 0.7605 acc_train: 0.8500 loss_val: 0.9403 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0103 loss_train: 0.7305 acc_train: 0.8643 loss_val: 0.9344 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0104 loss_train: 0.7029 acc_train: 0.8571 loss_val: 0.9294 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0105 loss_train: 0.7449 acc_train: 0.8714 loss_val: 0.9249 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0106 loss_train: 0.7349 acc_train: 0.8500 loss_val: 0.9205 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0107 loss_train: 0.7025 acc_train: 0.8357 loss_val: 0.9157 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0108 loss_train: 0.7142 acc_train: 0.8357 loss_val: 0.9105 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0109 loss_train: 0.7027 acc_train: 0.8286 loss_val: 0.9050 acc_val: 0.8000 time: 0.0110s\n",
      "Epoch: 0110 loss_train: 0.6756 acc_train: 0.8571 loss_val: 0.8988 acc_val: 0.8100 time: 0.0100s\n",
      "Epoch: 0111 loss_train: 0.6789 acc_train: 0.8714 loss_val: 0.8930 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0112 loss_train: 0.7063 acc_train: 0.8714 loss_val: 0.8877 acc_val: 0.8067 time: 0.0090s\n",
      "Epoch: 0113 loss_train: 0.6641 acc_train: 0.8357 loss_val: 0.8827 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0114 loss_train: 0.6818 acc_train: 0.8643 loss_val: 0.8779 acc_val: 0.8133 time: 0.0090s\n",
      "Epoch: 0115 loss_train: 0.6195 acc_train: 0.8786 loss_val: 0.8739 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0116 loss_train: 0.6979 acc_train: 0.8643 loss_val: 0.8706 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0117 loss_train: 0.6489 acc_train: 0.9071 loss_val: 0.8672 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0118 loss_train: 0.6616 acc_train: 0.9071 loss_val: 0.8638 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0119 loss_train: 0.6453 acc_train: 0.8643 loss_val: 0.8600 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0120 loss_train: 0.6046 acc_train: 0.8571 loss_val: 0.8559 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0121 loss_train: 0.6379 acc_train: 0.8714 loss_val: 0.8517 acc_val: 0.8033 time: 0.0090s\n",
      "Epoch: 0122 loss_train: 0.5948 acc_train: 0.9071 loss_val: 0.8484 acc_val: 0.7967 time: 0.0090s\n",
      "Epoch: 0123 loss_train: 0.6274 acc_train: 0.8714 loss_val: 0.8455 acc_val: 0.7967 time: 0.0100s\n",
      "Epoch: 0124 loss_train: 0.5966 acc_train: 0.8929 loss_val: 0.8420 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0125 loss_train: 0.6045 acc_train: 0.8929 loss_val: 0.8377 acc_val: 0.7967 time: 0.0090s\n",
      "Epoch: 0126 loss_train: 0.6091 acc_train: 0.8643 loss_val: 0.8332 acc_val: 0.7967 time: 0.0090s\n",
      "Epoch: 0127 loss_train: 0.6290 acc_train: 0.8643 loss_val: 0.8288 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0128 loss_train: 0.5546 acc_train: 0.9214 loss_val: 0.8242 acc_val: 0.8000 time: 0.0090s\n",
      "Epoch: 0129 loss_train: 0.6308 acc_train: 0.8857 loss_val: 0.8210 acc_val: 0.7967 time: 0.0090s\n",
      "Epoch: 0130 loss_train: 0.6119 acc_train: 0.8714 loss_val: 0.8190 acc_val: 0.8000 time: 0.0090s\n",
      "Epoch: 0131 loss_train: 0.5516 acc_train: 0.9000 loss_val: 0.8170 acc_val: 0.7967 time: 0.0090s\n",
      "Epoch: 0132 loss_train: 0.6003 acc_train: 0.8643 loss_val: 0.8155 acc_val: 0.8033 time: 0.0090s\n",
      "Epoch: 0133 loss_train: 0.6182 acc_train: 0.8500 loss_val: 0.8141 acc_val: 0.8033 time: 0.0090s\n",
      "Epoch: 0134 loss_train: 0.5626 acc_train: 0.8857 loss_val: 0.8116 acc_val: 0.8033 time: 0.0090s\n",
      "Epoch: 0135 loss_train: 0.5653 acc_train: 0.9071 loss_val: 0.8070 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0136 loss_train: 0.5621 acc_train: 0.9214 loss_val: 0.8019 acc_val: 0.8100 time: 0.0100s\n",
      "Epoch: 0137 loss_train: 0.5641 acc_train: 0.9000 loss_val: 0.7968 acc_val: 0.8167 time: 0.0110s\n",
      "Epoch: 0138 loss_train: 0.5360 acc_train: 0.9143 loss_val: 0.7930 acc_val: 0.8167 time: 0.0100s\n",
      "Epoch: 0139 loss_train: 0.5883 acc_train: 0.8929 loss_val: 0.7899 acc_val: 0.8133 time: 0.0100s\n",
      "Epoch: 0140 loss_train: 0.5571 acc_train: 0.9071 loss_val: 0.7879 acc_val: 0.8133 time: 0.0100s\n",
      "Epoch: 0141 loss_train: 0.5716 acc_train: 0.9143 loss_val: 0.7869 acc_val: 0.8133 time: 0.0100s\n",
      "Epoch: 0142 loss_train: 0.5538 acc_train: 0.8857 loss_val: 0.7875 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0143 loss_train: 0.5616 acc_train: 0.8714 loss_val: 0.7886 acc_val: 0.7933 time: 0.0100s\n",
      "Epoch: 0144 loss_train: 0.5793 acc_train: 0.8714 loss_val: 0.7877 acc_val: 0.7933 time: 0.0100s\n",
      "Epoch: 0145 loss_train: 0.5300 acc_train: 0.9214 loss_val: 0.7863 acc_val: 0.8000 time: 0.0090s\n",
      "Epoch: 0146 loss_train: 0.5110 acc_train: 0.8929 loss_val: 0.7838 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0147 loss_train: 0.5334 acc_train: 0.9071 loss_val: 0.7795 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0148 loss_train: 0.5217 acc_train: 0.9286 loss_val: 0.7731 acc_val: 0.8100 time: 0.0110s\n",
      "Epoch: 0149 loss_train: 0.4967 acc_train: 0.9000 loss_val: 0.7674 acc_val: 0.8200 time: 0.0110s\n",
      "Epoch: 0150 loss_train: 0.5155 acc_train: 0.8929 loss_val: 0.7628 acc_val: 0.8200 time: 0.0110s\n",
      "Epoch: 0151 loss_train: 0.4911 acc_train: 0.9214 loss_val: 0.7595 acc_val: 0.8167 time: 0.0099s\n",
      "Epoch: 0152 loss_train: 0.4452 acc_train: 0.9357 loss_val: 0.7564 acc_val: 0.8167 time: 0.0100s\n",
      "Epoch: 0153 loss_train: 0.5314 acc_train: 0.9000 loss_val: 0.7537 acc_val: 0.8167 time: 0.0090s\n",
      "Epoch: 0154 loss_train: 0.5010 acc_train: 0.9357 loss_val: 0.7516 acc_val: 0.8167 time: 0.0100s\n",
      "Epoch: 0155 loss_train: 0.4765 acc_train: 0.9286 loss_val: 0.7503 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0156 loss_train: 0.4946 acc_train: 0.9214 loss_val: 0.7488 acc_val: 0.8067 time: 0.0120s\n",
      "Epoch: 0157 loss_train: 0.5360 acc_train: 0.9143 loss_val: 0.7483 acc_val: 0.8100 time: 0.0110s\n",
      "Epoch: 0158 loss_train: 0.4962 acc_train: 0.9071 loss_val: 0.7486 acc_val: 0.8100 time: 0.0109s\n",
      "Epoch: 0159 loss_train: 0.4487 acc_train: 0.9214 loss_val: 0.7484 acc_val: 0.8100 time: 0.0100s\n",
      "Epoch: 0160 loss_train: 0.5050 acc_train: 0.9143 loss_val: 0.7477 acc_val: 0.8100 time: 0.0090s\n",
      "Epoch: 0161 loss_train: 0.4859 acc_train: 0.9357 loss_val: 0.7472 acc_val: 0.8100 time: 0.0090s\n",
      "Epoch: 0162 loss_train: 0.4859 acc_train: 0.9214 loss_val: 0.7460 acc_val: 0.8067 time: 0.0090s\n",
      "Epoch: 0163 loss_train: 0.4883 acc_train: 0.9286 loss_val: 0.7434 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0164 loss_train: 0.4767 acc_train: 0.9143 loss_val: 0.7403 acc_val: 0.8067 time: 0.0090s\n",
      "Epoch: 0165 loss_train: 0.4912 acc_train: 0.9071 loss_val: 0.7388 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0166 loss_train: 0.4791 acc_train: 0.9214 loss_val: 0.7372 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0167 loss_train: 0.4976 acc_train: 0.8929 loss_val: 0.7351 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0168 loss_train: 0.4812 acc_train: 0.8929 loss_val: 0.7335 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0169 loss_train: 0.4350 acc_train: 0.9500 loss_val: 0.7321 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0170 loss_train: 0.4551 acc_train: 0.9571 loss_val: 0.7302 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0171 loss_train: 0.4317 acc_train: 0.9214 loss_val: 0.7290 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0172 loss_train: 0.4372 acc_train: 0.9214 loss_val: 0.7276 acc_val: 0.8100 time: 0.0090s\n",
      "Epoch: 0173 loss_train: 0.4777 acc_train: 0.9357 loss_val: 0.7260 acc_val: 0.8100 time: 0.0100s\n",
      "Epoch: 0174 loss_train: 0.4573 acc_train: 0.9571 loss_val: 0.7245 acc_val: 0.8100 time: 0.0100s\n",
      "Epoch: 0175 loss_train: 0.4482 acc_train: 0.9000 loss_val: 0.7222 acc_val: 0.8133 time: 0.0110s\n",
      "Epoch: 0176 loss_train: 0.4736 acc_train: 0.8929 loss_val: 0.7209 acc_val: 0.8100 time: 0.0120s\n",
      "Epoch: 0177 loss_train: 0.4484 acc_train: 0.9000 loss_val: 0.7188 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0178 loss_train: 0.4793 acc_train: 0.9071 loss_val: 0.7159 acc_val: 0.8100 time: 0.0090s\n",
      "Epoch: 0179 loss_train: 0.4219 acc_train: 0.9286 loss_val: 0.7136 acc_val: 0.8067 time: 0.0090s\n",
      "Epoch: 0180 loss_train: 0.4498 acc_train: 0.9071 loss_val: 0.7107 acc_val: 0.8100 time: 0.0110s\n",
      "Epoch: 0181 loss_train: 0.4260 acc_train: 0.9571 loss_val: 0.7077 acc_val: 0.8133 time: 0.0100s\n",
      "Epoch: 0182 loss_train: 0.4377 acc_train: 0.9143 loss_val: 0.7046 acc_val: 0.8133 time: 0.0090s\n",
      "Epoch: 0183 loss_train: 0.4628 acc_train: 0.9500 loss_val: 0.7026 acc_val: 0.8233 time: 0.0110s\n",
      "Epoch: 0184 loss_train: 0.4588 acc_train: 0.9357 loss_val: 0.7013 acc_val: 0.8167 time: 0.0090s\n",
      "Epoch: 0185 loss_train: 0.4293 acc_train: 0.9357 loss_val: 0.6998 acc_val: 0.8233 time: 0.0090s\n",
      "Epoch: 0186 loss_train: 0.3616 acc_train: 0.9500 loss_val: 0.6991 acc_val: 0.8233 time: 0.0090s\n",
      "Epoch: 0187 loss_train: 0.4312 acc_train: 0.9357 loss_val: 0.6998 acc_val: 0.8133 time: 0.0100s\n",
      "Epoch: 0188 loss_train: 0.4202 acc_train: 0.9643 loss_val: 0.7017 acc_val: 0.8133 time: 0.0100s\n",
      "Epoch: 0189 loss_train: 0.3969 acc_train: 0.9286 loss_val: 0.7026 acc_val: 0.8100 time: 0.0090s\n",
      "Epoch: 0190 loss_train: 0.4190 acc_train: 0.9286 loss_val: 0.7024 acc_val: 0.8100 time: 0.0100s\n",
      "Epoch: 0191 loss_train: 0.4082 acc_train: 0.9500 loss_val: 0.6999 acc_val: 0.8133 time: 0.0100s\n",
      "Epoch: 0192 loss_train: 0.4321 acc_train: 0.9214 loss_val: 0.6971 acc_val: 0.8167 time: 0.0080s\n",
      "Epoch: 0193 loss_train: 0.4122 acc_train: 0.9500 loss_val: 0.6925 acc_val: 0.8133 time: 0.0090s\n",
      "Epoch: 0194 loss_train: 0.4119 acc_train: 0.9429 loss_val: 0.6894 acc_val: 0.8167 time: 0.0090s\n",
      "Epoch: 0195 loss_train: 0.4476 acc_train: 0.9286 loss_val: 0.6877 acc_val: 0.8133 time: 0.0110s\n",
      "Epoch: 0196 loss_train: 0.3816 acc_train: 0.9571 loss_val: 0.6879 acc_val: 0.8100 time: 0.0110s\n",
      "Epoch: 0197 loss_train: 0.3908 acc_train: 0.9429 loss_val: 0.6890 acc_val: 0.8033 time: 0.0090s\n",
      "Epoch: 0198 loss_train: 0.3731 acc_train: 0.9571 loss_val: 0.6904 acc_val: 0.8067 time: 0.0090s\n",
      "Epoch: 0199 loss_train: 0.4279 acc_train: 0.9643 loss_val: 0.6927 acc_val: 0.8100 time: 0.0090s\n",
      "Epoch: 0200 loss_train: 0.3948 acc_train: 0.9357 loss_val: 0.6930 acc_val: 0.8100 time: 0.0090s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 2.0645s\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98fc41e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.7138 accuracy= 0.8420\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfcca19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
