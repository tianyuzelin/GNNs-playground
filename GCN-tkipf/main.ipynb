{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53f32db5-33c5-4bfa-8e8f-87c27a93369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model is currently not optimized for GPU!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44582f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import argparse\n",
    "import scipy.sparse as sp\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75c034c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(Module):\n",
    "    # GCN Layer\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else: \n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8910a2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f621b0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "def normalize(mx):\n",
    "    # Row-normalize sparse matrix\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    # Convert a scipy sparse matrix to a torch sparse tensor\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1895fe4a-5031-49ee-8215-b667cc98a41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Dataset Attribute (Commented)\n",
    "cora_path = \"../data/cora/\"\n",
    "def test_dataset(path=cora_path, dataset=\"cora\"):\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset), dtype=np.dtype(str))\n",
    "    print(type(idx_features_labels)); print(idx_features_labels.shape); \n",
    "    # print(idx_features_labels)\n",
    "    \n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    print(type(edges_unordered)); print(edges_unordered.shape); \n",
    "    # print(edges_unordered)\n",
    "    # print(edges)\n",
    "    return None\n",
    "# test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4899fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path=cora_path, dataset=\"cora\"):\n",
    "    # Load citation network dataset (cora only for now)\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset), dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset), dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "    \n",
    "    print(\"Dataset Loaded!\")\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8156b5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n",
      "Dataset Loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62c2691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "epochs = 200\n",
    "weightDecay = 5e-4\n",
    "learningRate = 0.01\n",
    "hidden = 16\n",
    "dropout = 0.5\n",
    "\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learningRate, weight_decay=weightDecay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16c9b592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate validation set performance separately,\n",
    "    # deactivates dropout during validation run.\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "256a6311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "008610b6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9990 acc_train: 0.1500 loss_val: 2.0243 acc_val: 0.1267 time: 0.0575s\n",
      "Epoch: 0002 loss_train: 1.9942 acc_train: 0.1500 loss_val: 2.0023 acc_val: 0.1267 time: 0.0140s\n",
      "Epoch: 0003 loss_train: 1.9633 acc_train: 0.1571 loss_val: 1.9816 acc_val: 0.1267 time: 0.0120s\n",
      "Epoch: 0004 loss_train: 1.9425 acc_train: 0.1500 loss_val: 1.9620 acc_val: 0.1267 time: 0.0130s\n",
      "Epoch: 0005 loss_train: 1.9329 acc_train: 0.1571 loss_val: 1.9438 acc_val: 0.1700 time: 0.0090s\n",
      "Epoch: 0006 loss_train: 1.9151 acc_train: 0.2357 loss_val: 1.9266 acc_val: 0.1600 time: 0.0090s\n",
      "Epoch: 0007 loss_train: 1.8935 acc_train: 0.2714 loss_val: 1.9102 acc_val: 0.1567 time: 0.0110s\n",
      "Epoch: 0008 loss_train: 1.8905 acc_train: 0.2214 loss_val: 1.8943 acc_val: 0.1567 time: 0.0100s\n",
      "Epoch: 0009 loss_train: 1.8692 acc_train: 0.2286 loss_val: 1.8787 acc_val: 0.1567 time: 0.0100s\n",
      "Epoch: 0010 loss_train: 1.8486 acc_train: 0.2429 loss_val: 1.8634 acc_val: 0.1567 time: 0.0110s\n",
      "Epoch: 0011 loss_train: 1.8455 acc_train: 0.2500 loss_val: 1.8484 acc_val: 0.1567 time: 0.0110s\n",
      "Epoch: 0012 loss_train: 1.8165 acc_train: 0.2500 loss_val: 1.8337 acc_val: 0.1567 time: 0.0100s\n",
      "Epoch: 0013 loss_train: 1.8036 acc_train: 0.3357 loss_val: 1.8195 acc_val: 0.1933 time: 0.0090s\n",
      "Epoch: 0014 loss_train: 1.8025 acc_train: 0.3143 loss_val: 1.8059 acc_val: 0.3633 time: 0.0100s\n",
      "Epoch: 0015 loss_train: 1.7805 acc_train: 0.3500 loss_val: 1.7927 acc_val: 0.4567 time: 0.0100s\n",
      "Epoch: 0016 loss_train: 1.7719 acc_train: 0.3929 loss_val: 1.7800 acc_val: 0.4367 time: 0.0100s\n",
      "Epoch: 0017 loss_train: 1.7601 acc_train: 0.3929 loss_val: 1.7679 acc_val: 0.3800 time: 0.0110s\n",
      "Epoch: 0018 loss_train: 1.7351 acc_train: 0.3286 loss_val: 1.7562 acc_val: 0.3667 time: 0.0129s\n",
      "Epoch: 0019 loss_train: 1.7522 acc_train: 0.4071 loss_val: 1.7453 acc_val: 0.3533 time: 0.0110s\n",
      "Epoch: 0020 loss_train: 1.7158 acc_train: 0.3786 loss_val: 1.7350 acc_val: 0.3533 time: 0.0110s\n",
      "Epoch: 0021 loss_train: 1.7157 acc_train: 0.3429 loss_val: 1.7254 acc_val: 0.3500 time: 0.0120s\n",
      "Epoch: 0022 loss_train: 1.7244 acc_train: 0.3571 loss_val: 1.7165 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0023 loss_train: 1.7036 acc_train: 0.3357 loss_val: 1.7080 acc_val: 0.3500 time: 0.0119s\n",
      "Epoch: 0024 loss_train: 1.6801 acc_train: 0.3643 loss_val: 1.6999 acc_val: 0.3500 time: 0.0130s\n",
      "Epoch: 0025 loss_train: 1.7119 acc_train: 0.3143 loss_val: 1.6921 acc_val: 0.3500 time: 0.0099s\n",
      "Epoch: 0026 loss_train: 1.6818 acc_train: 0.3214 loss_val: 1.6845 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0027 loss_train: 1.6775 acc_train: 0.3357 loss_val: 1.6769 acc_val: 0.3500 time: 0.0100s\n",
      "Epoch: 0028 loss_train: 1.6469 acc_train: 0.3357 loss_val: 1.6694 acc_val: 0.3500 time: 0.0120s\n",
      "Epoch: 0029 loss_train: 1.6629 acc_train: 0.3143 loss_val: 1.6620 acc_val: 0.3533 time: 0.0120s\n",
      "Epoch: 0030 loss_train: 1.6479 acc_train: 0.3214 loss_val: 1.6546 acc_val: 0.3533 time: 0.0120s\n",
      "Epoch: 0031 loss_train: 1.6104 acc_train: 0.3500 loss_val: 1.6473 acc_val: 0.3633 time: 0.0110s\n",
      "Epoch: 0032 loss_train: 1.6261 acc_train: 0.3929 loss_val: 1.6400 acc_val: 0.3633 time: 0.0110s\n",
      "Epoch: 0033 loss_train: 1.5914 acc_train: 0.3571 loss_val: 1.6327 acc_val: 0.3667 time: 0.0100s\n",
      "Epoch: 0034 loss_train: 1.5975 acc_train: 0.3929 loss_val: 1.6252 acc_val: 0.3733 time: 0.0100s\n",
      "Epoch: 0035 loss_train: 1.6060 acc_train: 0.3714 loss_val: 1.6178 acc_val: 0.3867 time: 0.0129s\n",
      "Epoch: 0036 loss_train: 1.5766 acc_train: 0.3929 loss_val: 1.6100 acc_val: 0.3967 time: 0.0130s\n",
      "Epoch: 0037 loss_train: 1.5666 acc_train: 0.4000 loss_val: 1.6019 acc_val: 0.4033 time: 0.0120s\n",
      "Epoch: 0038 loss_train: 1.5377 acc_train: 0.4500 loss_val: 1.5934 acc_val: 0.4067 time: 0.0120s\n",
      "Epoch: 0039 loss_train: 1.5263 acc_train: 0.4500 loss_val: 1.5841 acc_val: 0.4133 time: 0.0120s\n",
      "Epoch: 0040 loss_train: 1.5057 acc_train: 0.4786 loss_val: 1.5741 acc_val: 0.4233 time: 0.0120s\n",
      "Epoch: 0041 loss_train: 1.4735 acc_train: 0.4929 loss_val: 1.5631 acc_val: 0.4333 time: 0.0120s\n",
      "Epoch: 0042 loss_train: 1.4460 acc_train: 0.4929 loss_val: 1.5517 acc_val: 0.4400 time: 0.0110s\n",
      "Epoch: 0043 loss_train: 1.4397 acc_train: 0.4500 loss_val: 1.5397 acc_val: 0.4467 time: 0.0100s\n",
      "Epoch: 0044 loss_train: 1.4339 acc_train: 0.5214 loss_val: 1.5273 acc_val: 0.4467 time: 0.0090s\n",
      "Epoch: 0045 loss_train: 1.4184 acc_train: 0.4714 loss_val: 1.5146 acc_val: 0.4433 time: 0.0100s\n",
      "Epoch: 0046 loss_train: 1.4104 acc_train: 0.4929 loss_val: 1.5019 acc_val: 0.4433 time: 0.0090s\n",
      "Epoch: 0047 loss_train: 1.4011 acc_train: 0.5286 loss_val: 1.4892 acc_val: 0.4533 time: 0.0100s\n",
      "Epoch: 0048 loss_train: 1.3618 acc_train: 0.5000 loss_val: 1.4764 acc_val: 0.4667 time: 0.0100s\n",
      "Epoch: 0049 loss_train: 1.3854 acc_train: 0.5000 loss_val: 1.4637 acc_val: 0.4800 time: 0.0109s\n",
      "Epoch: 0050 loss_train: 1.3899 acc_train: 0.4929 loss_val: 1.4511 acc_val: 0.4933 time: 0.0110s\n",
      "Epoch: 0051 loss_train: 1.3078 acc_train: 0.5714 loss_val: 1.4387 acc_val: 0.5100 time: 0.0100s\n",
      "Epoch: 0052 loss_train: 1.3209 acc_train: 0.5357 loss_val: 1.4265 acc_val: 0.5300 time: 0.0119s\n",
      "Epoch: 0053 loss_train: 1.2894 acc_train: 0.6214 loss_val: 1.4140 acc_val: 0.5500 time: 0.0100s\n",
      "Epoch: 0054 loss_train: 1.2694 acc_train: 0.5786 loss_val: 1.4014 acc_val: 0.5733 time: 0.0110s\n",
      "Epoch: 0055 loss_train: 1.2676 acc_train: 0.6071 loss_val: 1.3887 acc_val: 0.5867 time: 0.0109s\n",
      "Epoch: 0056 loss_train: 1.2662 acc_train: 0.6429 loss_val: 1.3761 acc_val: 0.6067 time: 0.0110s\n",
      "Epoch: 0057 loss_train: 1.2361 acc_train: 0.6857 loss_val: 1.3635 acc_val: 0.6167 time: 0.0120s\n",
      "Epoch: 0058 loss_train: 1.2132 acc_train: 0.6857 loss_val: 1.3509 acc_val: 0.6300 time: 0.0110s\n",
      "Epoch: 0059 loss_train: 1.1654 acc_train: 0.7000 loss_val: 1.3388 acc_val: 0.6533 time: 0.0109s\n",
      "Epoch: 0060 loss_train: 1.1918 acc_train: 0.7071 loss_val: 1.3264 acc_val: 0.6733 time: 0.0110s\n",
      "Epoch: 0061 loss_train: 1.1363 acc_train: 0.7571 loss_val: 1.3141 acc_val: 0.6833 time: 0.0110s\n",
      "Epoch: 0062 loss_train: 1.1622 acc_train: 0.7500 loss_val: 1.3014 acc_val: 0.7000 time: 0.0100s\n",
      "Epoch: 0063 loss_train: 1.1320 acc_train: 0.7857 loss_val: 1.2887 acc_val: 0.7167 time: 0.0089s\n",
      "Epoch: 0064 loss_train: 1.1255 acc_train: 0.7643 loss_val: 1.2756 acc_val: 0.7200 time: 0.0100s\n",
      "Epoch: 0065 loss_train: 1.0759 acc_train: 0.8000 loss_val: 1.2624 acc_val: 0.7300 time: 0.0110s\n",
      "Epoch: 0066 loss_train: 1.0923 acc_train: 0.7786 loss_val: 1.2494 acc_val: 0.7300 time: 0.0100s\n",
      "Epoch: 0067 loss_train: 1.1059 acc_train: 0.7929 loss_val: 1.2368 acc_val: 0.7333 time: 0.0110s\n",
      "Epoch: 0068 loss_train: 1.0819 acc_train: 0.7929 loss_val: 1.2243 acc_val: 0.7367 time: 0.0100s\n",
      "Epoch: 0069 loss_train: 1.0801 acc_train: 0.7500 loss_val: 1.2120 acc_val: 0.7433 time: 0.0110s\n",
      "Epoch: 0070 loss_train: 1.0300 acc_train: 0.8071 loss_val: 1.1999 acc_val: 0.7467 time: 0.0109s\n",
      "Epoch: 0071 loss_train: 1.0058 acc_train: 0.8214 loss_val: 1.1878 acc_val: 0.7433 time: 0.0140s\n",
      "Epoch: 0072 loss_train: 1.0286 acc_train: 0.8071 loss_val: 1.1761 acc_val: 0.7467 time: 0.0110s\n",
      "Epoch: 0073 loss_train: 0.9796 acc_train: 0.8000 loss_val: 1.1650 acc_val: 0.7567 time: 0.0119s\n",
      "Epoch: 0074 loss_train: 0.9918 acc_train: 0.8143 loss_val: 1.1543 acc_val: 0.7600 time: 0.0120s\n",
      "Epoch: 0075 loss_train: 0.9882 acc_train: 0.8000 loss_val: 1.1441 acc_val: 0.7567 time: 0.0120s\n",
      "Epoch: 0076 loss_train: 0.9576 acc_train: 0.8286 loss_val: 1.1339 acc_val: 0.7600 time: 0.0120s\n",
      "Epoch: 0077 loss_train: 0.9305 acc_train: 0.8429 loss_val: 1.1238 acc_val: 0.7600 time: 0.0110s\n",
      "Epoch: 0078 loss_train: 0.9410 acc_train: 0.8357 loss_val: 1.1140 acc_val: 0.7667 time: 0.0129s\n",
      "Epoch: 0079 loss_train: 0.8989 acc_train: 0.8500 loss_val: 1.1043 acc_val: 0.7733 time: 0.0130s\n",
      "Epoch: 0080 loss_train: 0.9229 acc_train: 0.7786 loss_val: 1.0944 acc_val: 0.7833 time: 0.0100s\n",
      "Epoch: 0081 loss_train: 0.8947 acc_train: 0.8357 loss_val: 1.0849 acc_val: 0.7933 time: 0.0120s\n",
      "Epoch: 0082 loss_train: 0.8612 acc_train: 0.8429 loss_val: 1.0761 acc_val: 0.8000 time: 0.0120s\n",
      "Epoch: 0083 loss_train: 0.8677 acc_train: 0.8500 loss_val: 1.0671 acc_val: 0.8000 time: 0.0109s\n",
      "Epoch: 0084 loss_train: 0.8377 acc_train: 0.8857 loss_val: 1.0583 acc_val: 0.8000 time: 0.0110s\n",
      "Epoch: 0085 loss_train: 0.8537 acc_train: 0.8429 loss_val: 1.0502 acc_val: 0.7967 time: 0.0110s\n",
      "Epoch: 0086 loss_train: 0.8512 acc_train: 0.8500 loss_val: 1.0421 acc_val: 0.8000 time: 0.0119s\n",
      "Epoch: 0087 loss_train: 0.8451 acc_train: 0.8429 loss_val: 1.0341 acc_val: 0.8000 time: 0.0120s\n",
      "Epoch: 0088 loss_train: 0.8721 acc_train: 0.8571 loss_val: 1.0255 acc_val: 0.8000 time: 0.0120s\n",
      "Epoch: 0089 loss_train: 0.7635 acc_train: 0.8500 loss_val: 1.0168 acc_val: 0.8000 time: 0.0120s\n",
      "Epoch: 0090 loss_train: 0.8154 acc_train: 0.8429 loss_val: 1.0080 acc_val: 0.8000 time: 0.0120s\n",
      "Epoch: 0091 loss_train: 0.7940 acc_train: 0.8643 loss_val: 0.9993 acc_val: 0.7967 time: 0.0120s\n",
      "Epoch: 0092 loss_train: 0.8452 acc_train: 0.8357 loss_val: 0.9910 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0093 loss_train: 0.7682 acc_train: 0.8714 loss_val: 0.9829 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0094 loss_train: 0.7936 acc_train: 0.8429 loss_val: 0.9757 acc_val: 0.8000 time: 0.0110s\n",
      "Epoch: 0095 loss_train: 0.7684 acc_train: 0.8286 loss_val: 0.9685 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0096 loss_train: 0.7488 acc_train: 0.8714 loss_val: 0.9616 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0097 loss_train: 0.7485 acc_train: 0.8714 loss_val: 0.9545 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0098 loss_train: 0.7416 acc_train: 0.8500 loss_val: 0.9480 acc_val: 0.8033 time: 0.0120s\n",
      "Epoch: 0099 loss_train: 0.7045 acc_train: 0.8786 loss_val: 0.9413 acc_val: 0.8033 time: 0.0109s\n",
      "Epoch: 0100 loss_train: 0.6815 acc_train: 0.8571 loss_val: 0.9350 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0101 loss_train: 0.7076 acc_train: 0.8571 loss_val: 0.9288 acc_val: 0.8067 time: 0.0140s\n",
      "Epoch: 0102 loss_train: 0.6743 acc_train: 0.8786 loss_val: 0.9224 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0103 loss_train: 0.6835 acc_train: 0.8786 loss_val: 0.9167 acc_val: 0.8033 time: 0.0109s\n",
      "Epoch: 0104 loss_train: 0.7372 acc_train: 0.8500 loss_val: 0.9106 acc_val: 0.8033 time: 0.0120s\n",
      "Epoch: 0105 loss_train: 0.6797 acc_train: 0.8929 loss_val: 0.9041 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0106 loss_train: 0.6778 acc_train: 0.8643 loss_val: 0.8971 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0107 loss_train: 0.6870 acc_train: 0.8857 loss_val: 0.8906 acc_val: 0.8100 time: 0.0130s\n",
      "Epoch: 0108 loss_train: 0.6819 acc_train: 0.8643 loss_val: 0.8850 acc_val: 0.8067 time: 0.0119s\n",
      "Epoch: 0109 loss_train: 0.6727 acc_train: 0.8857 loss_val: 0.8795 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0110 loss_train: 0.6406 acc_train: 0.8714 loss_val: 0.8742 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0111 loss_train: 0.6510 acc_train: 0.9071 loss_val: 0.8696 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0112 loss_train: 0.6303 acc_train: 0.9071 loss_val: 0.8659 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0113 loss_train: 0.6630 acc_train: 0.8500 loss_val: 0.8622 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0114 loss_train: 0.6456 acc_train: 0.8786 loss_val: 0.8593 acc_val: 0.8100 time: 0.0100s\n",
      "Epoch: 0115 loss_train: 0.5768 acc_train: 0.9000 loss_val: 0.8568 acc_val: 0.8100 time: 0.0100s\n",
      "Epoch: 0116 loss_train: 0.5876 acc_train: 0.9000 loss_val: 0.8544 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0117 loss_train: 0.6308 acc_train: 0.8786 loss_val: 0.8516 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0118 loss_train: 0.5925 acc_train: 0.9071 loss_val: 0.8468 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0119 loss_train: 0.6705 acc_train: 0.8857 loss_val: 0.8405 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0120 loss_train: 0.6049 acc_train: 0.9000 loss_val: 0.8336 acc_val: 0.8067 time: 0.0119s\n",
      "Epoch: 0121 loss_train: 0.6180 acc_train: 0.8857 loss_val: 0.8278 acc_val: 0.8133 time: 0.0110s\n",
      "Epoch: 0122 loss_train: 0.5256 acc_train: 0.9143 loss_val: 0.8234 acc_val: 0.8167 time: 0.0110s\n",
      "Epoch: 0123 loss_train: 0.5830 acc_train: 0.9143 loss_val: 0.8193 acc_val: 0.8167 time: 0.0110s\n",
      "Epoch: 0124 loss_train: 0.5372 acc_train: 0.9286 loss_val: 0.8155 acc_val: 0.8133 time: 0.0110s\n",
      "Epoch: 0125 loss_train: 0.5324 acc_train: 0.9000 loss_val: 0.8124 acc_val: 0.7967 time: 0.0110s\n",
      "Epoch: 0126 loss_train: 0.6010 acc_train: 0.8786 loss_val: 0.8090 acc_val: 0.7900 time: 0.0099s\n",
      "Epoch: 0127 loss_train: 0.5691 acc_train: 0.8929 loss_val: 0.8058 acc_val: 0.7933 time: 0.0100s\n",
      "Epoch: 0128 loss_train: 0.5661 acc_train: 0.9071 loss_val: 0.8025 acc_val: 0.7900 time: 0.0100s\n",
      "Epoch: 0129 loss_train: 0.5254 acc_train: 0.9214 loss_val: 0.7995 acc_val: 0.7900 time: 0.0100s\n",
      "Epoch: 0130 loss_train: 0.5370 acc_train: 0.9000 loss_val: 0.7966 acc_val: 0.7900 time: 0.0110s\n",
      "Epoch: 0131 loss_train: 0.5424 acc_train: 0.9000 loss_val: 0.7928 acc_val: 0.7900 time: 0.0100s\n",
      "Epoch: 0132 loss_train: 0.5187 acc_train: 0.9071 loss_val: 0.7887 acc_val: 0.7900 time: 0.0120s\n",
      "Epoch: 0133 loss_train: 0.5650 acc_train: 0.9071 loss_val: 0.7844 acc_val: 0.7900 time: 0.0099s\n",
      "Epoch: 0134 loss_train: 0.5291 acc_train: 0.8857 loss_val: 0.7803 acc_val: 0.7900 time: 0.0090s\n",
      "Epoch: 0135 loss_train: 0.5100 acc_train: 0.9071 loss_val: 0.7759 acc_val: 0.7900 time: 0.0110s\n",
      "Epoch: 0136 loss_train: 0.5287 acc_train: 0.9071 loss_val: 0.7724 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0137 loss_train: 0.5165 acc_train: 0.8857 loss_val: 0.7697 acc_val: 0.8067 time: 0.0099s\n",
      "Epoch: 0138 loss_train: 0.5429 acc_train: 0.9143 loss_val: 0.7675 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0139 loss_train: 0.5118 acc_train: 0.8929 loss_val: 0.7646 acc_val: 0.8033 time: 0.0109s\n",
      "Epoch: 0140 loss_train: 0.5345 acc_train: 0.9571 loss_val: 0.7615 acc_val: 0.8067 time: 0.0120s\n",
      "Epoch: 0141 loss_train: 0.5317 acc_train: 0.9071 loss_val: 0.7593 acc_val: 0.8167 time: 0.0110s\n",
      "Epoch: 0142 loss_train: 0.5009 acc_train: 0.9143 loss_val: 0.7578 acc_val: 0.8100 time: 0.0100s\n",
      "Epoch: 0143 loss_train: 0.5148 acc_train: 0.9071 loss_val: 0.7568 acc_val: 0.8100 time: 0.0099s\n",
      "Epoch: 0144 loss_train: 0.5151 acc_train: 0.9000 loss_val: 0.7553 acc_val: 0.8100 time: 0.0100s\n",
      "Epoch: 0145 loss_train: 0.4897 acc_train: 0.8714 loss_val: 0.7531 acc_val: 0.8067 time: 0.0109s\n",
      "Epoch: 0146 loss_train: 0.5092 acc_train: 0.9143 loss_val: 0.7509 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0147 loss_train: 0.5254 acc_train: 0.9071 loss_val: 0.7484 acc_val: 0.8033 time: 0.0120s\n",
      "Epoch: 0148 loss_train: 0.4552 acc_train: 0.9143 loss_val: 0.7473 acc_val: 0.7967 time: 0.0109s\n",
      "Epoch: 0149 loss_train: 0.4996 acc_train: 0.9143 loss_val: 0.7457 acc_val: 0.7967 time: 0.0100s\n",
      "Epoch: 0150 loss_train: 0.4849 acc_train: 0.9071 loss_val: 0.7432 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0151 loss_train: 0.4906 acc_train: 0.9071 loss_val: 0.7415 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0152 loss_train: 0.4824 acc_train: 0.9000 loss_val: 0.7391 acc_val: 0.8000 time: 0.0110s\n",
      "Epoch: 0153 loss_train: 0.4555 acc_train: 0.9071 loss_val: 0.7366 acc_val: 0.8000 time: 0.0110s\n",
      "Epoch: 0154 loss_train: 0.4361 acc_train: 0.9357 loss_val: 0.7345 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0155 loss_train: 0.4853 acc_train: 0.9143 loss_val: 0.7326 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0156 loss_train: 0.4390 acc_train: 0.9286 loss_val: 0.7310 acc_val: 0.8033 time: 0.0109s\n",
      "Epoch: 0157 loss_train: 0.4295 acc_train: 0.9214 loss_val: 0.7302 acc_val: 0.8100 time: 0.0120s\n",
      "Epoch: 0158 loss_train: 0.4521 acc_train: 0.9214 loss_val: 0.7292 acc_val: 0.8133 time: 0.0100s\n",
      "Epoch: 0159 loss_train: 0.4379 acc_train: 0.9214 loss_val: 0.7288 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0160 loss_train: 0.4440 acc_train: 0.9143 loss_val: 0.7283 acc_val: 0.8033 time: 0.0109s\n",
      "Epoch: 0161 loss_train: 0.4530 acc_train: 0.9429 loss_val: 0.7276 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0162 loss_train: 0.4259 acc_train: 0.9429 loss_val: 0.7276 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0163 loss_train: 0.4842 acc_train: 0.8857 loss_val: 0.7274 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0164 loss_train: 0.4297 acc_train: 0.9143 loss_val: 0.7264 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0165 loss_train: 0.4612 acc_train: 0.9143 loss_val: 0.7252 acc_val: 0.8000 time: 0.0099s\n",
      "Epoch: 0166 loss_train: 0.4616 acc_train: 0.9357 loss_val: 0.7220 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0167 loss_train: 0.4056 acc_train: 0.9286 loss_val: 0.7187 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0168 loss_train: 0.4099 acc_train: 0.9286 loss_val: 0.7159 acc_val: 0.7967 time: 0.0120s\n",
      "Epoch: 0169 loss_train: 0.4116 acc_train: 0.9286 loss_val: 0.7131 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0170 loss_train: 0.4594 acc_train: 0.9214 loss_val: 0.7122 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0171 loss_train: 0.4372 acc_train: 0.9571 loss_val: 0.7118 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0172 loss_train: 0.4575 acc_train: 0.9286 loss_val: 0.7123 acc_val: 0.7967 time: 0.0099s\n",
      "Epoch: 0173 loss_train: 0.4127 acc_train: 0.9429 loss_val: 0.7126 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0174 loss_train: 0.4154 acc_train: 0.9214 loss_val: 0.7135 acc_val: 0.7967 time: 0.0110s\n",
      "Epoch: 0175 loss_train: 0.4543 acc_train: 0.9500 loss_val: 0.7120 acc_val: 0.7967 time: 0.0110s\n",
      "Epoch: 0176 loss_train: 0.4385 acc_train: 0.9500 loss_val: 0.7090 acc_val: 0.8000 time: 0.0090s\n",
      "Epoch: 0177 loss_train: 0.4119 acc_train: 0.9429 loss_val: 0.7058 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0178 loss_train: 0.4120 acc_train: 0.9357 loss_val: 0.7017 acc_val: 0.8200 time: 0.0110s\n",
      "Epoch: 0179 loss_train: 0.4475 acc_train: 0.9143 loss_val: 0.6977 acc_val: 0.8200 time: 0.0110s\n",
      "Epoch: 0180 loss_train: 0.4193 acc_train: 0.9143 loss_val: 0.6942 acc_val: 0.8167 time: 0.0100s\n",
      "Epoch: 0181 loss_train: 0.4298 acc_train: 0.9143 loss_val: 0.6921 acc_val: 0.8000 time: 0.0110s\n",
      "Epoch: 0182 loss_train: 0.4599 acc_train: 0.9357 loss_val: 0.6911 acc_val: 0.7967 time: 0.0089s\n",
      "Epoch: 0183 loss_train: 0.3983 acc_train: 0.9429 loss_val: 0.6918 acc_val: 0.7967 time: 0.0110s\n",
      "Epoch: 0184 loss_train: 0.4041 acc_train: 0.9429 loss_val: 0.6930 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0185 loss_train: 0.4208 acc_train: 0.9286 loss_val: 0.6927 acc_val: 0.8033 time: 0.0099s\n",
      "Epoch: 0186 loss_train: 0.4237 acc_train: 0.9571 loss_val: 0.6898 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0187 loss_train: 0.3862 acc_train: 0.9429 loss_val: 0.6874 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0188 loss_train: 0.3873 acc_train: 0.9429 loss_val: 0.6871 acc_val: 0.8200 time: 0.0119s\n",
      "Epoch: 0189 loss_train: 0.3920 acc_train: 0.9429 loss_val: 0.6866 acc_val: 0.8200 time: 0.0090s\n",
      "Epoch: 0190 loss_train: 0.4330 acc_train: 0.9071 loss_val: 0.6853 acc_val: 0.8133 time: 0.0100s\n",
      "Epoch: 0191 loss_train: 0.4476 acc_train: 0.9143 loss_val: 0.6840 acc_val: 0.8100 time: 0.0110s\n",
      "Epoch: 0192 loss_train: 0.4015 acc_train: 0.9143 loss_val: 0.6830 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0193 loss_train: 0.3959 acc_train: 0.9286 loss_val: 0.6828 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0194 loss_train: 0.3732 acc_train: 0.9143 loss_val: 0.6839 acc_val: 0.7967 time: 0.0120s\n",
      "Epoch: 0195 loss_train: 0.4074 acc_train: 0.9429 loss_val: 0.6841 acc_val: 0.7967 time: 0.0110s\n",
      "Epoch: 0196 loss_train: 0.3902 acc_train: 0.9214 loss_val: 0.6847 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0197 loss_train: 0.3921 acc_train: 0.9500 loss_val: 0.6837 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0198 loss_train: 0.3777 acc_train: 0.9500 loss_val: 0.6818 acc_val: 0.8067 time: 0.0099s\n",
      "Epoch: 0199 loss_train: 0.3644 acc_train: 0.9571 loss_val: 0.6796 acc_val: 0.8167 time: 0.0110s\n",
      "Epoch: 0200 loss_train: 0.4050 acc_train: 0.9357 loss_val: 0.6771 acc_val: 0.8167 time: 0.0100s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 2.2506s\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98fc41e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.7000 accuracy= 0.8400\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfcca19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
