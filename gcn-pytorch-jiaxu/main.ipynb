{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44582f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import argparse\n",
    "import scipy.sparse as sp\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75c034c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8910a2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f621b0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4899fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path=\"./data/cora/\", dataset=\"cora\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "    \n",
    "    print(\"Dataset Loaded!\")\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8156b5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n",
      "Dataset Loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62c2691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "epochs = 200\n",
    "weightDecay = 5e-4\n",
    "learningRate = 0.01\n",
    "hidden = 16\n",
    "dropout = 0.5\n",
    "\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=learningRate, weight_decay=weightDecay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16c9b592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate validation set performance separately,\n",
    "    # deactivates dropout during validation run.\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "256a6311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "008610b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 2.0385 acc_train: 0.0714 loss_val: 2.0173 acc_val: 0.0567 time: 0.1025s\n",
      "Epoch: 0002 loss_train: 2.0195 acc_train: 0.0857 loss_val: 1.9998 acc_val: 0.0433 time: 0.0070s\n",
      "Epoch: 0003 loss_train: 2.0009 acc_train: 0.1071 loss_val: 1.9829 acc_val: 0.0700 time: 0.0060s\n",
      "Epoch: 0004 loss_train: 1.9821 acc_train: 0.1357 loss_val: 1.9666 acc_val: 0.1133 time: 0.0070s\n",
      "Epoch: 0005 loss_train: 1.9672 acc_train: 0.0857 loss_val: 1.9500 acc_val: 0.1400 time: 0.0060s\n",
      "Epoch: 0006 loss_train: 1.9592 acc_train: 0.1357 loss_val: 1.9335 acc_val: 0.1733 time: 0.0070s\n",
      "Epoch: 0007 loss_train: 1.9288 acc_train: 0.2357 loss_val: 1.9167 acc_val: 0.2200 time: 0.0070s\n",
      "Epoch: 0008 loss_train: 1.9112 acc_train: 0.1929 loss_val: 1.8996 acc_val: 0.3133 time: 0.0070s\n",
      "Epoch: 0009 loss_train: 1.9037 acc_train: 0.2500 loss_val: 1.8824 acc_val: 0.4633 time: 0.0070s\n",
      "Epoch: 0010 loss_train: 1.8822 acc_train: 0.3357 loss_val: 1.8651 acc_val: 0.3967 time: 0.0070s\n",
      "Epoch: 0011 loss_train: 1.8689 acc_train: 0.2786 loss_val: 1.8477 acc_val: 0.3533 time: 0.0070s\n",
      "Epoch: 0012 loss_train: 1.8410 acc_train: 0.3500 loss_val: 1.8303 acc_val: 0.3467 time: 0.0060s\n",
      "Epoch: 0013 loss_train: 1.8172 acc_train: 0.3071 loss_val: 1.8133 acc_val: 0.3500 time: 0.0060s\n",
      "Epoch: 0014 loss_train: 1.8059 acc_train: 0.3000 loss_val: 1.7969 acc_val: 0.3500 time: 0.0080s\n",
      "Epoch: 0015 loss_train: 1.8008 acc_train: 0.3071 loss_val: 1.7815 acc_val: 0.3500 time: 0.0080s\n",
      "Epoch: 0016 loss_train: 1.7857 acc_train: 0.2857 loss_val: 1.7674 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0017 loss_train: 1.7640 acc_train: 0.3000 loss_val: 1.7548 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0018 loss_train: 1.7204 acc_train: 0.2929 loss_val: 1.7435 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0019 loss_train: 1.7348 acc_train: 0.2929 loss_val: 1.7335 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0020 loss_train: 1.7089 acc_train: 0.3143 loss_val: 1.7247 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0021 loss_train: 1.7101 acc_train: 0.3071 loss_val: 1.7167 acc_val: 0.3500 time: 0.0060s\n",
      "Epoch: 0022 loss_train: 1.7060 acc_train: 0.3000 loss_val: 1.7091 acc_val: 0.3500 time: 0.0060s\n",
      "Epoch: 0023 loss_train: 1.6925 acc_train: 0.3143 loss_val: 1.7016 acc_val: 0.3500 time: 0.0060s\n",
      "Epoch: 0024 loss_train: 1.7097 acc_train: 0.3071 loss_val: 1.6939 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0025 loss_train: 1.6557 acc_train: 0.3214 loss_val: 1.6860 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0026 loss_train: 1.6358 acc_train: 0.3286 loss_val: 1.6780 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0027 loss_train: 1.6589 acc_train: 0.3214 loss_val: 1.6700 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0028 loss_train: 1.6599 acc_train: 0.3429 loss_val: 1.6618 acc_val: 0.3467 time: 0.0090s\n",
      "Epoch: 0029 loss_train: 1.5981 acc_train: 0.3500 loss_val: 1.6533 acc_val: 0.3467 time: 0.0070s\n",
      "Epoch: 0030 loss_train: 1.6096 acc_train: 0.3929 loss_val: 1.6448 acc_val: 0.3500 time: 0.0080s\n",
      "Epoch: 0031 loss_train: 1.5962 acc_train: 0.3429 loss_val: 1.6362 acc_val: 0.3533 time: 0.0070s\n",
      "Epoch: 0032 loss_train: 1.5849 acc_train: 0.4000 loss_val: 1.6274 acc_val: 0.3733 time: 0.0090s\n",
      "Epoch: 0033 loss_train: 1.5776 acc_train: 0.4214 loss_val: 1.6185 acc_val: 0.3833 time: 0.0070s\n",
      "Epoch: 0034 loss_train: 1.5797 acc_train: 0.3786 loss_val: 1.6093 acc_val: 0.3933 time: 0.0070s\n",
      "Epoch: 0035 loss_train: 1.5754 acc_train: 0.4286 loss_val: 1.5999 acc_val: 0.4033 time: 0.0070s\n",
      "Epoch: 0036 loss_train: 1.5402 acc_train: 0.4571 loss_val: 1.5901 acc_val: 0.4333 time: 0.0060s\n",
      "Epoch: 0037 loss_train: 1.5033 acc_train: 0.4714 loss_val: 1.5801 acc_val: 0.4400 time: 0.0060s\n",
      "Epoch: 0038 loss_train: 1.5184 acc_train: 0.5000 loss_val: 1.5697 acc_val: 0.4500 time: 0.0070s\n",
      "Epoch: 0039 loss_train: 1.4840 acc_train: 0.5071 loss_val: 1.5590 acc_val: 0.4600 time: 0.0070s\n",
      "Epoch: 0040 loss_train: 1.4649 acc_train: 0.5429 loss_val: 1.5480 acc_val: 0.4633 time: 0.0070s\n",
      "Epoch: 0041 loss_train: 1.4414 acc_train: 0.5429 loss_val: 1.5363 acc_val: 0.4667 time: 0.0060s\n",
      "Epoch: 0042 loss_train: 1.4561 acc_train: 0.5071 loss_val: 1.5246 acc_val: 0.4700 time: 0.0060s\n",
      "Epoch: 0043 loss_train: 1.4270 acc_train: 0.5214 loss_val: 1.5128 acc_val: 0.4800 time: 0.0070s\n",
      "Epoch: 0044 loss_train: 1.4278 acc_train: 0.5071 loss_val: 1.5009 acc_val: 0.4933 time: 0.0070s\n",
      "Epoch: 0045 loss_train: 1.3900 acc_train: 0.5286 loss_val: 1.4887 acc_val: 0.5033 time: 0.0060s\n",
      "Epoch: 0046 loss_train: 1.3777 acc_train: 0.5357 loss_val: 1.4764 acc_val: 0.5067 time: 0.0059s\n",
      "Epoch: 0047 loss_train: 1.3814 acc_train: 0.5429 loss_val: 1.4640 acc_val: 0.5100 time: 0.0070s\n",
      "Epoch: 0048 loss_train: 1.3495 acc_train: 0.5643 loss_val: 1.4513 acc_val: 0.5167 time: 0.0080s\n",
      "Epoch: 0049 loss_train: 1.3136 acc_train: 0.5714 loss_val: 1.4383 acc_val: 0.5200 time: 0.0070s\n",
      "Epoch: 0050 loss_train: 1.3210 acc_train: 0.5857 loss_val: 1.4251 acc_val: 0.5300 time: 0.0070s\n",
      "Epoch: 0051 loss_train: 1.3267 acc_train: 0.5500 loss_val: 1.4119 acc_val: 0.5300 time: 0.0060s\n",
      "Epoch: 0052 loss_train: 1.2676 acc_train: 0.6357 loss_val: 1.3985 acc_val: 0.5333 time: 0.0070s\n",
      "Epoch: 0053 loss_train: 1.2551 acc_train: 0.6000 loss_val: 1.3850 acc_val: 0.5367 time: 0.0070s\n",
      "Epoch: 0054 loss_train: 1.2539 acc_train: 0.6143 loss_val: 1.3714 acc_val: 0.5500 time: 0.0080s\n",
      "Epoch: 0055 loss_train: 1.2052 acc_train: 0.6429 loss_val: 1.3581 acc_val: 0.5600 time: 0.0070s\n",
      "Epoch: 0056 loss_train: 1.2071 acc_train: 0.6357 loss_val: 1.3451 acc_val: 0.5667 time: 0.0070s\n",
      "Epoch: 0057 loss_train: 1.2166 acc_train: 0.6357 loss_val: 1.3324 acc_val: 0.5833 time: 0.0070s\n",
      "Epoch: 0058 loss_train: 1.1887 acc_train: 0.7000 loss_val: 1.3198 acc_val: 0.5833 time: 0.0070s\n",
      "Epoch: 0059 loss_train: 1.1327 acc_train: 0.6786 loss_val: 1.3067 acc_val: 0.5867 time: 0.0070s\n",
      "Epoch: 0060 loss_train: 1.1588 acc_train: 0.7071 loss_val: 1.2938 acc_val: 0.6100 time: 0.0060s\n",
      "Epoch: 0061 loss_train: 1.1724 acc_train: 0.7286 loss_val: 1.2808 acc_val: 0.6267 time: 0.0060s\n",
      "Epoch: 0062 loss_train: 1.1469 acc_train: 0.7000 loss_val: 1.2674 acc_val: 0.6467 time: 0.0070s\n",
      "Epoch: 0063 loss_train: 1.1175 acc_train: 0.7214 loss_val: 1.2538 acc_val: 0.6533 time: 0.0070s\n",
      "Epoch: 0064 loss_train: 1.0539 acc_train: 0.7500 loss_val: 1.2401 acc_val: 0.6567 time: 0.0080s\n",
      "Epoch: 0065 loss_train: 1.1014 acc_train: 0.7500 loss_val: 1.2268 acc_val: 0.6567 time: 0.0070s\n",
      "Epoch: 0066 loss_train: 1.0106 acc_train: 0.8000 loss_val: 1.2140 acc_val: 0.6700 time: 0.0060s\n",
      "Epoch: 0067 loss_train: 1.0471 acc_train: 0.7857 loss_val: 1.2010 acc_val: 0.6733 time: 0.0070s\n",
      "Epoch: 0068 loss_train: 1.0295 acc_train: 0.8000 loss_val: 1.1884 acc_val: 0.6900 time: 0.0060s\n",
      "Epoch: 0069 loss_train: 0.9919 acc_train: 0.7929 loss_val: 1.1760 acc_val: 0.6933 time: 0.0070s\n",
      "Epoch: 0070 loss_train: 0.9717 acc_train: 0.8000 loss_val: 1.1639 acc_val: 0.7033 time: 0.0080s\n",
      "Epoch: 0071 loss_train: 0.9795 acc_train: 0.7857 loss_val: 1.1523 acc_val: 0.7200 time: 0.0080s\n",
      "Epoch: 0072 loss_train: 0.9832 acc_train: 0.8214 loss_val: 1.1411 acc_val: 0.7300 time: 0.0060s\n",
      "Epoch: 0073 loss_train: 0.9537 acc_train: 0.8214 loss_val: 1.1304 acc_val: 0.7467 time: 0.0070s\n",
      "Epoch: 0074 loss_train: 0.9663 acc_train: 0.8000 loss_val: 1.1196 acc_val: 0.7600 time: 0.0070s\n",
      "Epoch: 0075 loss_train: 0.9405 acc_train: 0.8143 loss_val: 1.1083 acc_val: 0.7700 time: 0.0060s\n",
      "Epoch: 0076 loss_train: 0.9183 acc_train: 0.8143 loss_val: 1.0976 acc_val: 0.7933 time: 0.0060s\n",
      "Epoch: 0077 loss_train: 0.9353 acc_train: 0.8143 loss_val: 1.0870 acc_val: 0.7967 time: 0.0060s\n",
      "Epoch: 0078 loss_train: 0.9152 acc_train: 0.8357 loss_val: 1.0771 acc_val: 0.8000 time: 0.0060s\n",
      "Epoch: 0079 loss_train: 0.8773 acc_train: 0.8571 loss_val: 1.0670 acc_val: 0.8067 time: 0.0070s\n",
      "Epoch: 0080 loss_train: 0.8651 acc_train: 0.8643 loss_val: 1.0571 acc_val: 0.8067 time: 0.0060s\n",
      "Epoch: 0081 loss_train: 0.8717 acc_train: 0.8357 loss_val: 1.0473 acc_val: 0.8067 time: 0.0080s\n",
      "Epoch: 0082 loss_train: 0.8394 acc_train: 0.8429 loss_val: 1.0377 acc_val: 0.8067 time: 0.0080s\n",
      "Epoch: 0083 loss_train: 0.8535 acc_train: 0.8286 loss_val: 1.0293 acc_val: 0.8067 time: 0.0070s\n",
      "Epoch: 0084 loss_train: 0.8873 acc_train: 0.7929 loss_val: 1.0210 acc_val: 0.8100 time: 0.0070s\n",
      "Epoch: 0085 loss_train: 0.8065 acc_train: 0.8857 loss_val: 1.0132 acc_val: 0.8133 time: 0.0060s\n",
      "Epoch: 0086 loss_train: 0.8290 acc_train: 0.8357 loss_val: 1.0054 acc_val: 0.8167 time: 0.0060s\n",
      "Epoch: 0087 loss_train: 0.8000 acc_train: 0.8643 loss_val: 0.9982 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0088 loss_train: 0.7892 acc_train: 0.8500 loss_val: 0.9911 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0089 loss_train: 0.7917 acc_train: 0.8571 loss_val: 0.9840 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0090 loss_train: 0.7837 acc_train: 0.8857 loss_val: 0.9759 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0091 loss_train: 0.7693 acc_train: 0.8500 loss_val: 0.9682 acc_val: 0.8100 time: 0.0060s\n",
      "Epoch: 0092 loss_train: 0.7624 acc_train: 0.8929 loss_val: 0.9607 acc_val: 0.8100 time: 0.0090s\n",
      "Epoch: 0093 loss_train: 0.7514 acc_train: 0.8714 loss_val: 0.9529 acc_val: 0.8100 time: 0.0060s\n",
      "Epoch: 0094 loss_train: 0.7106 acc_train: 0.8786 loss_val: 0.9455 acc_val: 0.8100 time: 0.0070s\n",
      "Epoch: 0095 loss_train: 0.7184 acc_train: 0.8786 loss_val: 0.9389 acc_val: 0.8100 time: 0.0060s\n",
      "Epoch: 0096 loss_train: 0.7198 acc_train: 0.8786 loss_val: 0.9331 acc_val: 0.8133 time: 0.0080s\n",
      "Epoch: 0097 loss_train: 0.7491 acc_train: 0.8429 loss_val: 0.9278 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0098 loss_train: 0.6743 acc_train: 0.9000 loss_val: 0.9223 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0099 loss_train: 0.7271 acc_train: 0.8643 loss_val: 0.9168 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0100 loss_train: 0.7478 acc_train: 0.8500 loss_val: 0.9108 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0101 loss_train: 0.6807 acc_train: 0.8857 loss_val: 0.9047 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0102 loss_train: 0.6816 acc_train: 0.8786 loss_val: 0.8992 acc_val: 0.8167 time: 0.0080s\n",
      "Epoch: 0103 loss_train: 0.7050 acc_train: 0.9000 loss_val: 0.8943 acc_val: 0.8200 time: 0.0060s\n",
      "Epoch: 0104 loss_train: 0.6901 acc_train: 0.8500 loss_val: 0.8908 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0105 loss_train: 0.6865 acc_train: 0.8786 loss_val: 0.8861 acc_val: 0.8200 time: 0.0060s\n",
      "Epoch: 0106 loss_train: 0.6603 acc_train: 0.8786 loss_val: 0.8809 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0107 loss_train: 0.6781 acc_train: 0.8929 loss_val: 0.8758 acc_val: 0.8200 time: 0.0060s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0108 loss_train: 0.6497 acc_train: 0.8786 loss_val: 0.8713 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0109 loss_train: 0.6063 acc_train: 0.9071 loss_val: 0.8666 acc_val: 0.8200 time: 0.0100s\n",
      "Epoch: 0110 loss_train: 0.6491 acc_train: 0.8857 loss_val: 0.8625 acc_val: 0.8167 time: 0.0090s\n",
      "Epoch: 0111 loss_train: 0.6026 acc_train: 0.8786 loss_val: 0.8582 acc_val: 0.8200 time: 0.0060s\n",
      "Epoch: 0112 loss_train: 0.6601 acc_train: 0.8857 loss_val: 0.8540 acc_val: 0.8233 time: 0.0060s\n",
      "Epoch: 0113 loss_train: 0.6224 acc_train: 0.8786 loss_val: 0.8494 acc_val: 0.8233 time: 0.0060s\n",
      "Epoch: 0114 loss_train: 0.5992 acc_train: 0.9000 loss_val: 0.8451 acc_val: 0.8233 time: 0.0070s\n",
      "Epoch: 0115 loss_train: 0.6514 acc_train: 0.8857 loss_val: 0.8408 acc_val: 0.8233 time: 0.0070s\n",
      "Epoch: 0116 loss_train: 0.6119 acc_train: 0.8714 loss_val: 0.8370 acc_val: 0.8233 time: 0.0070s\n",
      "Epoch: 0117 loss_train: 0.5956 acc_train: 0.8786 loss_val: 0.8334 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0118 loss_train: 0.5947 acc_train: 0.9000 loss_val: 0.8300 acc_val: 0.8233 time: 0.0070s\n",
      "Epoch: 0119 loss_train: 0.6012 acc_train: 0.8857 loss_val: 0.8271 acc_val: 0.8233 time: 0.0070s\n",
      "Epoch: 0120 loss_train: 0.6142 acc_train: 0.9214 loss_val: 0.8247 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0121 loss_train: 0.5502 acc_train: 0.8929 loss_val: 0.8225 acc_val: 0.8233 time: 0.0060s\n",
      "Epoch: 0122 loss_train: 0.6018 acc_train: 0.9071 loss_val: 0.8197 acc_val: 0.8167 time: 0.0080s\n",
      "Epoch: 0123 loss_train: 0.6061 acc_train: 0.9000 loss_val: 0.8165 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0124 loss_train: 0.5785 acc_train: 0.9071 loss_val: 0.8129 acc_val: 0.8167 time: 0.0080s\n",
      "Epoch: 0125 loss_train: 0.5726 acc_train: 0.9071 loss_val: 0.8091 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0126 loss_train: 0.5736 acc_train: 0.9000 loss_val: 0.8055 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0127 loss_train: 0.5334 acc_train: 0.9143 loss_val: 0.8013 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0128 loss_train: 0.5351 acc_train: 0.8857 loss_val: 0.7977 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0129 loss_train: 0.5785 acc_train: 0.8929 loss_val: 0.7942 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0130 loss_train: 0.5812 acc_train: 0.8929 loss_val: 0.7911 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0131 loss_train: 0.5471 acc_train: 0.9071 loss_val: 0.7880 acc_val: 0.8267 time: 0.0080s\n",
      "Epoch: 0132 loss_train: 0.5322 acc_train: 0.8929 loss_val: 0.7852 acc_val: 0.8233 time: 0.0070s\n",
      "Epoch: 0133 loss_train: 0.5504 acc_train: 0.8857 loss_val: 0.7832 acc_val: 0.8267 time: 0.0090s\n",
      "Epoch: 0134 loss_train: 0.5217 acc_train: 0.9071 loss_val: 0.7819 acc_val: 0.8267 time: 0.0070s\n",
      "Epoch: 0135 loss_train: 0.5306 acc_train: 0.9000 loss_val: 0.7808 acc_val: 0.8267 time: 0.0080s\n",
      "Epoch: 0136 loss_train: 0.5255 acc_train: 0.9000 loss_val: 0.7796 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0137 loss_train: 0.5421 acc_train: 0.8786 loss_val: 0.7778 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0138 loss_train: 0.5134 acc_train: 0.9214 loss_val: 0.7757 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0139 loss_train: 0.5327 acc_train: 0.9000 loss_val: 0.7735 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0140 loss_train: 0.5386 acc_train: 0.9071 loss_val: 0.7710 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0141 loss_train: 0.4884 acc_train: 0.9286 loss_val: 0.7677 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0142 loss_train: 0.4629 acc_train: 0.9357 loss_val: 0.7645 acc_val: 0.8133 time: 0.0080s\n",
      "Epoch: 0143 loss_train: 0.4872 acc_train: 0.9071 loss_val: 0.7616 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0144 loss_train: 0.5226 acc_train: 0.9071 loss_val: 0.7591 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0145 loss_train: 0.4847 acc_train: 0.9000 loss_val: 0.7577 acc_val: 0.8133 time: 0.0060s\n",
      "Epoch: 0146 loss_train: 0.4896 acc_train: 0.9143 loss_val: 0.7574 acc_val: 0.8100 time: 0.0070s\n",
      "Epoch: 0147 loss_train: 0.5275 acc_train: 0.8929 loss_val: 0.7568 acc_val: 0.8100 time: 0.0070s\n",
      "Epoch: 0148 loss_train: 0.4956 acc_train: 0.9143 loss_val: 0.7559 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0149 loss_train: 0.5034 acc_train: 0.9214 loss_val: 0.7541 acc_val: 0.8167 time: 0.0080s\n",
      "Epoch: 0150 loss_train: 0.4992 acc_train: 0.8786 loss_val: 0.7522 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0151 loss_train: 0.5242 acc_train: 0.9000 loss_val: 0.7498 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0152 loss_train: 0.4947 acc_train: 0.9000 loss_val: 0.7479 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0153 loss_train: 0.4967 acc_train: 0.9286 loss_val: 0.7472 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0154 loss_train: 0.4897 acc_train: 0.9143 loss_val: 0.7463 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0155 loss_train: 0.4817 acc_train: 0.9214 loss_val: 0.7453 acc_val: 0.8133 time: 0.0080s\n",
      "Epoch: 0156 loss_train: 0.4998 acc_train: 0.9071 loss_val: 0.7435 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0157 loss_train: 0.4710 acc_train: 0.9357 loss_val: 0.7415 acc_val: 0.8200 time: 0.0090s\n",
      "Epoch: 0158 loss_train: 0.5243 acc_train: 0.9143 loss_val: 0.7390 acc_val: 0.8200 time: 0.0090s\n",
      "Epoch: 0159 loss_train: 0.4583 acc_train: 0.9143 loss_val: 0.7372 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0160 loss_train: 0.4556 acc_train: 0.9214 loss_val: 0.7347 acc_val: 0.8167 time: 0.0090s\n",
      "Epoch: 0161 loss_train: 0.4633 acc_train: 0.9143 loss_val: 0.7323 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0162 loss_train: 0.4637 acc_train: 0.9143 loss_val: 0.7299 acc_val: 0.8167 time: 0.0090s\n",
      "Epoch: 0163 loss_train: 0.4929 acc_train: 0.9214 loss_val: 0.7279 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0164 loss_train: 0.4782 acc_train: 0.9143 loss_val: 0.7265 acc_val: 0.8167 time: 0.0080s\n",
      "Epoch: 0165 loss_train: 0.4590 acc_train: 0.9214 loss_val: 0.7255 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0166 loss_train: 0.4781 acc_train: 0.9071 loss_val: 0.7244 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0167 loss_train: 0.4506 acc_train: 0.9143 loss_val: 0.7234 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0168 loss_train: 0.4473 acc_train: 0.9429 loss_val: 0.7225 acc_val: 0.8167 time: 0.0080s\n",
      "Epoch: 0169 loss_train: 0.4191 acc_train: 0.9500 loss_val: 0.7215 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0170 loss_train: 0.4601 acc_train: 0.9071 loss_val: 0.7209 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0171 loss_train: 0.4444 acc_train: 0.9357 loss_val: 0.7203 acc_val: 0.8167 time: 0.0080s\n",
      "Epoch: 0172 loss_train: 0.4730 acc_train: 0.9071 loss_val: 0.7196 acc_val: 0.8100 time: 0.0080s\n",
      "Epoch: 0173 loss_train: 0.4231 acc_train: 0.9286 loss_val: 0.7180 acc_val: 0.8133 time: 0.0080s\n",
      "Epoch: 0174 loss_train: 0.4110 acc_train: 0.9214 loss_val: 0.7157 acc_val: 0.8133 time: 0.0060s\n",
      "Epoch: 0175 loss_train: 0.4359 acc_train: 0.9286 loss_val: 0.7138 acc_val: 0.8167 time: 0.0080s\n",
      "Epoch: 0176 loss_train: 0.4227 acc_train: 0.9214 loss_val: 0.7122 acc_val: 0.8133 time: 0.0060s\n",
      "Epoch: 0177 loss_train: 0.4442 acc_train: 0.9143 loss_val: 0.7111 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0178 loss_train: 0.4531 acc_train: 0.9000 loss_val: 0.7104 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0179 loss_train: 0.4439 acc_train: 0.9071 loss_val: 0.7103 acc_val: 0.8167 time: 0.0080s\n",
      "Epoch: 0180 loss_train: 0.4104 acc_train: 0.9429 loss_val: 0.7101 acc_val: 0.8200 time: 0.0090s\n",
      "Epoch: 0181 loss_train: 0.4008 acc_train: 0.9714 loss_val: 0.7091 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0182 loss_train: 0.4393 acc_train: 0.9286 loss_val: 0.7075 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0183 loss_train: 0.4037 acc_train: 0.9429 loss_val: 0.7045 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0184 loss_train: 0.4210 acc_train: 0.9143 loss_val: 0.7029 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0185 loss_train: 0.3923 acc_train: 0.9429 loss_val: 0.7019 acc_val: 0.8233 time: 0.0070s\n",
      "Epoch: 0186 loss_train: 0.4076 acc_train: 0.9571 loss_val: 0.7017 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0187 loss_train: 0.4144 acc_train: 0.9357 loss_val: 0.7025 acc_val: 0.8167 time: 0.0060s\n",
      "Epoch: 0188 loss_train: 0.4416 acc_train: 0.9286 loss_val: 0.7037 acc_val: 0.8100 time: 0.0070s\n",
      "Epoch: 0189 loss_train: 0.4026 acc_train: 0.9214 loss_val: 0.7046 acc_val: 0.8100 time: 0.0070s\n",
      "Epoch: 0190 loss_train: 0.4180 acc_train: 0.9429 loss_val: 0.7056 acc_val: 0.8167 time: 0.0060s\n",
      "Epoch: 0191 loss_train: 0.4150 acc_train: 0.9143 loss_val: 0.7056 acc_val: 0.8133 time: 0.0080s\n",
      "Epoch: 0192 loss_train: 0.4486 acc_train: 0.9214 loss_val: 0.7047 acc_val: 0.8067 time: 0.0060s\n",
      "Epoch: 0193 loss_train: 0.4030 acc_train: 0.9429 loss_val: 0.7023 acc_val: 0.8033 time: 0.0070s\n",
      "Epoch: 0194 loss_train: 0.4206 acc_train: 0.9214 loss_val: 0.6992 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0195 loss_train: 0.4166 acc_train: 0.9429 loss_val: 0.6966 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0196 loss_train: 0.3721 acc_train: 0.9571 loss_val: 0.6950 acc_val: 0.8133 time: 0.0060s\n",
      "Epoch: 0197 loss_train: 0.4048 acc_train: 0.9357 loss_val: 0.6932 acc_val: 0.8133 time: 0.0080s\n",
      "Epoch: 0198 loss_train: 0.3988 acc_train: 0.9071 loss_val: 0.6917 acc_val: 0.8100 time: 0.0080s\n",
      "Epoch: 0199 loss_train: 0.3931 acc_train: 0.9286 loss_val: 0.6905 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0200 loss_train: 0.3947 acc_train: 0.9429 loss_val: 0.6894 acc_val: 0.8100 time: 0.0070s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 1.5396s\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98fc41e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.7187 accuracy= 0.8240\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfcca19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
