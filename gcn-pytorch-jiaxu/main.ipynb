{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53f32db5-33c5-4bfa-8e8f-87c27a93369a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis model is currently not optimized for GPU!!!\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This model is currently not optimized for GPU!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44582f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import argparse\n",
    "import scipy.sparse as sp\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75c034c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(Module):\n",
    "    # GCN Layer\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else: \n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8910a2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f621b0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "def normalize(mx):\n",
    "    # Row-normalize sparse matrix\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    # Convert a scipy sparse matrix to a torch sparse tensor\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1895fe4a-5031-49ee-8215-b667cc98a41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Dataset Attribute (Commented)\n",
    "cora_path = \"../data/cora/\"\n",
    "def test_dataset(path=cora_path, dataset=\"cora\"):\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset), dtype=np.dtype(str))\n",
    "    print(type(idx_features_labels)); print(idx_features_labels.shape); \n",
    "    # print(idx_features_labels)\n",
    "    \n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    print(type(edges_unordered)); print(edges_unordered.shape); \n",
    "    # print(edges_unordered)\n",
    "    # print(edges)\n",
    "    return None\n",
    "# test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4899fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path=cora_path, dataset=\"cora\"):\n",
    "    # Load citation network dataset (cora only for now)\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset), dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset), dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "    \n",
    "    print(\"Dataset Loaded!\")\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8156b5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n",
      "Dataset Loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62c2691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "epochs = 200\n",
    "weightDecay = 5e-4\n",
    "learningRate = 0.01\n",
    "hidden = 16\n",
    "dropout = 0.5\n",
    "\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learningRate, weight_decay=weightDecay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16c9b592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate validation set performance separately,\n",
    "    # deactivates dropout during validation run.\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "256a6311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "008610b6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 2.0633 acc_train: 0.0786 loss_val: 2.0723 acc_val: 0.0833 time: 0.1133s\n",
      "Epoch: 0002 loss_train: 2.0450 acc_train: 0.0786 loss_val: 2.0569 acc_val: 0.0833 time: 0.0070s\n",
      "Epoch: 0003 loss_train: 2.0259 acc_train: 0.0786 loss_val: 2.0421 acc_val: 0.0833 time: 0.0080s\n",
      "Epoch: 0004 loss_train: 2.0108 acc_train: 0.1071 loss_val: 2.0279 acc_val: 0.1467 time: 0.0090s\n",
      "Epoch: 0005 loss_train: 1.9966 acc_train: 0.1714 loss_val: 2.0145 acc_val: 0.1833 time: 0.0080s\n",
      "Epoch: 0006 loss_train: 1.9816 acc_train: 0.2143 loss_val: 2.0018 acc_val: 0.1567 time: 0.0070s\n",
      "Epoch: 0007 loss_train: 1.9709 acc_train: 0.2071 loss_val: 1.9895 acc_val: 0.1567 time: 0.0080s\n",
      "Epoch: 0008 loss_train: 1.9559 acc_train: 0.2071 loss_val: 1.9775 acc_val: 0.1567 time: 0.0070s\n",
      "Epoch: 0009 loss_train: 1.9384 acc_train: 0.2000 loss_val: 1.9655 acc_val: 0.1567 time: 0.0080s\n",
      "Epoch: 0010 loss_train: 1.9302 acc_train: 0.2000 loss_val: 1.9534 acc_val: 0.1567 time: 0.0060s\n",
      "Epoch: 0011 loss_train: 1.9235 acc_train: 0.2000 loss_val: 1.9409 acc_val: 0.1567 time: 0.0070s\n",
      "Epoch: 0012 loss_train: 1.8952 acc_train: 0.2000 loss_val: 1.9278 acc_val: 0.1567 time: 0.0070s\n",
      "Epoch: 0013 loss_train: 1.8859 acc_train: 0.2000 loss_val: 1.9142 acc_val: 0.1567 time: 0.0060s\n",
      "Epoch: 0014 loss_train: 1.8734 acc_train: 0.2000 loss_val: 1.9003 acc_val: 0.1567 time: 0.0070s\n",
      "Epoch: 0015 loss_train: 1.8639 acc_train: 0.2000 loss_val: 1.8860 acc_val: 0.1567 time: 0.0060s\n",
      "Epoch: 0016 loss_train: 1.8542 acc_train: 0.2000 loss_val: 1.8715 acc_val: 0.1567 time: 0.0070s\n",
      "Epoch: 0017 loss_train: 1.8407 acc_train: 0.2000 loss_val: 1.8567 acc_val: 0.1567 time: 0.0070s\n",
      "Epoch: 0018 loss_train: 1.8025 acc_train: 0.2000 loss_val: 1.8417 acc_val: 0.1567 time: 0.0060s\n",
      "Epoch: 0019 loss_train: 1.7928 acc_train: 0.2000 loss_val: 1.8265 acc_val: 0.1567 time: 0.0060s\n",
      "Epoch: 0020 loss_train: 1.8045 acc_train: 0.2071 loss_val: 1.8114 acc_val: 0.1567 time: 0.0070s\n",
      "Epoch: 0021 loss_train: 1.7793 acc_train: 0.2143 loss_val: 1.7965 acc_val: 0.1567 time: 0.0070s\n",
      "Epoch: 0022 loss_train: 1.7506 acc_train: 0.2643 loss_val: 1.7816 acc_val: 0.1667 time: 0.0070s\n",
      "Epoch: 0023 loss_train: 1.7532 acc_train: 0.2429 loss_val: 1.7672 acc_val: 0.4467 time: 0.0070s\n",
      "Epoch: 0024 loss_train: 1.7436 acc_train: 0.4071 loss_val: 1.7533 acc_val: 0.3867 time: 0.0070s\n",
      "Epoch: 0025 loss_train: 1.7434 acc_train: 0.3786 loss_val: 1.7401 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0026 loss_train: 1.7356 acc_train: 0.3143 loss_val: 1.7277 acc_val: 0.3500 time: 0.0080s\n",
      "Epoch: 0027 loss_train: 1.7204 acc_train: 0.2857 loss_val: 1.7161 acc_val: 0.3500 time: 0.0080s\n",
      "Epoch: 0028 loss_train: 1.6676 acc_train: 0.3286 loss_val: 1.7051 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0029 loss_train: 1.6746 acc_train: 0.3071 loss_val: 1.6948 acc_val: 0.3500 time: 0.0080s\n",
      "Epoch: 0030 loss_train: 1.6733 acc_train: 0.3143 loss_val: 1.6852 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0031 loss_train: 1.6675 acc_train: 0.3000 loss_val: 1.6763 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0032 loss_train: 1.6678 acc_train: 0.3000 loss_val: 1.6679 acc_val: 0.3500 time: 0.0199s\n",
      "Epoch: 0033 loss_train: 1.6561 acc_train: 0.2929 loss_val: 1.6597 acc_val: 0.3500 time: 0.0189s\n",
      "Epoch: 0034 loss_train: 1.6593 acc_train: 0.2929 loss_val: 1.6517 acc_val: 0.3500 time: 0.0080s\n",
      "Epoch: 0035 loss_train: 1.6204 acc_train: 0.2857 loss_val: 1.6437 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0036 loss_train: 1.6152 acc_train: 0.3000 loss_val: 1.6358 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0037 loss_train: 1.5935 acc_train: 0.3000 loss_val: 1.6277 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0038 loss_train: 1.5641 acc_train: 0.3143 loss_val: 1.6196 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0039 loss_train: 1.6042 acc_train: 0.3286 loss_val: 1.6114 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0040 loss_train: 1.5562 acc_train: 0.3214 loss_val: 1.6030 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0041 loss_train: 1.5525 acc_train: 0.3357 loss_val: 1.5944 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0042 loss_train: 1.5487 acc_train: 0.3500 loss_val: 1.5855 acc_val: 0.3467 time: 0.0070s\n",
      "Epoch: 0043 loss_train: 1.5445 acc_train: 0.3429 loss_val: 1.5763 acc_val: 0.3600 time: 0.0070s\n",
      "Epoch: 0044 loss_train: 1.5224 acc_train: 0.3929 loss_val: 1.5669 acc_val: 0.3600 time: 0.0070s\n",
      "Epoch: 0045 loss_train: 1.5412 acc_train: 0.3500 loss_val: 1.5575 acc_val: 0.3700 time: 0.0080s\n",
      "Epoch: 0046 loss_train: 1.5008 acc_train: 0.4714 loss_val: 1.5478 acc_val: 0.3800 time: 0.0080s\n",
      "Epoch: 0047 loss_train: 1.5077 acc_train: 0.4143 loss_val: 1.5377 acc_val: 0.3900 time: 0.0080s\n",
      "Epoch: 0048 loss_train: 1.4487 acc_train: 0.4857 loss_val: 1.5272 acc_val: 0.4067 time: 0.0090s\n",
      "Epoch: 0049 loss_train: 1.4414 acc_train: 0.5000 loss_val: 1.5159 acc_val: 0.4200 time: 0.0100s\n",
      "Epoch: 0050 loss_train: 1.4273 acc_train: 0.5286 loss_val: 1.5041 acc_val: 0.4200 time: 0.0079s\n",
      "Epoch: 0051 loss_train: 1.4163 acc_train: 0.5071 loss_val: 1.4920 acc_val: 0.4233 time: 0.0070s\n",
      "Epoch: 0052 loss_train: 1.4077 acc_train: 0.4714 loss_val: 1.4798 acc_val: 0.4233 time: 0.0070s\n",
      "Epoch: 0053 loss_train: 1.3652 acc_train: 0.4857 loss_val: 1.4672 acc_val: 0.4267 time: 0.0079s\n",
      "Epoch: 0054 loss_train: 1.3408 acc_train: 0.5357 loss_val: 1.4543 acc_val: 0.4333 time: 0.0080s\n",
      "Epoch: 0055 loss_train: 1.3429 acc_train: 0.5286 loss_val: 1.4415 acc_val: 0.4400 time: 0.0070s\n",
      "Epoch: 0056 loss_train: 1.3386 acc_train: 0.5929 loss_val: 1.4290 acc_val: 0.4533 time: 0.0070s\n",
      "Epoch: 0057 loss_train: 1.3016 acc_train: 0.5500 loss_val: 1.4165 acc_val: 0.4733 time: 0.0060s\n",
      "Epoch: 0058 loss_train: 1.2685 acc_train: 0.5929 loss_val: 1.4042 acc_val: 0.5067 time: 0.0080s\n",
      "Epoch: 0059 loss_train: 1.3005 acc_train: 0.5929 loss_val: 1.3918 acc_val: 0.5267 time: 0.0070s\n",
      "Epoch: 0060 loss_train: 1.2526 acc_train: 0.6071 loss_val: 1.3793 acc_val: 0.5533 time: 0.0070s\n",
      "Epoch: 0061 loss_train: 1.2321 acc_train: 0.6357 loss_val: 1.3664 acc_val: 0.5700 time: 0.0070s\n",
      "Epoch: 0062 loss_train: 1.2263 acc_train: 0.6214 loss_val: 1.3537 acc_val: 0.6033 time: 0.0060s\n",
      "Epoch: 0063 loss_train: 1.2297 acc_train: 0.6571 loss_val: 1.3410 acc_val: 0.6300 time: 0.0080s\n",
      "Epoch: 0064 loss_train: 1.1951 acc_train: 0.6714 loss_val: 1.3283 acc_val: 0.6367 time: 0.0070s\n",
      "Epoch: 0065 loss_train: 1.1713 acc_train: 0.6786 loss_val: 1.3153 acc_val: 0.6433 time: 0.0080s\n",
      "Epoch: 0066 loss_train: 1.1570 acc_train: 0.7143 loss_val: 1.3023 acc_val: 0.6567 time: 0.0060s\n",
      "Epoch: 0067 loss_train: 1.1667 acc_train: 0.7357 loss_val: 1.2892 acc_val: 0.6667 time: 0.0070s\n",
      "Epoch: 0068 loss_train: 1.1328 acc_train: 0.7071 loss_val: 1.2761 acc_val: 0.6767 time: 0.0070s\n",
      "Epoch: 0069 loss_train: 1.1428 acc_train: 0.7286 loss_val: 1.2631 acc_val: 0.6833 time: 0.0060s\n",
      "Epoch: 0070 loss_train: 1.1384 acc_train: 0.7214 loss_val: 1.2500 acc_val: 0.6933 time: 0.0070s\n",
      "Epoch: 0071 loss_train: 1.0943 acc_train: 0.7429 loss_val: 1.2370 acc_val: 0.7033 time: 0.0060s\n",
      "Epoch: 0072 loss_train: 1.0907 acc_train: 0.7714 loss_val: 1.2241 acc_val: 0.7067 time: 0.0080s\n",
      "Epoch: 0073 loss_train: 1.0791 acc_train: 0.7357 loss_val: 1.2115 acc_val: 0.7133 time: 0.0080s\n",
      "Epoch: 0074 loss_train: 1.0613 acc_train: 0.7643 loss_val: 1.1996 acc_val: 0.7167 time: 0.0080s\n",
      "Epoch: 0075 loss_train: 1.0349 acc_train: 0.7571 loss_val: 1.1879 acc_val: 0.7233 time: 0.0070s\n",
      "Epoch: 0076 loss_train: 1.0771 acc_train: 0.7857 loss_val: 1.1765 acc_val: 0.7333 time: 0.0080s\n",
      "Epoch: 0077 loss_train: 0.9991 acc_train: 0.7714 loss_val: 1.1650 acc_val: 0.7333 time: 0.0070s\n",
      "Epoch: 0078 loss_train: 0.9785 acc_train: 0.8000 loss_val: 1.1539 acc_val: 0.7500 time: 0.0069s\n",
      "Epoch: 0079 loss_train: 0.9582 acc_train: 0.8000 loss_val: 1.1430 acc_val: 0.7567 time: 0.0080s\n",
      "Epoch: 0080 loss_train: 0.9495 acc_train: 0.8357 loss_val: 1.1319 acc_val: 0.7600 time: 0.0070s\n",
      "Epoch: 0081 loss_train: 0.9187 acc_train: 0.8071 loss_val: 1.1211 acc_val: 0.7633 time: 0.0080s\n",
      "Epoch: 0082 loss_train: 0.9237 acc_train: 0.8000 loss_val: 1.1104 acc_val: 0.7633 time: 0.0070s\n",
      "Epoch: 0083 loss_train: 0.9012 acc_train: 0.8000 loss_val: 1.0999 acc_val: 0.7667 time: 0.0070s\n",
      "Epoch: 0084 loss_train: 0.9037 acc_train: 0.8143 loss_val: 1.0895 acc_val: 0.7700 time: 0.0070s\n",
      "Epoch: 0085 loss_train: 0.9237 acc_train: 0.8429 loss_val: 1.0792 acc_val: 0.7700 time: 0.0070s\n",
      "Epoch: 0086 loss_train: 0.8648 acc_train: 0.8357 loss_val: 1.0693 acc_val: 0.7700 time: 0.0070s\n",
      "Epoch: 0087 loss_train: 0.8688 acc_train: 0.8071 loss_val: 1.0596 acc_val: 0.7767 time: 0.0070s\n",
      "Epoch: 0088 loss_train: 0.8545 acc_train: 0.8214 loss_val: 1.0503 acc_val: 0.7800 time: 0.0070s\n",
      "Epoch: 0089 loss_train: 0.8775 acc_train: 0.8357 loss_val: 1.0410 acc_val: 0.7867 time: 0.0070s\n",
      "Epoch: 0090 loss_train: 0.8424 acc_train: 0.8286 loss_val: 1.0321 acc_val: 0.7933 time: 0.0070s\n",
      "Epoch: 0091 loss_train: 0.8447 acc_train: 0.8357 loss_val: 1.0230 acc_val: 0.7967 time: 0.0080s\n",
      "Epoch: 0092 loss_train: 0.8349 acc_train: 0.8000 loss_val: 1.0148 acc_val: 0.8000 time: 0.0070s\n",
      "Epoch: 0093 loss_train: 0.8424 acc_train: 0.8500 loss_val: 1.0066 acc_val: 0.8000 time: 0.0090s\n",
      "Epoch: 0094 loss_train: 0.8289 acc_train: 0.8214 loss_val: 0.9987 acc_val: 0.8033 time: 0.0080s\n",
      "Epoch: 0095 loss_train: 0.7864 acc_train: 0.8286 loss_val: 0.9911 acc_val: 0.7900 time: 0.0070s\n",
      "Epoch: 0096 loss_train: 0.7535 acc_train: 0.9071 loss_val: 0.9833 acc_val: 0.7900 time: 0.0070s\n",
      "Epoch: 0097 loss_train: 0.7231 acc_train: 0.8643 loss_val: 0.9756 acc_val: 0.7967 time: 0.0070s\n",
      "Epoch: 0098 loss_train: 0.7681 acc_train: 0.8500 loss_val: 0.9685 acc_val: 0.7967 time: 0.0070s\n",
      "Epoch: 0099 loss_train: 0.7893 acc_train: 0.8571 loss_val: 0.9620 acc_val: 0.8000 time: 0.0070s\n",
      "Epoch: 0100 loss_train: 0.7633 acc_train: 0.8643 loss_val: 0.9552 acc_val: 0.8067 time: 0.0090s\n",
      "Epoch: 0101 loss_train: 0.7234 acc_train: 0.8857 loss_val: 0.9484 acc_val: 0.8100 time: 0.0090s\n",
      "Epoch: 0102 loss_train: 0.7892 acc_train: 0.8714 loss_val: 0.9416 acc_val: 0.8100 time: 0.0080s\n",
      "Epoch: 0103 loss_train: 0.6967 acc_train: 0.8643 loss_val: 0.9352 acc_val: 0.8033 time: 0.0080s\n",
      "Epoch: 0104 loss_train: 0.7009 acc_train: 0.8643 loss_val: 0.9288 acc_val: 0.8100 time: 0.0070s\n",
      "Epoch: 0105 loss_train: 0.7279 acc_train: 0.8929 loss_val: 0.9221 acc_val: 0.8100 time: 0.0080s\n",
      "Epoch: 0106 loss_train: 0.7221 acc_train: 0.8571 loss_val: 0.9157 acc_val: 0.8100 time: 0.0080s\n",
      "Epoch: 0107 loss_train: 0.6791 acc_train: 0.8786 loss_val: 0.9094 acc_val: 0.8100 time: 0.0080s\n",
      "Epoch: 0108 loss_train: 0.6920 acc_train: 0.8643 loss_val: 0.9044 acc_val: 0.8100 time: 0.0070s\n",
      "Epoch: 0109 loss_train: 0.6749 acc_train: 0.8643 loss_val: 0.8993 acc_val: 0.8067 time: 0.0070s\n",
      "Epoch: 0110 loss_train: 0.6808 acc_train: 0.8857 loss_val: 0.8938 acc_val: 0.8033 time: 0.0070s\n",
      "Epoch: 0111 loss_train: 0.6851 acc_train: 0.8786 loss_val: 0.8888 acc_val: 0.8067 time: 0.0070s\n",
      "Epoch: 0112 loss_train: 0.6712 acc_train: 0.8571 loss_val: 0.8838 acc_val: 0.8067 time: 0.0060s\n",
      "Epoch: 0113 loss_train: 0.6475 acc_train: 0.8929 loss_val: 0.8793 acc_val: 0.8100 time: 0.0070s\n",
      "Epoch: 0114 loss_train: 0.6790 acc_train: 0.8786 loss_val: 0.8748 acc_val: 0.8100 time: 0.0070s\n",
      "Epoch: 0115 loss_train: 0.6229 acc_train: 0.8929 loss_val: 0.8703 acc_val: 0.8133 time: 0.0060s\n",
      "Epoch: 0116 loss_train: 0.6349 acc_train: 0.8786 loss_val: 0.8661 acc_val: 0.8133 time: 0.0090s\n",
      "Epoch: 0117 loss_train: 0.6895 acc_train: 0.9000 loss_val: 0.8618 acc_val: 0.8133 time: 0.0090s\n",
      "Epoch: 0118 loss_train: 0.6146 acc_train: 0.8929 loss_val: 0.8565 acc_val: 0.8267 time: 0.0070s\n",
      "Epoch: 0119 loss_train: 0.5792 acc_train: 0.9000 loss_val: 0.8508 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0120 loss_train: 0.6337 acc_train: 0.9000 loss_val: 0.8461 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0121 loss_train: 0.6015 acc_train: 0.8929 loss_val: 0.8419 acc_val: 0.8167 time: 0.0060s\n",
      "Epoch: 0122 loss_train: 0.5793 acc_train: 0.9214 loss_val: 0.8388 acc_val: 0.8100 time: 0.0100s\n",
      "Epoch: 0123 loss_train: 0.5932 acc_train: 0.9000 loss_val: 0.8360 acc_val: 0.8100 time: 0.0080s\n",
      "Epoch: 0124 loss_train: 0.5676 acc_train: 0.9000 loss_val: 0.8330 acc_val: 0.8100 time: 0.0090s\n",
      "Epoch: 0125 loss_train: 0.6412 acc_train: 0.8786 loss_val: 0.8295 acc_val: 0.8100 time: 0.0070s\n",
      "Epoch: 0126 loss_train: 0.5515 acc_train: 0.9214 loss_val: 0.8259 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0127 loss_train: 0.5814 acc_train: 0.9143 loss_val: 0.8231 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0128 loss_train: 0.6060 acc_train: 0.8929 loss_val: 0.8202 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0129 loss_train: 0.5820 acc_train: 0.9071 loss_val: 0.8173 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0130 loss_train: 0.5625 acc_train: 0.9000 loss_val: 0.8144 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0131 loss_train: 0.6034 acc_train: 0.9214 loss_val: 0.8116 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0132 loss_train: 0.5571 acc_train: 0.9143 loss_val: 0.8082 acc_val: 0.8167 time: 0.0060s\n",
      "Epoch: 0133 loss_train: 0.5800 acc_train: 0.9357 loss_val: 0.8048 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0134 loss_train: 0.5322 acc_train: 0.9500 loss_val: 0.8011 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0135 loss_train: 0.5579 acc_train: 0.9357 loss_val: 0.7976 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0136 loss_train: 0.5585 acc_train: 0.9214 loss_val: 0.7941 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0137 loss_train: 0.5513 acc_train: 0.9286 loss_val: 0.7913 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0138 loss_train: 0.5287 acc_train: 0.9214 loss_val: 0.7876 acc_val: 0.8233 time: 0.0060s\n",
      "Epoch: 0139 loss_train: 0.5391 acc_train: 0.9000 loss_val: 0.7847 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0140 loss_train: 0.5313 acc_train: 0.9357 loss_val: 0.7819 acc_val: 0.8167 time: 0.0060s\n",
      "Epoch: 0141 loss_train: 0.5201 acc_train: 0.9071 loss_val: 0.7789 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0142 loss_train: 0.5149 acc_train: 0.9000 loss_val: 0.7749 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0143 loss_train: 0.5452 acc_train: 0.9143 loss_val: 0.7708 acc_val: 0.8167 time: 0.0080s\n",
      "Epoch: 0144 loss_train: 0.5710 acc_train: 0.8929 loss_val: 0.7670 acc_val: 0.8167 time: 0.0060s\n",
      "Epoch: 0145 loss_train: 0.5022 acc_train: 0.9214 loss_val: 0.7634 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0146 loss_train: 0.5387 acc_train: 0.9429 loss_val: 0.7611 acc_val: 0.8200 time: 0.0060s\n",
      "Epoch: 0147 loss_train: 0.5106 acc_train: 0.9214 loss_val: 0.7588 acc_val: 0.8233 time: 0.0060s\n",
      "Epoch: 0148 loss_train: 0.5320 acc_train: 0.9000 loss_val: 0.7570 acc_val: 0.8233 time: 0.0100s\n",
      "Epoch: 0149 loss_train: 0.5037 acc_train: 0.9071 loss_val: 0.7553 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0150 loss_train: 0.4689 acc_train: 0.9357 loss_val: 0.7538 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0151 loss_train: 0.5200 acc_train: 0.9286 loss_val: 0.7523 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0152 loss_train: 0.5174 acc_train: 0.9286 loss_val: 0.7508 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0153 loss_train: 0.4898 acc_train: 0.9643 loss_val: 0.7489 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0154 loss_train: 0.4851 acc_train: 0.9214 loss_val: 0.7465 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0155 loss_train: 0.4835 acc_train: 0.9214 loss_val: 0.7435 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0156 loss_train: 0.4640 acc_train: 0.9357 loss_val: 0.7408 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0157 loss_train: 0.4766 acc_train: 0.9143 loss_val: 0.7384 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0158 loss_train: 0.4947 acc_train: 0.9143 loss_val: 0.7370 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0159 loss_train: 0.5032 acc_train: 0.9000 loss_val: 0.7357 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0160 loss_train: 0.5127 acc_train: 0.8929 loss_val: 0.7350 acc_val: 0.8267 time: 0.0060s\n",
      "Epoch: 0161 loss_train: 0.4575 acc_train: 0.9571 loss_val: 0.7339 acc_val: 0.8267 time: 0.0070s\n",
      "Epoch: 0162 loss_train: 0.4718 acc_train: 0.9429 loss_val: 0.7336 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0163 loss_train: 0.4977 acc_train: 0.9286 loss_val: 0.7322 acc_val: 0.8233 time: 0.0060s\n",
      "Epoch: 0164 loss_train: 0.4530 acc_train: 0.9143 loss_val: 0.7302 acc_val: 0.8233 time: 0.0060s\n",
      "Epoch: 0165 loss_train: 0.4776 acc_train: 0.9143 loss_val: 0.7282 acc_val: 0.8233 time: 0.0070s\n",
      "Epoch: 0166 loss_train: 0.4735 acc_train: 0.9429 loss_val: 0.7258 acc_val: 0.8233 time: 0.0070s\n",
      "Epoch: 0167 loss_train: 0.4421 acc_train: 0.9500 loss_val: 0.7235 acc_val: 0.8233 time: 0.0070s\n",
      "Epoch: 0168 loss_train: 0.4779 acc_train: 0.9071 loss_val: 0.7215 acc_val: 0.8167 time: 0.0060s\n",
      "Epoch: 0169 loss_train: 0.4531 acc_train: 0.9286 loss_val: 0.7194 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0170 loss_train: 0.4793 acc_train: 0.9214 loss_val: 0.7173 acc_val: 0.8267 time: 0.0060s\n",
      "Epoch: 0171 loss_train: 0.4425 acc_train: 0.9571 loss_val: 0.7158 acc_val: 0.8233 time: 0.0060s\n",
      "Epoch: 0172 loss_train: 0.4538 acc_train: 0.9286 loss_val: 0.7146 acc_val: 0.8200 time: 0.0060s\n",
      "Epoch: 0173 loss_train: 0.4518 acc_train: 0.9286 loss_val: 0.7134 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0174 loss_train: 0.4451 acc_train: 0.9000 loss_val: 0.7130 acc_val: 0.8267 time: 0.0080s\n",
      "Epoch: 0175 loss_train: 0.4523 acc_train: 0.9429 loss_val: 0.7130 acc_val: 0.8233 time: 0.0090s\n",
      "Epoch: 0176 loss_train: 0.4008 acc_train: 0.9357 loss_val: 0.7125 acc_val: 0.8233 time: 0.0070s\n",
      "Epoch: 0177 loss_train: 0.4476 acc_train: 0.9071 loss_val: 0.7108 acc_val: 0.8267 time: 0.0080s\n",
      "Epoch: 0178 loss_train: 0.4306 acc_train: 0.9643 loss_val: 0.7087 acc_val: 0.8233 time: 0.0070s\n",
      "Epoch: 0179 loss_train: 0.4313 acc_train: 0.9429 loss_val: 0.7059 acc_val: 0.8233 time: 0.0070s\n",
      "Epoch: 0180 loss_train: 0.3905 acc_train: 0.9571 loss_val: 0.7033 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0181 loss_train: 0.4350 acc_train: 0.9429 loss_val: 0.7016 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0182 loss_train: 0.4608 acc_train: 0.9214 loss_val: 0.7012 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0183 loss_train: 0.4159 acc_train: 0.9500 loss_val: 0.7013 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0184 loss_train: 0.4108 acc_train: 0.9429 loss_val: 0.7022 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0185 loss_train: 0.4349 acc_train: 0.9643 loss_val: 0.7017 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0186 loss_train: 0.4230 acc_train: 0.9571 loss_val: 0.7007 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0187 loss_train: 0.3910 acc_train: 0.9714 loss_val: 0.6987 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0188 loss_train: 0.4599 acc_train: 0.9214 loss_val: 0.6965 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0189 loss_train: 0.4005 acc_train: 0.9286 loss_val: 0.6957 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0190 loss_train: 0.3925 acc_train: 0.9429 loss_val: 0.6953 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0191 loss_train: 0.4143 acc_train: 0.9643 loss_val: 0.6942 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0192 loss_train: 0.3929 acc_train: 0.9714 loss_val: 0.6933 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0193 loss_train: 0.3600 acc_train: 0.9643 loss_val: 0.6918 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0194 loss_train: 0.3994 acc_train: 0.9429 loss_val: 0.6899 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0195 loss_train: 0.3963 acc_train: 0.9500 loss_val: 0.6878 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0196 loss_train: 0.3790 acc_train: 0.9643 loss_val: 0.6864 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0197 loss_train: 0.4055 acc_train: 0.9429 loss_val: 0.6857 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0198 loss_train: 0.3918 acc_train: 0.9500 loss_val: 0.6838 acc_val: 0.8167 time: 0.0080s\n",
      "Epoch: 0199 loss_train: 0.3564 acc_train: 0.9571 loss_val: 0.6827 acc_val: 0.8100 time: 0.0070s\n",
      "Epoch: 0200 loss_train: 0.3677 acc_train: 0.9714 loss_val: 0.6811 acc_val: 0.8167 time: 0.0090s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 1.6122s\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98fc41e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.7195 accuracy= 0.8330\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfcca19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
