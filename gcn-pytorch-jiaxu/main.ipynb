{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44582f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import argparse\n",
    "import scipy.sparse as sp\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75c034c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8910a2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f621b0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4899fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path=\"../data/cora/\", dataset=\"cora\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "    \n",
    "    print(\"Dataset Loaded!\")\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8156b5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n",
      "Dataset Loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62c2691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "epochs = 200\n",
    "weightDecay = 5e-4\n",
    "learningRate = 0.01\n",
    "hidden = 16\n",
    "dropout = 0.5\n",
    "\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=learningRate, weight_decay=weightDecay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16c9b592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate validation set performance separately,\n",
    "    # deactivates dropout during validation run.\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "256a6311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "008610b6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 0.1929 acc_train: 0.9714 loss_val: 0.6218 acc_val: 0.8300 time: 0.0109s\n",
      "Epoch: 0002 loss_train: 0.2084 acc_train: 0.9786 loss_val: 0.6231 acc_val: 0.8233 time: 0.0100s\n",
      "Epoch: 0003 loss_train: 0.2110 acc_train: 0.9786 loss_val: 0.6217 acc_val: 0.8267 time: 0.0090s\n",
      "Epoch: 0004 loss_train: 0.2105 acc_train: 0.9786 loss_val: 0.6180 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0005 loss_train: 0.1963 acc_train: 0.9714 loss_val: 0.6158 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0006 loss_train: 0.1935 acc_train: 0.9714 loss_val: 0.6144 acc_val: 0.8233 time: 0.0100s\n",
      "Epoch: 0007 loss_train: 0.2257 acc_train: 0.9714 loss_val: 0.6110 acc_val: 0.8233 time: 0.0090s\n",
      "Epoch: 0008 loss_train: 0.2088 acc_train: 0.9929 loss_val: 0.6090 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0009 loss_train: 0.2063 acc_train: 0.9857 loss_val: 0.6084 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0010 loss_train: 0.1856 acc_train: 0.9857 loss_val: 0.6086 acc_val: 0.8233 time: 0.0089s\n",
      "Epoch: 0011 loss_train: 0.1948 acc_train: 0.9857 loss_val: 0.6096 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0012 loss_train: 0.1948 acc_train: 0.9786 loss_val: 0.6123 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0013 loss_train: 0.2188 acc_train: 0.9714 loss_val: 0.6160 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0014 loss_train: 0.2050 acc_train: 0.9929 loss_val: 0.6205 acc_val: 0.8200 time: 0.0090s\n",
      "Epoch: 0015 loss_train: 0.2162 acc_train: 1.0000 loss_val: 0.6210 acc_val: 0.8167 time: 0.0090s\n",
      "Epoch: 0016 loss_train: 0.2499 acc_train: 0.9571 loss_val: 0.6158 acc_val: 0.8167 time: 0.0080s\n",
      "Epoch: 0017 loss_train: 0.2135 acc_train: 0.9714 loss_val: 0.6102 acc_val: 0.8167 time: 0.0080s\n",
      "Epoch: 0018 loss_train: 0.2161 acc_train: 0.9786 loss_val: 0.6068 acc_val: 0.8267 time: 0.0090s\n",
      "Epoch: 0019 loss_train: 0.2013 acc_train: 0.9643 loss_val: 0.6047 acc_val: 0.8267 time: 0.0080s\n",
      "Epoch: 0020 loss_train: 0.2355 acc_train: 0.9643 loss_val: 0.6038 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0021 loss_train: 0.2262 acc_train: 0.9643 loss_val: 0.6038 acc_val: 0.8233 time: 0.0090s\n",
      "Epoch: 0022 loss_train: 0.1960 acc_train: 0.9786 loss_val: 0.6045 acc_val: 0.8300 time: 0.0100s\n",
      "Epoch: 0023 loss_train: 0.2213 acc_train: 0.9786 loss_val: 0.6051 acc_val: 0.8333 time: 0.0090s\n",
      "Epoch: 0024 loss_train: 0.1933 acc_train: 0.9929 loss_val: 0.6063 acc_val: 0.8300 time: 0.0100s\n",
      "Epoch: 0025 loss_train: 0.2169 acc_train: 0.9857 loss_val: 0.6091 acc_val: 0.8233 time: 0.0090s\n",
      "Epoch: 0026 loss_train: 0.2133 acc_train: 0.9714 loss_val: 0.6124 acc_val: 0.8233 time: 0.0090s\n",
      "Epoch: 0027 loss_train: 0.2439 acc_train: 0.9786 loss_val: 0.6146 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0028 loss_train: 0.2005 acc_train: 0.9714 loss_val: 0.6163 acc_val: 0.8233 time: 0.0101s\n",
      "Epoch: 0029 loss_train: 0.2194 acc_train: 0.9786 loss_val: 0.6164 acc_val: 0.8267 time: 0.0099s\n",
      "Epoch: 0030 loss_train: 0.2232 acc_train: 0.9786 loss_val: 0.6177 acc_val: 0.8233 time: 0.0090s\n",
      "Epoch: 0031 loss_train: 0.2005 acc_train: 0.9786 loss_val: 0.6172 acc_val: 0.8200 time: 0.0090s\n",
      "Epoch: 0032 loss_train: 0.2102 acc_train: 0.9643 loss_val: 0.6164 acc_val: 0.8167 time: 0.0080s\n",
      "Epoch: 0033 loss_train: 0.1966 acc_train: 0.9857 loss_val: 0.6140 acc_val: 0.8133 time: 0.0090s\n",
      "Epoch: 0034 loss_train: 0.2376 acc_train: 0.9714 loss_val: 0.6131 acc_val: 0.8133 time: 0.0080s\n",
      "Epoch: 0035 loss_train: 0.1716 acc_train: 0.9857 loss_val: 0.6139 acc_val: 0.8167 time: 0.0080s\n",
      "Epoch: 0036 loss_train: 0.2078 acc_train: 0.9786 loss_val: 0.6161 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0037 loss_train: 0.2021 acc_train: 0.9643 loss_val: 0.6176 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0038 loss_train: 0.1907 acc_train: 0.9786 loss_val: 0.6194 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0039 loss_train: 0.2181 acc_train: 0.9714 loss_val: 0.6209 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0040 loss_train: 0.1943 acc_train: 0.9857 loss_val: 0.6215 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0041 loss_train: 0.2275 acc_train: 0.9714 loss_val: 0.6209 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0042 loss_train: 0.1992 acc_train: 0.9786 loss_val: 0.6189 acc_val: 0.8200 time: 0.0071s\n",
      "Epoch: 0043 loss_train: 0.2384 acc_train: 0.9857 loss_val: 0.6162 acc_val: 0.8200 time: 0.0089s\n",
      "Epoch: 0044 loss_train: 0.2031 acc_train: 0.9857 loss_val: 0.6136 acc_val: 0.8167 time: 0.0090s\n",
      "Epoch: 0045 loss_train: 0.1948 acc_train: 0.9929 loss_val: 0.6108 acc_val: 0.8133 time: 0.0100s\n",
      "Epoch: 0046 loss_train: 0.1963 acc_train: 0.9929 loss_val: 0.6087 acc_val: 0.8200 time: 0.0090s\n",
      "Epoch: 0047 loss_train: 0.2084 acc_train: 0.9714 loss_val: 0.6081 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0048 loss_train: 0.1937 acc_train: 0.9714 loss_val: 0.6096 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0049 loss_train: 0.2191 acc_train: 0.9714 loss_val: 0.6144 acc_val: 0.8133 time: 0.0090s\n",
      "Epoch: 0050 loss_train: 0.2296 acc_train: 0.9571 loss_val: 0.6205 acc_val: 0.8133 time: 0.0090s\n",
      "Epoch: 0051 loss_train: 0.2253 acc_train: 0.9714 loss_val: 0.6276 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0052 loss_train: 0.2050 acc_train: 0.9786 loss_val: 0.6297 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0053 loss_train: 0.2191 acc_train: 0.9857 loss_val: 0.6289 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0054 loss_train: 0.2090 acc_train: 1.0000 loss_val: 0.6263 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0055 loss_train: 0.2336 acc_train: 0.9786 loss_val: 0.6223 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0056 loss_train: 0.2238 acc_train: 0.9714 loss_val: 0.6162 acc_val: 0.8267 time: 0.0080s\n",
      "Epoch: 0057 loss_train: 0.2125 acc_train: 0.9643 loss_val: 0.6098 acc_val: 0.8300 time: 0.0080s\n",
      "Epoch: 0058 loss_train: 0.2260 acc_train: 0.9714 loss_val: 0.6073 acc_val: 0.8267 time: 0.0090s\n",
      "Epoch: 0059 loss_train: 0.1577 acc_train: 0.9786 loss_val: 0.6065 acc_val: 0.8267 time: 0.0080s\n",
      "Epoch: 0060 loss_train: 0.1811 acc_train: 0.9714 loss_val: 0.6073 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0061 loss_train: 0.2200 acc_train: 0.9714 loss_val: 0.6108 acc_val: 0.8167 time: 0.0080s\n",
      "Epoch: 0062 loss_train: 0.2234 acc_train: 0.9714 loss_val: 0.6161 acc_val: 0.8133 time: 0.0090s\n",
      "Epoch: 0063 loss_train: 0.2199 acc_train: 0.9857 loss_val: 0.6219 acc_val: 0.8100 time: 0.0100s\n",
      "Epoch: 0064 loss_train: 0.1726 acc_train: 0.9786 loss_val: 0.6265 acc_val: 0.8100 time: 0.0090s\n",
      "Epoch: 0065 loss_train: 0.2139 acc_train: 0.9786 loss_val: 0.6259 acc_val: 0.8200 time: 0.0100s\n",
      "Epoch: 0066 loss_train: 0.1967 acc_train: 0.9929 loss_val: 0.6218 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0067 loss_train: 0.2082 acc_train: 0.9714 loss_val: 0.6201 acc_val: 0.8267 time: 0.0080s\n",
      "Epoch: 0068 loss_train: 0.2171 acc_train: 0.9714 loss_val: 0.6184 acc_val: 0.8267 time: 0.0100s\n",
      "Epoch: 0069 loss_train: 0.2277 acc_train: 0.9714 loss_val: 0.6179 acc_val: 0.8267 time: 0.0080s\n",
      "Epoch: 0070 loss_train: 0.1922 acc_train: 0.9857 loss_val: 0.6185 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0071 loss_train: 0.2127 acc_train: 0.9714 loss_val: 0.6200 acc_val: 0.8233 time: 0.0070s\n",
      "Epoch: 0072 loss_train: 0.2108 acc_train: 0.9714 loss_val: 0.6186 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0073 loss_train: 0.2077 acc_train: 0.9857 loss_val: 0.6171 acc_val: 0.8233 time: 0.0070s\n",
      "Epoch: 0074 loss_train: 0.2277 acc_train: 0.9500 loss_val: 0.6161 acc_val: 0.8233 time: 0.0070s\n",
      "Epoch: 0075 loss_train: 0.2264 acc_train: 0.9786 loss_val: 0.6161 acc_val: 0.8233 time: 0.0070s\n",
      "Epoch: 0076 loss_train: 0.1995 acc_train: 0.9786 loss_val: 0.6162 acc_val: 0.8300 time: 0.0070s\n",
      "Epoch: 0077 loss_train: 0.2002 acc_train: 0.9786 loss_val: 0.6170 acc_val: 0.8267 time: 0.0070s\n",
      "Epoch: 0078 loss_train: 0.1938 acc_train: 0.9857 loss_val: 0.6156 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0079 loss_train: 0.2369 acc_train: 0.9714 loss_val: 0.6151 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0080 loss_train: 0.2312 acc_train: 0.9857 loss_val: 0.6143 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0081 loss_train: 0.1998 acc_train: 0.9857 loss_val: 0.6142 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0082 loss_train: 0.2047 acc_train: 0.9786 loss_val: 0.6147 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0083 loss_train: 0.2016 acc_train: 0.9857 loss_val: 0.6154 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0084 loss_train: 0.2221 acc_train: 0.9643 loss_val: 0.6144 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0085 loss_train: 0.1821 acc_train: 0.9929 loss_val: 0.6149 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0086 loss_train: 0.2265 acc_train: 0.9786 loss_val: 0.6169 acc_val: 0.8233 time: 0.0070s\n",
      "Epoch: 0087 loss_train: 0.1933 acc_train: 0.9786 loss_val: 0.6161 acc_val: 0.8233 time: 0.0060s\n",
      "Epoch: 0088 loss_train: 0.1937 acc_train: 0.9929 loss_val: 0.6163 acc_val: 0.8233 time: 0.0070s\n",
      "Epoch: 0089 loss_train: 0.2092 acc_train: 0.9643 loss_val: 0.6182 acc_val: 0.8267 time: 0.0070s\n",
      "Epoch: 0090 loss_train: 0.2126 acc_train: 0.9786 loss_val: 0.6181 acc_val: 0.8267 time: 0.0070s\n",
      "Epoch: 0091 loss_train: 0.1968 acc_train: 0.9857 loss_val: 0.6176 acc_val: 0.8233 time: 0.0070s\n",
      "Epoch: 0092 loss_train: 0.2235 acc_train: 0.9429 loss_val: 0.6162 acc_val: 0.8300 time: 0.0070s\n",
      "Epoch: 0093 loss_train: 0.2315 acc_train: 0.9500 loss_val: 0.6150 acc_val: 0.8300 time: 0.0070s\n",
      "Epoch: 0094 loss_train: 0.2079 acc_train: 0.9500 loss_val: 0.6143 acc_val: 0.8267 time: 0.0100s\n",
      "Epoch: 0095 loss_train: 0.2201 acc_train: 0.9643 loss_val: 0.6155 acc_val: 0.8300 time: 0.0070s\n",
      "Epoch: 0096 loss_train: 0.2229 acc_train: 0.9857 loss_val: 0.6174 acc_val: 0.8300 time: 0.0070s\n",
      "Epoch: 0097 loss_train: 0.2273 acc_train: 0.9786 loss_val: 0.6192 acc_val: 0.8267 time: 0.0070s\n",
      "Epoch: 0098 loss_train: 0.2410 acc_train: 0.9786 loss_val: 0.6208 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0099 loss_train: 0.2019 acc_train: 0.9786 loss_val: 0.6210 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0100 loss_train: 0.2304 acc_train: 0.9786 loss_val: 0.6232 acc_val: 0.8167 time: 0.0090s\n",
      "Epoch: 0101 loss_train: 0.1905 acc_train: 0.9857 loss_val: 0.6241 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0102 loss_train: 0.2149 acc_train: 0.9714 loss_val: 0.6214 acc_val: 0.8167 time: 0.0080s\n",
      "Epoch: 0103 loss_train: 0.2158 acc_train: 0.9643 loss_val: 0.6176 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0104 loss_train: 0.2249 acc_train: 0.9643 loss_val: 0.6131 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0105 loss_train: 0.2024 acc_train: 0.9857 loss_val: 0.6107 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0106 loss_train: 0.1797 acc_train: 0.9857 loss_val: 0.6089 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0107 loss_train: 0.1944 acc_train: 0.9857 loss_val: 0.6091 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0108 loss_train: 0.2128 acc_train: 0.9571 loss_val: 0.6109 acc_val: 0.8233 time: 0.0070s\n",
      "Epoch: 0109 loss_train: 0.2198 acc_train: 0.9714 loss_val: 0.6123 acc_val: 0.8233 time: 0.0070s\n",
      "Epoch: 0110 loss_train: 0.2290 acc_train: 0.9714 loss_val: 0.6127 acc_val: 0.8233 time: 0.0070s\n",
      "Epoch: 0111 loss_train: 0.2079 acc_train: 0.9929 loss_val: 0.6128 acc_val: 0.8233 time: 0.0070s\n",
      "Epoch: 0112 loss_train: 0.1987 acc_train: 0.9643 loss_val: 0.6142 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0113 loss_train: 0.1994 acc_train: 0.9643 loss_val: 0.6140 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0114 loss_train: 0.2160 acc_train: 0.9643 loss_val: 0.6139 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0115 loss_train: 0.2292 acc_train: 0.9643 loss_val: 0.6159 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0116 loss_train: 0.2078 acc_train: 0.9929 loss_val: 0.6191 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0117 loss_train: 0.2067 acc_train: 0.9714 loss_val: 0.6220 acc_val: 0.8133 time: 0.0080s\n",
      "Epoch: 0118 loss_train: 0.2176 acc_train: 0.9714 loss_val: 0.6220 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0119 loss_train: 0.2246 acc_train: 0.9786 loss_val: 0.6188 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0120 loss_train: 0.2117 acc_train: 0.9571 loss_val: 0.6148 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0121 loss_train: 0.2139 acc_train: 0.9643 loss_val: 0.6123 acc_val: 0.8167 time: 0.0080s\n",
      "Epoch: 0122 loss_train: 0.2405 acc_train: 0.9786 loss_val: 0.6098 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0123 loss_train: 0.1840 acc_train: 0.9857 loss_val: 0.6094 acc_val: 0.8233 time: 0.0070s\n",
      "Epoch: 0124 loss_train: 0.2362 acc_train: 0.9643 loss_val: 0.6119 acc_val: 0.8267 time: 0.0070s\n",
      "Epoch: 0125 loss_train: 0.1979 acc_train: 0.9786 loss_val: 0.6152 acc_val: 0.8267 time: 0.0080s\n",
      "Epoch: 0126 loss_train: 0.2063 acc_train: 0.9786 loss_val: 0.6199 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0127 loss_train: 0.2225 acc_train: 0.9643 loss_val: 0.6219 acc_val: 0.8200 time: 0.0090s\n",
      "Epoch: 0128 loss_train: 0.2324 acc_train: 0.9571 loss_val: 0.6230 acc_val: 0.8267 time: 0.0080s\n",
      "Epoch: 0129 loss_train: 0.2116 acc_train: 0.9786 loss_val: 0.6256 acc_val: 0.8267 time: 0.0090s\n",
      "Epoch: 0130 loss_train: 0.2260 acc_train: 0.9786 loss_val: 0.6263 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0131 loss_train: 0.1998 acc_train: 0.9714 loss_val: 0.6256 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0132 loss_train: 0.2043 acc_train: 0.9786 loss_val: 0.6221 acc_val: 0.8200 time: 0.0090s\n",
      "Epoch: 0133 loss_train: 0.2110 acc_train: 0.9857 loss_val: 0.6186 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0134 loss_train: 0.1859 acc_train: 0.9929 loss_val: 0.6152 acc_val: 0.8167 time: 0.0080s\n",
      "Epoch: 0135 loss_train: 0.1998 acc_train: 0.9786 loss_val: 0.6124 acc_val: 0.8167 time: 0.0090s\n",
      "Epoch: 0136 loss_train: 0.1990 acc_train: 0.9929 loss_val: 0.6104 acc_val: 0.8233 time: 0.0090s\n",
      "Epoch: 0137 loss_train: 0.1880 acc_train: 0.9929 loss_val: 0.6093 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0138 loss_train: 0.1956 acc_train: 0.9786 loss_val: 0.6079 acc_val: 0.8267 time: 0.0100s\n",
      "Epoch: 0139 loss_train: 0.1974 acc_train: 0.9786 loss_val: 0.6080 acc_val: 0.8267 time: 0.0080s\n",
      "Epoch: 0140 loss_train: 0.2359 acc_train: 0.9714 loss_val: 0.6095 acc_val: 0.8267 time: 0.0080s\n",
      "Epoch: 0141 loss_train: 0.2189 acc_train: 0.9857 loss_val: 0.6116 acc_val: 0.8300 time: 0.0080s\n",
      "Epoch: 0142 loss_train: 0.1968 acc_train: 0.9643 loss_val: 0.6156 acc_val: 0.8267 time: 0.0080s\n",
      "Epoch: 0143 loss_train: 0.2179 acc_train: 0.9714 loss_val: 0.6201 acc_val: 0.8267 time: 0.0100s\n",
      "Epoch: 0144 loss_train: 0.2238 acc_train: 0.9500 loss_val: 0.6215 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0145 loss_train: 0.1966 acc_train: 0.9857 loss_val: 0.6204 acc_val: 0.8167 time: 0.0090s\n",
      "Epoch: 0146 loss_train: 0.2258 acc_train: 0.9786 loss_val: 0.6179 acc_val: 0.8133 time: 0.0090s\n",
      "Epoch: 0147 loss_train: 0.2126 acc_train: 0.9786 loss_val: 0.6158 acc_val: 0.8100 time: 0.0090s\n",
      "Epoch: 0148 loss_train: 0.2195 acc_train: 0.9643 loss_val: 0.6118 acc_val: 0.8100 time: 0.0070s\n",
      "Epoch: 0149 loss_train: 0.1921 acc_train: 0.9929 loss_val: 0.6099 acc_val: 0.8167 time: 0.0090s\n",
      "Epoch: 0150 loss_train: 0.2174 acc_train: 0.9714 loss_val: 0.6089 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0151 loss_train: 0.2405 acc_train: 0.9643 loss_val: 0.6090 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0152 loss_train: 0.2335 acc_train: 0.9714 loss_val: 0.6106 acc_val: 0.8267 time: 0.0080s\n",
      "Epoch: 0153 loss_train: 0.2146 acc_train: 0.9857 loss_val: 0.6134 acc_val: 0.8267 time: 0.0080s\n",
      "Epoch: 0154 loss_train: 0.2130 acc_train: 0.9643 loss_val: 0.6160 acc_val: 0.8267 time: 0.0080s\n",
      "Epoch: 0155 loss_train: 0.1737 acc_train: 0.9929 loss_val: 0.6184 acc_val: 0.8267 time: 0.0100s\n",
      "Epoch: 0156 loss_train: 0.2355 acc_train: 0.9714 loss_val: 0.6196 acc_val: 0.8233 time: 0.0090s\n",
      "Epoch: 0157 loss_train: 0.2027 acc_train: 0.9929 loss_val: 0.6196 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0158 loss_train: 0.2238 acc_train: 0.9643 loss_val: 0.6194 acc_val: 0.8233 time: 0.0090s\n",
      "Epoch: 0159 loss_train: 0.1869 acc_train: 0.9857 loss_val: 0.6187 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0160 loss_train: 0.2020 acc_train: 0.9857 loss_val: 0.6179 acc_val: 0.8267 time: 0.0090s\n",
      "Epoch: 0161 loss_train: 0.2141 acc_train: 0.9714 loss_val: 0.6151 acc_val: 0.8167 time: 0.0090s\n",
      "Epoch: 0162 loss_train: 0.2050 acc_train: 0.9786 loss_val: 0.6133 acc_val: 0.8133 time: 0.0080s\n",
      "Epoch: 0163 loss_train: 0.2073 acc_train: 0.9786 loss_val: 0.6116 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0164 loss_train: 0.1928 acc_train: 0.9786 loss_val: 0.6111 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0165 loss_train: 0.2073 acc_train: 0.9786 loss_val: 0.6121 acc_val: 0.8167 time: 0.0100s\n",
      "Epoch: 0166 loss_train: 0.2052 acc_train: 0.9786 loss_val: 0.6145 acc_val: 0.8200 time: 0.0090s\n",
      "Epoch: 0167 loss_train: 0.2034 acc_train: 0.9857 loss_val: 0.6162 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0168 loss_train: 0.2107 acc_train: 0.9786 loss_val: 0.6173 acc_val: 0.8267 time: 0.0080s\n",
      "Epoch: 0169 loss_train: 0.1879 acc_train: 0.9714 loss_val: 0.6189 acc_val: 0.8267 time: 0.0080s\n",
      "Epoch: 0170 loss_train: 0.1936 acc_train: 0.9857 loss_val: 0.6190 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0171 loss_train: 0.1824 acc_train: 0.9929 loss_val: 0.6183 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0172 loss_train: 0.1910 acc_train: 0.9786 loss_val: 0.6170 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0173 loss_train: 0.2005 acc_train: 0.9643 loss_val: 0.6159 acc_val: 0.8233 time: 0.0090s\n",
      "Epoch: 0174 loss_train: 0.2413 acc_train: 0.9643 loss_val: 0.6163 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0175 loss_train: 0.2289 acc_train: 0.9786 loss_val: 0.6165 acc_val: 0.8233 time: 0.0090s\n",
      "Epoch: 0176 loss_train: 0.2001 acc_train: 0.9643 loss_val: 0.6166 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0177 loss_train: 0.2181 acc_train: 0.9714 loss_val: 0.6165 acc_val: 0.8167 time: 0.0080s\n",
      "Epoch: 0178 loss_train: 0.2088 acc_train: 0.9857 loss_val: 0.6182 acc_val: 0.8133 time: 0.0080s\n",
      "Epoch: 0179 loss_train: 0.2162 acc_train: 0.9714 loss_val: 0.6206 acc_val: 0.8167 time: 0.0080s\n",
      "Epoch: 0180 loss_train: 0.1989 acc_train: 0.9857 loss_val: 0.6214 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0181 loss_train: 0.1982 acc_train: 0.9857 loss_val: 0.6202 acc_val: 0.8133 time: 0.0090s\n",
      "Epoch: 0182 loss_train: 0.1948 acc_train: 0.9857 loss_val: 0.6163 acc_val: 0.8167 time: 0.0080s\n",
      "Epoch: 0183 loss_train: 0.1945 acc_train: 0.9929 loss_val: 0.6124 acc_val: 0.8200 time: 0.0090s\n",
      "Epoch: 0184 loss_train: 0.2218 acc_train: 0.9714 loss_val: 0.6091 acc_val: 0.8300 time: 0.0080s\n",
      "Epoch: 0185 loss_train: 0.1904 acc_train: 0.9857 loss_val: 0.6085 acc_val: 0.8300 time: 0.0090s\n",
      "Epoch: 0186 loss_train: 0.2048 acc_train: 0.9857 loss_val: 0.6083 acc_val: 0.8267 time: 0.0090s\n",
      "Epoch: 0187 loss_train: 0.2099 acc_train: 0.9857 loss_val: 0.6095 acc_val: 0.8267 time: 0.0090s\n",
      "Epoch: 0188 loss_train: 0.2061 acc_train: 0.9786 loss_val: 0.6109 acc_val: 0.8267 time: 0.0090s\n",
      "Epoch: 0189 loss_train: 0.1977 acc_train: 0.9714 loss_val: 0.6131 acc_val: 0.8267 time: 0.0080s\n",
      "Epoch: 0190 loss_train: 0.2324 acc_train: 0.9643 loss_val: 0.6165 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0191 loss_train: 0.2235 acc_train: 0.9857 loss_val: 0.6201 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0192 loss_train: 0.1954 acc_train: 0.9786 loss_val: 0.6218 acc_val: 0.8200 time: 0.0090s\n",
      "Epoch: 0193 loss_train: 0.2500 acc_train: 0.9714 loss_val: 0.6193 acc_val: 0.8133 time: 0.0080s\n",
      "Epoch: 0194 loss_train: 0.1991 acc_train: 0.9714 loss_val: 0.6150 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0195 loss_train: 0.1845 acc_train: 0.9929 loss_val: 0.6114 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0196 loss_train: 0.1982 acc_train: 0.9786 loss_val: 0.6080 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0197 loss_train: 0.2020 acc_train: 0.9857 loss_val: 0.6053 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0198 loss_train: 0.1920 acc_train: 0.9929 loss_val: 0.6032 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0199 loss_train: 0.2085 acc_train: 0.9929 loss_val: 0.6037 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0200 loss_train: 0.2158 acc_train: 0.9643 loss_val: 0.6074 acc_val: 0.8167 time: 0.0070s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 1.6486s\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98fc41e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.5897 accuracy= 0.8390\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfcca19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
