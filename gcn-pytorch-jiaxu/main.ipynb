{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f32db5-33c5-4bfa-8e8f-87c27a93369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This model is currently not optimized for GPU!!!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44582f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import argparse\n",
    "import scipy.sparse as sp\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75c034c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(Module):\n",
    "    # GCN Layer\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else: \n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8910a2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f621b0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "def normalize(mx):\n",
    "    # Row-normalize sparse matrix\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    # Convert a scipy sparse matrix to a torch sparse tensor\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1895fe4a-5031-49ee-8215-b667cc98a41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(2708, 1435)\n",
      "<class 'numpy.ndarray'>\n",
      "(5429, 2)\n",
      "[[ 163  402]\n",
      " [ 163  659]\n",
      " [ 163 1696]\n",
      " ...\n",
      " [1887 2258]\n",
      " [1902 1887]\n",
      " [ 837 1686]]\n"
     ]
    }
   ],
   "source": [
    "cora_path = \"../data/cora/\"\n",
    "def test_dataset(path=cora_path, dataset=\"cora\"):\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset), dtype=np.dtype(str))\n",
    "    print(type(idx_features_labels)); print(idx_features_labels.shape); \n",
    "    # print(idx_features_labels)\n",
    "    \n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    print(type(edges_unordered)); print(edges_unordered.shape); \n",
    "    # print(edges_unordered)\n",
    "    # print(edges)\n",
    "    return None\n",
    "test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4899fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path=cora_path, dataset=\"cora\"):\n",
    "    # Load citation network dataset (cora only for now)\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "    \n",
    "    print(\"Dataset Loaded!\")\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8156b5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n",
      "Dataset Loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62c2691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "epochs = 200\n",
    "weightDecay = 5e-4\n",
    "learningRate = 0.01\n",
    "hidden = 16\n",
    "dropout = 0.5\n",
    "\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=learningRate, weight_decay=weightDecay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16c9b592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate validation set performance separately,\n",
    "    # deactivates dropout during validation run.\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "256a6311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "008610b6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9320 acc_train: 0.2857 loss_val: 1.8998 acc_val: 0.3500 time: 0.0737s\n",
      "Epoch: 0002 loss_train: 1.9203 acc_train: 0.2929 loss_val: 1.8888 acc_val: 0.3500 time: 0.0080s\n",
      "Epoch: 0003 loss_train: 1.9087 acc_train: 0.2929 loss_val: 1.8783 acc_val: 0.3500 time: 0.0080s\n",
      "Epoch: 0004 loss_train: 1.8973 acc_train: 0.2929 loss_val: 1.8683 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0005 loss_train: 1.8813 acc_train: 0.2929 loss_val: 1.8588 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0006 loss_train: 1.8770 acc_train: 0.2929 loss_val: 1.8498 acc_val: 0.3500 time: 0.0090s\n",
      "Epoch: 0007 loss_train: 1.8674 acc_train: 0.2929 loss_val: 1.8410 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0008 loss_train: 1.8496 acc_train: 0.2929 loss_val: 1.8324 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0009 loss_train: 1.8442 acc_train: 0.2929 loss_val: 1.8241 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0010 loss_train: 1.8493 acc_train: 0.2929 loss_val: 1.8162 acc_val: 0.3500 time: 0.0080s\n",
      "Epoch: 0011 loss_train: 1.8293 acc_train: 0.2929 loss_val: 1.8088 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0012 loss_train: 1.8262 acc_train: 0.2929 loss_val: 1.8019 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0013 loss_train: 1.8098 acc_train: 0.2929 loss_val: 1.7953 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0014 loss_train: 1.8191 acc_train: 0.2929 loss_val: 1.7891 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0015 loss_train: 1.7908 acc_train: 0.2929 loss_val: 1.7831 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0016 loss_train: 1.7989 acc_train: 0.2929 loss_val: 1.7775 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0017 loss_train: 1.7597 acc_train: 0.2929 loss_val: 1.7721 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0018 loss_train: 1.7867 acc_train: 0.2929 loss_val: 1.7669 acc_val: 0.3500 time: 0.0060s\n",
      "Epoch: 0019 loss_train: 1.7663 acc_train: 0.2929 loss_val: 1.7619 acc_val: 0.3500 time: 0.0090s\n",
      "Epoch: 0020 loss_train: 1.7673 acc_train: 0.2929 loss_val: 1.7570 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0021 loss_train: 1.7329 acc_train: 0.2929 loss_val: 1.7522 acc_val: 0.3500 time: 0.0080s\n",
      "Epoch: 0022 loss_train: 1.7337 acc_train: 0.2929 loss_val: 1.7473 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0023 loss_train: 1.7493 acc_train: 0.2929 loss_val: 1.7423 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0024 loss_train: 1.7335 acc_train: 0.2929 loss_val: 1.7369 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0025 loss_train: 1.7095 acc_train: 0.3071 loss_val: 1.7315 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0026 loss_train: 1.7180 acc_train: 0.2929 loss_val: 1.7260 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0027 loss_train: 1.7000 acc_train: 0.2929 loss_val: 1.7202 acc_val: 0.3500 time: 0.0080s\n",
      "Epoch: 0028 loss_train: 1.6932 acc_train: 0.2929 loss_val: 1.7144 acc_val: 0.3500 time: 0.0080s\n",
      "Epoch: 0029 loss_train: 1.6874 acc_train: 0.3000 loss_val: 1.7083 acc_val: 0.3500 time: 0.0080s\n",
      "Epoch: 0030 loss_train: 1.6720 acc_train: 0.3071 loss_val: 1.7019 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0031 loss_train: 1.6705 acc_train: 0.3214 loss_val: 1.6953 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0032 loss_train: 1.6815 acc_train: 0.3071 loss_val: 1.6883 acc_val: 0.3500 time: 0.0080s\n",
      "Epoch: 0033 loss_train: 1.6436 acc_train: 0.3286 loss_val: 1.6812 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0034 loss_train: 1.6557 acc_train: 0.3357 loss_val: 1.6739 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0035 loss_train: 1.6258 acc_train: 0.3286 loss_val: 1.6662 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0036 loss_train: 1.6301 acc_train: 0.3571 loss_val: 1.6583 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0037 loss_train: 1.6148 acc_train: 0.3643 loss_val: 1.6501 acc_val: 0.3533 time: 0.0070s\n",
      "Epoch: 0038 loss_train: 1.6087 acc_train: 0.3571 loss_val: 1.6415 acc_val: 0.3567 time: 0.0070s\n",
      "Epoch: 0039 loss_train: 1.5676 acc_train: 0.4000 loss_val: 1.6326 acc_val: 0.3633 time: 0.0070s\n",
      "Epoch: 0040 loss_train: 1.5691 acc_train: 0.3857 loss_val: 1.6235 acc_val: 0.3700 time: 0.0080s\n",
      "Epoch: 0041 loss_train: 1.5701 acc_train: 0.4000 loss_val: 1.6143 acc_val: 0.3700 time: 0.0070s\n",
      "Epoch: 0042 loss_train: 1.5374 acc_train: 0.4214 loss_val: 1.6049 acc_val: 0.3733 time: 0.0080s\n",
      "Epoch: 0043 loss_train: 1.5349 acc_train: 0.4214 loss_val: 1.5951 acc_val: 0.3833 time: 0.0140s\n",
      "Epoch: 0044 loss_train: 1.5190 acc_train: 0.4286 loss_val: 1.5848 acc_val: 0.3867 time: 0.0199s\n",
      "Epoch: 0045 loss_train: 1.5194 acc_train: 0.4571 loss_val: 1.5746 acc_val: 0.3833 time: 0.0100s\n",
      "Epoch: 0046 loss_train: 1.4980 acc_train: 0.4714 loss_val: 1.5642 acc_val: 0.3867 time: 0.0070s\n",
      "Epoch: 0047 loss_train: 1.4720 acc_train: 0.4571 loss_val: 1.5535 acc_val: 0.4000 time: 0.0070s\n",
      "Epoch: 0048 loss_train: 1.4512 acc_train: 0.4643 loss_val: 1.5427 acc_val: 0.4000 time: 0.0070s\n",
      "Epoch: 0049 loss_train: 1.4476 acc_train: 0.5000 loss_val: 1.5320 acc_val: 0.4133 time: 0.0070s\n",
      "Epoch: 0050 loss_train: 1.4487 acc_train: 0.4714 loss_val: 1.5214 acc_val: 0.4267 time: 0.0070s\n",
      "Epoch: 0051 loss_train: 1.4209 acc_train: 0.4929 loss_val: 1.5107 acc_val: 0.4500 time: 0.0060s\n",
      "Epoch: 0052 loss_train: 1.3905 acc_train: 0.5357 loss_val: 1.4998 acc_val: 0.4667 time: 0.0090s\n",
      "Epoch: 0053 loss_train: 1.3867 acc_train: 0.5571 loss_val: 1.4884 acc_val: 0.4700 time: 0.0060s\n",
      "Epoch: 0054 loss_train: 1.3598 acc_train: 0.5929 loss_val: 1.4765 acc_val: 0.4900 time: 0.0080s\n",
      "Epoch: 0055 loss_train: 1.3530 acc_train: 0.5429 loss_val: 1.4648 acc_val: 0.5100 time: 0.0070s\n",
      "Epoch: 0056 loss_train: 1.3628 acc_train: 0.5643 loss_val: 1.4526 acc_val: 0.5233 time: 0.0080s\n",
      "Epoch: 0057 loss_train: 1.3511 acc_train: 0.6143 loss_val: 1.4404 acc_val: 0.5367 time: 0.0070s\n",
      "Epoch: 0058 loss_train: 1.2942 acc_train: 0.6500 loss_val: 1.4281 acc_val: 0.5433 time: 0.0070s\n",
      "Epoch: 0059 loss_train: 1.2809 acc_train: 0.5929 loss_val: 1.4158 acc_val: 0.5467 time: 0.0070s\n",
      "Epoch: 0060 loss_train: 1.2598 acc_train: 0.6143 loss_val: 1.4037 acc_val: 0.5633 time: 0.0070s\n",
      "Epoch: 0061 loss_train: 1.2790 acc_train: 0.6000 loss_val: 1.3916 acc_val: 0.5733 time: 0.0070s\n",
      "Epoch: 0062 loss_train: 1.2516 acc_train: 0.5857 loss_val: 1.3794 acc_val: 0.5900 time: 0.0070s\n",
      "Epoch: 0063 loss_train: 1.2164 acc_train: 0.6714 loss_val: 1.3673 acc_val: 0.6067 time: 0.0070s\n",
      "Epoch: 0064 loss_train: 1.2274 acc_train: 0.5857 loss_val: 1.3554 acc_val: 0.6133 time: 0.0060s\n",
      "Epoch: 0065 loss_train: 1.2025 acc_train: 0.6429 loss_val: 1.3436 acc_val: 0.6200 time: 0.0060s\n",
      "Epoch: 0066 loss_train: 1.1921 acc_train: 0.6500 loss_val: 1.3316 acc_val: 0.6300 time: 0.0060s\n",
      "Epoch: 0067 loss_train: 1.1877 acc_train: 0.6571 loss_val: 1.3197 acc_val: 0.6333 time: 0.0070s\n",
      "Epoch: 0068 loss_train: 1.2063 acc_train: 0.6357 loss_val: 1.3078 acc_val: 0.6500 time: 0.0080s\n",
      "Epoch: 0069 loss_train: 1.1368 acc_train: 0.6714 loss_val: 1.2958 acc_val: 0.6500 time: 0.0070s\n",
      "Epoch: 0070 loss_train: 1.1277 acc_train: 0.7000 loss_val: 1.2843 acc_val: 0.6600 time: 0.0070s\n",
      "Epoch: 0071 loss_train: 1.1340 acc_train: 0.6786 loss_val: 1.2733 acc_val: 0.6700 time: 0.0070s\n",
      "Epoch: 0072 loss_train: 1.1245 acc_train: 0.6571 loss_val: 1.2626 acc_val: 0.6767 time: 0.0070s\n",
      "Epoch: 0073 loss_train: 1.0579 acc_train: 0.7214 loss_val: 1.2519 acc_val: 0.6767 time: 0.0070s\n",
      "Epoch: 0074 loss_train: 1.0497 acc_train: 0.7000 loss_val: 1.2415 acc_val: 0.6833 time: 0.0070s\n",
      "Epoch: 0075 loss_train: 1.0787 acc_train: 0.7643 loss_val: 1.2311 acc_val: 0.6900 time: 0.0070s\n",
      "Epoch: 0076 loss_train: 1.0199 acc_train: 0.8143 loss_val: 1.2208 acc_val: 0.7000 time: 0.0090s\n",
      "Epoch: 0077 loss_train: 1.0552 acc_train: 0.7286 loss_val: 1.2109 acc_val: 0.7167 time: 0.0080s\n",
      "Epoch: 0078 loss_train: 1.0509 acc_train: 0.7929 loss_val: 1.2014 acc_val: 0.7300 time: 0.0070s\n",
      "Epoch: 0079 loss_train: 1.0769 acc_train: 0.7214 loss_val: 1.1922 acc_val: 0.7467 time: 0.0070s\n",
      "Epoch: 0080 loss_train: 1.0036 acc_train: 0.7357 loss_val: 1.1828 acc_val: 0.7500 time: 0.0070s\n",
      "Epoch: 0081 loss_train: 1.0026 acc_train: 0.8214 loss_val: 1.1732 acc_val: 0.7500 time: 0.0070s\n",
      "Epoch: 0082 loss_train: 1.0212 acc_train: 0.7500 loss_val: 1.1643 acc_val: 0.7500 time: 0.0060s\n",
      "Epoch: 0083 loss_train: 0.9436 acc_train: 0.8071 loss_val: 1.1555 acc_val: 0.7533 time: 0.0070s\n",
      "Epoch: 0084 loss_train: 0.9469 acc_train: 0.8214 loss_val: 1.1469 acc_val: 0.7500 time: 0.0060s\n",
      "Epoch: 0085 loss_train: 0.9797 acc_train: 0.7643 loss_val: 1.1382 acc_val: 0.7500 time: 0.0070s\n",
      "Epoch: 0086 loss_train: 0.9420 acc_train: 0.8214 loss_val: 1.1296 acc_val: 0.7667 time: 0.0070s\n",
      "Epoch: 0087 loss_train: 0.9280 acc_train: 0.8429 loss_val: 1.1208 acc_val: 0.7633 time: 0.0070s\n",
      "Epoch: 0088 loss_train: 0.9046 acc_train: 0.8286 loss_val: 1.1122 acc_val: 0.7667 time: 0.0070s\n",
      "Epoch: 0089 loss_train: 0.8927 acc_train: 0.7857 loss_val: 1.1044 acc_val: 0.7633 time: 0.0060s\n",
      "Epoch: 0090 loss_train: 0.9015 acc_train: 0.8286 loss_val: 1.0974 acc_val: 0.7700 time: 0.0070s\n",
      "Epoch: 0091 loss_train: 0.9194 acc_train: 0.7714 loss_val: 1.0910 acc_val: 0.7700 time: 0.0070s\n",
      "Epoch: 0092 loss_train: 0.8793 acc_train: 0.7929 loss_val: 1.0852 acc_val: 0.7767 time: 0.0080s\n",
      "Epoch: 0093 loss_train: 0.8662 acc_train: 0.8357 loss_val: 1.0796 acc_val: 0.7800 time: 0.0070s\n",
      "Epoch: 0094 loss_train: 0.9041 acc_train: 0.8286 loss_val: 1.0738 acc_val: 0.7867 time: 0.0070s\n",
      "Epoch: 0095 loss_train: 0.8627 acc_train: 0.8429 loss_val: 1.0677 acc_val: 0.7867 time: 0.0070s\n",
      "Epoch: 0096 loss_train: 0.8501 acc_train: 0.8214 loss_val: 1.0614 acc_val: 0.7867 time: 0.0070s\n",
      "Epoch: 0097 loss_train: 0.8643 acc_train: 0.8143 loss_val: 1.0552 acc_val: 0.7867 time: 0.0080s\n",
      "Epoch: 0098 loss_train: 0.8290 acc_train: 0.8429 loss_val: 1.0492 acc_val: 0.7867 time: 0.0070s\n",
      "Epoch: 0099 loss_train: 0.9090 acc_train: 0.8071 loss_val: 1.0429 acc_val: 0.7867 time: 0.0080s\n",
      "Epoch: 0100 loss_train: 0.8340 acc_train: 0.8429 loss_val: 1.0353 acc_val: 0.7867 time: 0.0070s\n",
      "Epoch: 0101 loss_train: 0.8353 acc_train: 0.8429 loss_val: 1.0278 acc_val: 0.7900 time: 0.0090s\n",
      "Epoch: 0102 loss_train: 0.8036 acc_train: 0.8143 loss_val: 1.0208 acc_val: 0.7900 time: 0.0090s\n",
      "Epoch: 0103 loss_train: 0.8181 acc_train: 0.8286 loss_val: 1.0138 acc_val: 0.7833 time: 0.0090s\n",
      "Epoch: 0104 loss_train: 0.8416 acc_train: 0.8071 loss_val: 1.0078 acc_val: 0.7833 time: 0.0060s\n",
      "Epoch: 0105 loss_train: 0.8245 acc_train: 0.8214 loss_val: 1.0017 acc_val: 0.7833 time: 0.0080s\n",
      "Epoch: 0106 loss_train: 0.7778 acc_train: 0.8429 loss_val: 0.9962 acc_val: 0.7867 time: 0.0080s\n",
      "Epoch: 0107 loss_train: 0.7838 acc_train: 0.8286 loss_val: 0.9910 acc_val: 0.7900 time: 0.0080s\n",
      "Epoch: 0108 loss_train: 0.7547 acc_train: 0.8571 loss_val: 0.9864 acc_val: 0.7933 time: 0.0070s\n",
      "Epoch: 0109 loss_train: 0.7393 acc_train: 0.8786 loss_val: 0.9819 acc_val: 0.7933 time: 0.0070s\n",
      "Epoch: 0110 loss_train: 0.7527 acc_train: 0.8857 loss_val: 0.9771 acc_val: 0.7867 time: 0.0070s\n",
      "Epoch: 0111 loss_train: 0.7941 acc_train: 0.8429 loss_val: 0.9709 acc_val: 0.7900 time: 0.0070s\n",
      "Epoch: 0112 loss_train: 0.7084 acc_train: 0.8929 loss_val: 0.9638 acc_val: 0.7933 time: 0.0070s\n",
      "Epoch: 0113 loss_train: 0.7334 acc_train: 0.8643 loss_val: 0.9567 acc_val: 0.8033 time: 0.0070s\n",
      "Epoch: 0114 loss_train: 0.7044 acc_train: 0.8429 loss_val: 0.9503 acc_val: 0.8000 time: 0.0080s\n",
      "Epoch: 0115 loss_train: 0.7248 acc_train: 0.8786 loss_val: 0.9444 acc_val: 0.7967 time: 0.0070s\n",
      "Epoch: 0116 loss_train: 0.6833 acc_train: 0.8714 loss_val: 0.9391 acc_val: 0.8000 time: 0.0070s\n",
      "Epoch: 0117 loss_train: 0.6533 acc_train: 0.8929 loss_val: 0.9344 acc_val: 0.8033 time: 0.0070s\n",
      "Epoch: 0118 loss_train: 0.6992 acc_train: 0.8714 loss_val: 0.9290 acc_val: 0.8033 time: 0.0070s\n",
      "Epoch: 0119 loss_train: 0.7133 acc_train: 0.8714 loss_val: 0.9243 acc_val: 0.7967 time: 0.0070s\n",
      "Epoch: 0120 loss_train: 0.7075 acc_train: 0.8429 loss_val: 0.9194 acc_val: 0.7933 time: 0.0090s\n",
      "Epoch: 0121 loss_train: 0.6958 acc_train: 0.8714 loss_val: 0.9138 acc_val: 0.7933 time: 0.0070s\n",
      "Epoch: 0122 loss_train: 0.6734 acc_train: 0.9000 loss_val: 0.9083 acc_val: 0.7933 time: 0.0070s\n",
      "Epoch: 0123 loss_train: 0.6574 acc_train: 0.8786 loss_val: 0.9027 acc_val: 0.7933 time: 0.0070s\n",
      "Epoch: 0124 loss_train: 0.6888 acc_train: 0.8500 loss_val: 0.8981 acc_val: 0.8000 time: 0.0070s\n",
      "Epoch: 0125 loss_train: 0.6356 acc_train: 0.9143 loss_val: 0.8931 acc_val: 0.8000 time: 0.0060s\n",
      "Epoch: 0126 loss_train: 0.7518 acc_train: 0.8571 loss_val: 0.8884 acc_val: 0.8000 time: 0.0070s\n",
      "Epoch: 0127 loss_train: 0.5817 acc_train: 0.8429 loss_val: 0.8834 acc_val: 0.8033 time: 0.0070s\n",
      "Epoch: 0128 loss_train: 0.6949 acc_train: 0.8357 loss_val: 0.8792 acc_val: 0.8000 time: 0.0070s\n",
      "Epoch: 0129 loss_train: 0.6447 acc_train: 0.8643 loss_val: 0.8754 acc_val: 0.7967 time: 0.0090s\n",
      "Epoch: 0130 loss_train: 0.6591 acc_train: 0.8643 loss_val: 0.8709 acc_val: 0.7967 time: 0.0090s\n",
      "Epoch: 0131 loss_train: 0.6399 acc_train: 0.8643 loss_val: 0.8674 acc_val: 0.7967 time: 0.0070s\n",
      "Epoch: 0132 loss_train: 0.6315 acc_train: 0.9000 loss_val: 0.8647 acc_val: 0.7967 time: 0.0070s\n",
      "Epoch: 0133 loss_train: 0.5841 acc_train: 0.8857 loss_val: 0.8621 acc_val: 0.8000 time: 0.0080s\n",
      "Epoch: 0134 loss_train: 0.6022 acc_train: 0.8857 loss_val: 0.8599 acc_val: 0.7933 time: 0.0080s\n",
      "Epoch: 0135 loss_train: 0.6271 acc_train: 0.8643 loss_val: 0.8575 acc_val: 0.7933 time: 0.0070s\n",
      "Epoch: 0136 loss_train: 0.5960 acc_train: 0.8857 loss_val: 0.8548 acc_val: 0.7933 time: 0.0080s\n",
      "Epoch: 0137 loss_train: 0.6054 acc_train: 0.8786 loss_val: 0.8521 acc_val: 0.7933 time: 0.0070s\n",
      "Epoch: 0138 loss_train: 0.6050 acc_train: 0.8929 loss_val: 0.8485 acc_val: 0.7933 time: 0.0070s\n",
      "Epoch: 0139 loss_train: 0.5708 acc_train: 0.9000 loss_val: 0.8459 acc_val: 0.7933 time: 0.0080s\n",
      "Epoch: 0140 loss_train: 0.5998 acc_train: 0.8929 loss_val: 0.8425 acc_val: 0.7933 time: 0.0070s\n",
      "Epoch: 0141 loss_train: 0.6629 acc_train: 0.8857 loss_val: 0.8377 acc_val: 0.8000 time: 0.0070s\n",
      "Epoch: 0142 loss_train: 0.5415 acc_train: 0.9071 loss_val: 0.8334 acc_val: 0.8033 time: 0.0080s\n",
      "Epoch: 0143 loss_train: 0.5614 acc_train: 0.9143 loss_val: 0.8293 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0144 loss_train: 0.6141 acc_train: 0.8786 loss_val: 0.8259 acc_val: 0.8167 time: 0.0080s\n",
      "Epoch: 0145 loss_train: 0.5812 acc_train: 0.8857 loss_val: 0.8221 acc_val: 0.8067 time: 0.0080s\n",
      "Epoch: 0146 loss_train: 0.5888 acc_train: 0.8929 loss_val: 0.8190 acc_val: 0.8067 time: 0.0070s\n",
      "Epoch: 0147 loss_train: 0.5664 acc_train: 0.9143 loss_val: 0.8173 acc_val: 0.8067 time: 0.0080s\n",
      "Epoch: 0148 loss_train: 0.5907 acc_train: 0.8571 loss_val: 0.8166 acc_val: 0.8033 time: 0.0070s\n",
      "Epoch: 0149 loss_train: 0.5900 acc_train: 0.8643 loss_val: 0.8155 acc_val: 0.8033 time: 0.0080s\n",
      "Epoch: 0150 loss_train: 0.5971 acc_train: 0.9143 loss_val: 0.8137 acc_val: 0.8033 time: 0.0070s\n",
      "Epoch: 0151 loss_train: 0.5559 acc_train: 0.9143 loss_val: 0.8117 acc_val: 0.8100 time: 0.0080s\n",
      "Epoch: 0152 loss_train: 0.6208 acc_train: 0.8500 loss_val: 0.8097 acc_val: 0.8000 time: 0.0080s\n",
      "Epoch: 0153 loss_train: 0.5438 acc_train: 0.9143 loss_val: 0.8070 acc_val: 0.8033 time: 0.0080s\n",
      "Epoch: 0154 loss_train: 0.5751 acc_train: 0.8857 loss_val: 0.8053 acc_val: 0.8033 time: 0.0090s\n",
      "Epoch: 0155 loss_train: 0.5498 acc_train: 0.9214 loss_val: 0.8040 acc_val: 0.8033 time: 0.0070s\n",
      "Epoch: 0156 loss_train: 0.5694 acc_train: 0.8857 loss_val: 0.8029 acc_val: 0.8033 time: 0.0070s\n",
      "Epoch: 0157 loss_train: 0.5736 acc_train: 0.8786 loss_val: 0.8019 acc_val: 0.8033 time: 0.0070s\n",
      "Epoch: 0158 loss_train: 0.5779 acc_train: 0.8714 loss_val: 0.8017 acc_val: 0.8067 time: 0.0070s\n",
      "Epoch: 0159 loss_train: 0.5734 acc_train: 0.8857 loss_val: 0.8010 acc_val: 0.8033 time: 0.0070s\n",
      "Epoch: 0160 loss_train: 0.5459 acc_train: 0.9071 loss_val: 0.7993 acc_val: 0.8000 time: 0.0070s\n",
      "Epoch: 0161 loss_train: 0.5537 acc_train: 0.9214 loss_val: 0.7979 acc_val: 0.8000 time: 0.0080s\n",
      "Epoch: 0162 loss_train: 0.4993 acc_train: 0.9357 loss_val: 0.7953 acc_val: 0.8067 time: 0.0070s\n",
      "Epoch: 0163 loss_train: 0.4878 acc_train: 0.9571 loss_val: 0.7924 acc_val: 0.8067 time: 0.0070s\n",
      "Epoch: 0164 loss_train: 0.5374 acc_train: 0.9143 loss_val: 0.7885 acc_val: 0.8067 time: 0.0070s\n",
      "Epoch: 0165 loss_train: 0.5152 acc_train: 0.9143 loss_val: 0.7847 acc_val: 0.8100 time: 0.0060s\n",
      "Epoch: 0166 loss_train: 0.5544 acc_train: 0.8857 loss_val: 0.7817 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0167 loss_train: 0.5234 acc_train: 0.9071 loss_val: 0.7795 acc_val: 0.8100 time: 0.0070s\n",
      "Epoch: 0168 loss_train: 0.5314 acc_train: 0.9143 loss_val: 0.7782 acc_val: 0.7967 time: 0.0070s\n",
      "Epoch: 0169 loss_train: 0.5431 acc_train: 0.9071 loss_val: 0.7778 acc_val: 0.7967 time: 0.0070s\n",
      "Epoch: 0170 loss_train: 0.5401 acc_train: 0.8857 loss_val: 0.7780 acc_val: 0.8000 time: 0.0070s\n",
      "Epoch: 0171 loss_train: 0.4811 acc_train: 0.9071 loss_val: 0.7777 acc_val: 0.8000 time: 0.0070s\n",
      "Epoch: 0172 loss_train: 0.5070 acc_train: 0.9286 loss_val: 0.7765 acc_val: 0.8000 time: 0.0080s\n",
      "Epoch: 0173 loss_train: 0.5161 acc_train: 0.9071 loss_val: 0.7733 acc_val: 0.8067 time: 0.0070s\n",
      "Epoch: 0174 loss_train: 0.4754 acc_train: 0.9214 loss_val: 0.7702 acc_val: 0.8067 time: 0.0070s\n",
      "Epoch: 0175 loss_train: 0.4725 acc_train: 0.9143 loss_val: 0.7668 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0176 loss_train: 0.5555 acc_train: 0.9071 loss_val: 0.7643 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0177 loss_train: 0.4682 acc_train: 0.9214 loss_val: 0.7624 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0178 loss_train: 0.4711 acc_train: 0.8929 loss_val: 0.7614 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0179 loss_train: 0.5155 acc_train: 0.9071 loss_val: 0.7605 acc_val: 0.8133 time: 0.0080s\n",
      "Epoch: 0180 loss_train: 0.5191 acc_train: 0.8786 loss_val: 0.7599 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0181 loss_train: 0.4783 acc_train: 0.9571 loss_val: 0.7602 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0182 loss_train: 0.5099 acc_train: 0.9286 loss_val: 0.7612 acc_val: 0.8000 time: 0.0070s\n",
      "Epoch: 0183 loss_train: 0.4810 acc_train: 0.9286 loss_val: 0.7621 acc_val: 0.7933 time: 0.0080s\n",
      "Epoch: 0184 loss_train: 0.5255 acc_train: 0.9000 loss_val: 0.7626 acc_val: 0.7933 time: 0.0080s\n",
      "Epoch: 0185 loss_train: 0.4676 acc_train: 0.9429 loss_val: 0.7597 acc_val: 0.7967 time: 0.0070s\n",
      "Epoch: 0186 loss_train: 0.4791 acc_train: 0.9429 loss_val: 0.7560 acc_val: 0.7967 time: 0.0070s\n",
      "Epoch: 0187 loss_train: 0.5040 acc_train: 0.9071 loss_val: 0.7520 acc_val: 0.8100 time: 0.0070s\n",
      "Epoch: 0188 loss_train: 0.4876 acc_train: 0.9357 loss_val: 0.7483 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0189 loss_train: 0.4607 acc_train: 0.9143 loss_val: 0.7460 acc_val: 0.8100 time: 0.0070s\n",
      "Epoch: 0190 loss_train: 0.4453 acc_train: 0.9071 loss_val: 0.7437 acc_val: 0.8100 time: 0.0070s\n",
      "Epoch: 0191 loss_train: 0.4391 acc_train: 0.9286 loss_val: 0.7414 acc_val: 0.8100 time: 0.0070s\n",
      "Epoch: 0192 loss_train: 0.4289 acc_train: 0.9429 loss_val: 0.7397 acc_val: 0.8100 time: 0.0070s\n",
      "Epoch: 0193 loss_train: 0.4536 acc_train: 0.9357 loss_val: 0.7380 acc_val: 0.8100 time: 0.0070s\n",
      "Epoch: 0194 loss_train: 0.4505 acc_train: 0.9214 loss_val: 0.7370 acc_val: 0.8100 time: 0.0060s\n",
      "Epoch: 0195 loss_train: 0.4641 acc_train: 0.9286 loss_val: 0.7360 acc_val: 0.8133 time: 0.0060s\n",
      "Epoch: 0196 loss_train: 0.4411 acc_train: 0.9214 loss_val: 0.7358 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0197 loss_train: 0.4598 acc_train: 0.9286 loss_val: 0.7358 acc_val: 0.8033 time: 0.0070s\n",
      "Epoch: 0198 loss_train: 0.4583 acc_train: 0.9143 loss_val: 0.7361 acc_val: 0.8033 time: 0.0070s\n",
      "Epoch: 0199 loss_train: 0.4496 acc_train: 0.8929 loss_val: 0.7370 acc_val: 0.7967 time: 0.0070s\n",
      "Epoch: 0200 loss_train: 0.4319 acc_train: 0.9143 loss_val: 0.7366 acc_val: 0.8000 time: 0.0070s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 1.5597s\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98fc41e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.7679 accuracy= 0.8110\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfcca19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
