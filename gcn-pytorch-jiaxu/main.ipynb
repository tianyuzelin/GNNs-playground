{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44582f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import argparse\n",
    "import scipy.sparse as sp\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75c034c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8910a2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f621b0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4899fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path=\"./data/cora/\", dataset=\"cora\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8156b5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62c2691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "epochs = 200\n",
    "weightDecay = 5e-4\n",
    "learningRate = 0.01\n",
    "hidden = 16\n",
    "dropout = 0.5\n",
    "\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=learningRate, weight_decay=weightDecay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16c9b592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate validation set performance separately,\n",
    "    # deactivates dropout during validation run.\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "256a6311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "008610b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9979 acc_train: 0.0857 loss_val: 1.9591 acc_val: 0.1033 time: 0.0179s\n",
      "Epoch: 0002 loss_train: 1.9739 acc_train: 0.0857 loss_val: 1.9368 acc_val: 0.1033 time: 0.0140s\n",
      "Epoch: 0003 loss_train: 1.9501 acc_train: 0.1643 loss_val: 1.9152 acc_val: 0.3800 time: 0.0100s\n",
      "Epoch: 0004 loss_train: 1.9329 acc_train: 0.1929 loss_val: 1.8946 acc_val: 0.3500 time: 0.0100s\n",
      "Epoch: 0005 loss_train: 1.9098 acc_train: 0.3000 loss_val: 1.8750 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0006 loss_train: 1.8829 acc_train: 0.2429 loss_val: 1.8567 acc_val: 0.3500 time: 0.0100s\n",
      "Epoch: 0007 loss_train: 1.8703 acc_train: 0.2643 loss_val: 1.8399 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0008 loss_train: 1.8521 acc_train: 0.2500 loss_val: 1.8249 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0009 loss_train: 1.8140 acc_train: 0.3071 loss_val: 1.8113 acc_val: 0.3500 time: 0.0100s\n",
      "Epoch: 0010 loss_train: 1.8152 acc_train: 0.2857 loss_val: 1.7992 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0011 loss_train: 1.7981 acc_train: 0.2929 loss_val: 1.7885 acc_val: 0.3500 time: 0.0120s\n",
      "Epoch: 0012 loss_train: 1.8038 acc_train: 0.3214 loss_val: 1.7792 acc_val: 0.3500 time: 0.0120s\n",
      "Epoch: 0013 loss_train: 1.7591 acc_train: 0.3429 loss_val: 1.7708 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0014 loss_train: 1.7585 acc_train: 0.2929 loss_val: 1.7634 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0015 loss_train: 1.7791 acc_train: 0.2929 loss_val: 1.7567 acc_val: 0.3500 time: 0.0100s\n",
      "Epoch: 0016 loss_train: 1.7443 acc_train: 0.2857 loss_val: 1.7505 acc_val: 0.3500 time: 0.0100s\n",
      "Epoch: 0017 loss_train: 1.7229 acc_train: 0.3000 loss_val: 1.7446 acc_val: 0.3500 time: 0.0100s\n",
      "Epoch: 0018 loss_train: 1.7482 acc_train: 0.3000 loss_val: 1.7388 acc_val: 0.3500 time: 0.0100s\n",
      "Epoch: 0019 loss_train: 1.7337 acc_train: 0.3000 loss_val: 1.7329 acc_val: 0.3500 time: 0.0120s\n",
      "Epoch: 0020 loss_train: 1.7027 acc_train: 0.3286 loss_val: 1.7269 acc_val: 0.3500 time: 0.0130s\n",
      "Epoch: 0021 loss_train: 1.7187 acc_train: 0.2929 loss_val: 1.7206 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0022 loss_train: 1.7087 acc_train: 0.2786 loss_val: 1.7139 acc_val: 0.3500 time: 0.0120s\n",
      "Epoch: 0023 loss_train: 1.7018 acc_train: 0.2929 loss_val: 1.7069 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0024 loss_train: 1.6674 acc_train: 0.2929 loss_val: 1.6995 acc_val: 0.3500 time: 0.0100s\n",
      "Epoch: 0025 loss_train: 1.6798 acc_train: 0.3071 loss_val: 1.6920 acc_val: 0.3500 time: 0.0100s\n",
      "Epoch: 0026 loss_train: 1.6680 acc_train: 0.2857 loss_val: 1.6844 acc_val: 0.3500 time: 0.0120s\n",
      "Epoch: 0027 loss_train: 1.6625 acc_train: 0.2857 loss_val: 1.6768 acc_val: 0.3500 time: 0.0100s\n",
      "Epoch: 0028 loss_train: 1.6540 acc_train: 0.3000 loss_val: 1.6691 acc_val: 0.3500 time: 0.0120s\n",
      "Epoch: 0029 loss_train: 1.6268 acc_train: 0.3071 loss_val: 1.6615 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0030 loss_train: 1.6207 acc_train: 0.3500 loss_val: 1.6536 acc_val: 0.3500 time: 0.0100s\n",
      "Epoch: 0031 loss_train: 1.6163 acc_train: 0.3143 loss_val: 1.6453 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0032 loss_train: 1.6054 acc_train: 0.3286 loss_val: 1.6368 acc_val: 0.3500 time: 0.0100s\n",
      "Epoch: 0033 loss_train: 1.5870 acc_train: 0.3500 loss_val: 1.6279 acc_val: 0.3467 time: 0.0120s\n",
      "Epoch: 0034 loss_train: 1.5873 acc_train: 0.3000 loss_val: 1.6189 acc_val: 0.3467 time: 0.0130s\n",
      "Epoch: 0035 loss_train: 1.5501 acc_train: 0.4071 loss_val: 1.6095 acc_val: 0.3467 time: 0.0120s\n",
      "Epoch: 0036 loss_train: 1.5719 acc_train: 0.3714 loss_val: 1.5998 acc_val: 0.3467 time: 0.0110s\n",
      "Epoch: 0037 loss_train: 1.5627 acc_train: 0.3571 loss_val: 1.5900 acc_val: 0.3600 time: 0.0120s\n",
      "Epoch: 0038 loss_train: 1.5426 acc_train: 0.3857 loss_val: 1.5803 acc_val: 0.3667 time: 0.0130s\n",
      "Epoch: 0039 loss_train: 1.5017 acc_train: 0.4571 loss_val: 1.5702 acc_val: 0.3667 time: 0.0120s\n",
      "Epoch: 0040 loss_train: 1.5162 acc_train: 0.3857 loss_val: 1.5600 acc_val: 0.3700 time: 0.0130s\n",
      "Epoch: 0041 loss_train: 1.4833 acc_train: 0.3857 loss_val: 1.5498 acc_val: 0.3833 time: 0.0130s\n",
      "Epoch: 0042 loss_train: 1.4721 acc_train: 0.4571 loss_val: 1.5393 acc_val: 0.3900 time: 0.0100s\n",
      "Epoch: 0043 loss_train: 1.4619 acc_train: 0.4714 loss_val: 1.5288 acc_val: 0.3900 time: 0.0120s\n",
      "Epoch: 0044 loss_train: 1.4555 acc_train: 0.4500 loss_val: 1.5182 acc_val: 0.4000 time: 0.0100s\n",
      "Epoch: 0045 loss_train: 1.4412 acc_train: 0.4714 loss_val: 1.5075 acc_val: 0.4200 time: 0.0120s\n",
      "Epoch: 0046 loss_train: 1.4278 acc_train: 0.4286 loss_val: 1.4968 acc_val: 0.4467 time: 0.0110s\n",
      "Epoch: 0047 loss_train: 1.4160 acc_train: 0.5000 loss_val: 1.4861 acc_val: 0.4633 time: 0.0110s\n",
      "Epoch: 0048 loss_train: 1.3585 acc_train: 0.5357 loss_val: 1.4751 acc_val: 0.4833 time: 0.0120s\n",
      "Epoch: 0049 loss_train: 1.3811 acc_train: 0.5571 loss_val: 1.4641 acc_val: 0.4900 time: 0.0110s\n",
      "Epoch: 0050 loss_train: 1.3529 acc_train: 0.5357 loss_val: 1.4529 acc_val: 0.5000 time: 0.0100s\n",
      "Epoch: 0051 loss_train: 1.3471 acc_train: 0.5357 loss_val: 1.4415 acc_val: 0.5067 time: 0.0120s\n",
      "Epoch: 0052 loss_train: 1.3312 acc_train: 0.5714 loss_val: 1.4300 acc_val: 0.5167 time: 0.0120s\n",
      "Epoch: 0053 loss_train: 1.3092 acc_train: 0.5857 loss_val: 1.4181 acc_val: 0.5233 time: 0.0110s\n",
      "Epoch: 0054 loss_train: 1.2911 acc_train: 0.6429 loss_val: 1.4059 acc_val: 0.5333 time: 0.0140s\n",
      "Epoch: 0055 loss_train: 1.2687 acc_train: 0.6000 loss_val: 1.3935 acc_val: 0.5367 time: 0.0110s\n",
      "Epoch: 0056 loss_train: 1.3033 acc_train: 0.6357 loss_val: 1.3809 acc_val: 0.5400 time: 0.0110s\n",
      "Epoch: 0057 loss_train: 1.2386 acc_train: 0.6429 loss_val: 1.3683 acc_val: 0.5400 time: 0.0110s\n",
      "Epoch: 0058 loss_train: 1.2407 acc_train: 0.6214 loss_val: 1.3556 acc_val: 0.5400 time: 0.0100s\n",
      "Epoch: 0059 loss_train: 1.2287 acc_train: 0.6357 loss_val: 1.3430 acc_val: 0.5400 time: 0.0110s\n",
      "Epoch: 0060 loss_train: 1.1872 acc_train: 0.6071 loss_val: 1.3306 acc_val: 0.5433 time: 0.0130s\n",
      "Epoch: 0061 loss_train: 1.2046 acc_train: 0.6643 loss_val: 1.3182 acc_val: 0.5500 time: 0.0110s\n",
      "Epoch: 0062 loss_train: 1.1631 acc_train: 0.6357 loss_val: 1.3064 acc_val: 0.5567 time: 0.0110s\n",
      "Epoch: 0063 loss_train: 1.1587 acc_train: 0.6643 loss_val: 1.2949 acc_val: 0.5733 time: 0.0100s\n",
      "Epoch: 0064 loss_train: 1.1138 acc_train: 0.7143 loss_val: 1.2836 acc_val: 0.5967 time: 0.0110s\n",
      "Epoch: 0065 loss_train: 1.1310 acc_train: 0.6857 loss_val: 1.2723 acc_val: 0.6233 time: 0.0110s\n",
      "Epoch: 0066 loss_train: 1.1213 acc_train: 0.6714 loss_val: 1.2612 acc_val: 0.6367 time: 0.0110s\n",
      "Epoch: 0067 loss_train: 1.1091 acc_train: 0.7143 loss_val: 1.2502 acc_val: 0.6500 time: 0.0120s\n",
      "Epoch: 0068 loss_train: 1.1034 acc_train: 0.7429 loss_val: 1.2393 acc_val: 0.6667 time: 0.0110s\n",
      "Epoch: 0069 loss_train: 1.0767 acc_train: 0.7143 loss_val: 1.2285 acc_val: 0.6800 time: 0.0100s\n",
      "Epoch: 0070 loss_train: 1.0424 acc_train: 0.7714 loss_val: 1.2177 acc_val: 0.6900 time: 0.0110s\n",
      "Epoch: 0071 loss_train: 1.0580 acc_train: 0.7643 loss_val: 1.2075 acc_val: 0.7000 time: 0.0110s\n",
      "Epoch: 0072 loss_train: 1.0354 acc_train: 0.7643 loss_val: 1.1972 acc_val: 0.7100 time: 0.0140s\n",
      "Epoch: 0073 loss_train: 1.0240 acc_train: 0.7571 loss_val: 1.1867 acc_val: 0.7133 time: 0.0120s\n",
      "Epoch: 0074 loss_train: 1.0352 acc_train: 0.7643 loss_val: 1.1763 acc_val: 0.7133 time: 0.0110s\n",
      "Epoch: 0075 loss_train: 0.9827 acc_train: 0.7929 loss_val: 1.1662 acc_val: 0.7167 time: 0.0100s\n",
      "Epoch: 0076 loss_train: 1.0143 acc_train: 0.7857 loss_val: 1.1559 acc_val: 0.7167 time: 0.0110s\n",
      "Epoch: 0077 loss_train: 0.9699 acc_train: 0.7643 loss_val: 1.1459 acc_val: 0.7200 time: 0.0110s\n",
      "Epoch: 0078 loss_train: 0.9391 acc_train: 0.8071 loss_val: 1.1364 acc_val: 0.7267 time: 0.0120s\n",
      "Epoch: 0079 loss_train: 0.9908 acc_train: 0.7714 loss_val: 1.1268 acc_val: 0.7367 time: 0.0130s\n",
      "Epoch: 0080 loss_train: 0.9484 acc_train: 0.8000 loss_val: 1.1174 acc_val: 0.7533 time: 0.0110s\n",
      "Epoch: 0081 loss_train: 0.9253 acc_train: 0.7786 loss_val: 1.1088 acc_val: 0.7567 time: 0.0110s\n",
      "Epoch: 0082 loss_train: 0.9183 acc_train: 0.7786 loss_val: 1.0994 acc_val: 0.7667 time: 0.0110s\n",
      "Epoch: 0083 loss_train: 0.9324 acc_train: 0.8214 loss_val: 1.0906 acc_val: 0.7667 time: 0.0120s\n",
      "Epoch: 0084 loss_train: 0.8923 acc_train: 0.8143 loss_val: 1.0819 acc_val: 0.7667 time: 0.0100s\n",
      "Epoch: 0085 loss_train: 0.8767 acc_train: 0.8357 loss_val: 1.0729 acc_val: 0.7667 time: 0.0110s\n",
      "Epoch: 0086 loss_train: 0.9058 acc_train: 0.8357 loss_val: 1.0641 acc_val: 0.7667 time: 0.0110s\n",
      "Epoch: 0087 loss_train: 0.9033 acc_train: 0.8000 loss_val: 1.0553 acc_val: 0.7700 time: 0.0120s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0088 loss_train: 0.8743 acc_train: 0.8214 loss_val: 1.0472 acc_val: 0.7700 time: 0.0120s\n",
      "Epoch: 0089 loss_train: 0.8639 acc_train: 0.8357 loss_val: 1.0395 acc_val: 0.7733 time: 0.0120s\n",
      "Epoch: 0090 loss_train: 0.8431 acc_train: 0.8000 loss_val: 1.0328 acc_val: 0.7767 time: 0.0100s\n",
      "Epoch: 0091 loss_train: 0.7865 acc_train: 0.8429 loss_val: 1.0262 acc_val: 0.7833 time: 0.0120s\n",
      "Epoch: 0092 loss_train: 0.8160 acc_train: 0.8357 loss_val: 1.0192 acc_val: 0.7900 time: 0.0120s\n",
      "Epoch: 0093 loss_train: 0.8104 acc_train: 0.8571 loss_val: 1.0114 acc_val: 0.7900 time: 0.0110s\n",
      "Epoch: 0094 loss_train: 0.8142 acc_train: 0.8500 loss_val: 1.0042 acc_val: 0.7967 time: 0.0100s\n",
      "Epoch: 0095 loss_train: 0.7982 acc_train: 0.8286 loss_val: 0.9957 acc_val: 0.7967 time: 0.0110s\n",
      "Epoch: 0096 loss_train: 0.8197 acc_train: 0.8500 loss_val: 0.9881 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0097 loss_train: 0.8027 acc_train: 0.8571 loss_val: 0.9809 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0098 loss_train: 0.7933 acc_train: 0.8357 loss_val: 0.9749 acc_val: 0.8000 time: 0.0120s\n",
      "Epoch: 0099 loss_train: 0.8251 acc_train: 0.8286 loss_val: 0.9692 acc_val: 0.8000 time: 0.0110s\n",
      "Epoch: 0100 loss_train: 0.7657 acc_train: 0.8643 loss_val: 0.9636 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0101 loss_train: 0.7929 acc_train: 0.8500 loss_val: 0.9584 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0102 loss_train: 0.7481 acc_train: 0.8500 loss_val: 0.9535 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0103 loss_train: 0.7090 acc_train: 0.8786 loss_val: 0.9487 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0104 loss_train: 0.7360 acc_train: 0.8571 loss_val: 0.9433 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0105 loss_train: 0.7439 acc_train: 0.8571 loss_val: 0.9376 acc_val: 0.8000 time: 0.0120s\n",
      "Epoch: 0106 loss_train: 0.7512 acc_train: 0.8643 loss_val: 0.9309 acc_val: 0.8000 time: 0.0120s\n",
      "Epoch: 0107 loss_train: 0.7865 acc_train: 0.8214 loss_val: 0.9233 acc_val: 0.8000 time: 0.0140s\n",
      "Epoch: 0108 loss_train: 0.7050 acc_train: 0.8714 loss_val: 0.9162 acc_val: 0.8000 time: 0.0120s\n",
      "Epoch: 0109 loss_train: 0.7311 acc_train: 0.8429 loss_val: 0.9107 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0110 loss_train: 0.7328 acc_train: 0.8429 loss_val: 0.9056 acc_val: 0.8000 time: 0.0120s\n",
      "Epoch: 0111 loss_train: 0.7005 acc_train: 0.8857 loss_val: 0.9018 acc_val: 0.8000 time: 0.0120s\n",
      "Epoch: 0112 loss_train: 0.7137 acc_train: 0.9000 loss_val: 0.8983 acc_val: 0.7933 time: 0.0120s\n",
      "Epoch: 0113 loss_train: 0.6493 acc_train: 0.8786 loss_val: 0.8950 acc_val: 0.7933 time: 0.0100s\n",
      "Epoch: 0114 loss_train: 0.6788 acc_train: 0.8929 loss_val: 0.8914 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0115 loss_train: 0.6354 acc_train: 0.9214 loss_val: 0.8862 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0116 loss_train: 0.6830 acc_train: 0.8929 loss_val: 0.8808 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0117 loss_train: 0.6859 acc_train: 0.8857 loss_val: 0.8754 acc_val: 0.8100 time: 0.0100s\n",
      "Epoch: 0118 loss_train: 0.6565 acc_train: 0.8857 loss_val: 0.8699 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0119 loss_train: 0.6401 acc_train: 0.8857 loss_val: 0.8643 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0120 loss_train: 0.6497 acc_train: 0.8786 loss_val: 0.8594 acc_val: 0.8000 time: 0.0110s\n",
      "Epoch: 0121 loss_train: 0.6496 acc_train: 0.8643 loss_val: 0.8556 acc_val: 0.8000 time: 0.0120s\n",
      "Epoch: 0122 loss_train: 0.6335 acc_train: 0.8786 loss_val: 0.8521 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0123 loss_train: 0.5936 acc_train: 0.8786 loss_val: 0.8496 acc_val: 0.8000 time: 0.0120s\n",
      "Epoch: 0124 loss_train: 0.6114 acc_train: 0.8929 loss_val: 0.8473 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0125 loss_train: 0.5696 acc_train: 0.8929 loss_val: 0.8453 acc_val: 0.8067 time: 0.0130s\n",
      "Epoch: 0126 loss_train: 0.6320 acc_train: 0.8929 loss_val: 0.8426 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0127 loss_train: 0.5816 acc_train: 0.8786 loss_val: 0.8399 acc_val: 0.8067 time: 0.0120s\n",
      "Epoch: 0128 loss_train: 0.6382 acc_train: 0.9000 loss_val: 0.8357 acc_val: 0.8000 time: 0.0110s\n",
      "Epoch: 0129 loss_train: 0.6044 acc_train: 0.9000 loss_val: 0.8323 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0130 loss_train: 0.5483 acc_train: 0.9000 loss_val: 0.8296 acc_val: 0.8100 time: 0.0110s\n",
      "Epoch: 0131 loss_train: 0.5973 acc_train: 0.8857 loss_val: 0.8260 acc_val: 0.8100 time: 0.0100s\n",
      "Epoch: 0132 loss_train: 0.6021 acc_train: 0.8786 loss_val: 0.8236 acc_val: 0.8100 time: 0.0120s\n",
      "Epoch: 0133 loss_train: 0.5789 acc_train: 0.9000 loss_val: 0.8215 acc_val: 0.8100 time: 0.0110s\n",
      "Epoch: 0134 loss_train: 0.6071 acc_train: 0.8786 loss_val: 0.8189 acc_val: 0.8000 time: 0.0090s\n",
      "Epoch: 0135 loss_train: 0.5442 acc_train: 0.8929 loss_val: 0.8161 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0136 loss_train: 0.5932 acc_train: 0.8857 loss_val: 0.8125 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0137 loss_train: 0.5516 acc_train: 0.9071 loss_val: 0.8083 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0138 loss_train: 0.5734 acc_train: 0.8786 loss_val: 0.8042 acc_val: 0.8033 time: 0.0120s\n",
      "Epoch: 0139 loss_train: 0.5881 acc_train: 0.8786 loss_val: 0.8000 acc_val: 0.8000 time: 0.0110s\n",
      "Epoch: 0140 loss_train: 0.5667 acc_train: 0.9214 loss_val: 0.7965 acc_val: 0.8100 time: 0.0130s\n",
      "Epoch: 0141 loss_train: 0.5089 acc_train: 0.9143 loss_val: 0.7936 acc_val: 0.8133 time: 0.0120s\n",
      "Epoch: 0142 loss_train: 0.5733 acc_train: 0.9071 loss_val: 0.7915 acc_val: 0.8200 time: 0.0120s\n",
      "Epoch: 0143 loss_train: 0.5171 acc_train: 0.9214 loss_val: 0.7898 acc_val: 0.8133 time: 0.0110s\n",
      "Epoch: 0144 loss_train: 0.5701 acc_train: 0.8929 loss_val: 0.7886 acc_val: 0.8167 time: 0.0110s\n",
      "Epoch: 0145 loss_train: 0.5669 acc_train: 0.9143 loss_val: 0.7859 acc_val: 0.8167 time: 0.0120s\n",
      "Epoch: 0146 loss_train: 0.5063 acc_train: 0.9071 loss_val: 0.7824 acc_val: 0.8133 time: 0.0120s\n",
      "Epoch: 0147 loss_train: 0.5420 acc_train: 0.9000 loss_val: 0.7791 acc_val: 0.8167 time: 0.0100s\n",
      "Epoch: 0148 loss_train: 0.5854 acc_train: 0.8786 loss_val: 0.7765 acc_val: 0.8200 time: 0.0110s\n",
      "Epoch: 0149 loss_train: 0.5154 acc_train: 0.9214 loss_val: 0.7737 acc_val: 0.8133 time: 0.0120s\n",
      "Epoch: 0150 loss_train: 0.5408 acc_train: 0.9071 loss_val: 0.7705 acc_val: 0.8167 time: 0.0110s\n",
      "Epoch: 0151 loss_train: 0.5435 acc_train: 0.9286 loss_val: 0.7673 acc_val: 0.8200 time: 0.0120s\n",
      "Epoch: 0152 loss_train: 0.5476 acc_train: 0.8857 loss_val: 0.7647 acc_val: 0.8233 time: 0.0110s\n",
      "Epoch: 0153 loss_train: 0.5241 acc_train: 0.9143 loss_val: 0.7622 acc_val: 0.8233 time: 0.0110s\n",
      "Epoch: 0154 loss_train: 0.5194 acc_train: 0.8857 loss_val: 0.7605 acc_val: 0.8200 time: 0.0110s\n",
      "Epoch: 0155 loss_train: 0.5063 acc_train: 0.9429 loss_val: 0.7594 acc_val: 0.8200 time: 0.0120s\n",
      "Epoch: 0156 loss_train: 0.5200 acc_train: 0.9000 loss_val: 0.7589 acc_val: 0.8133 time: 0.0120s\n",
      "Epoch: 0157 loss_train: 0.5139 acc_train: 0.9071 loss_val: 0.7585 acc_val: 0.8067 time: 0.0120s\n",
      "Epoch: 0158 loss_train: 0.4811 acc_train: 0.9143 loss_val: 0.7571 acc_val: 0.8067 time: 0.0120s\n",
      "Epoch: 0159 loss_train: 0.4448 acc_train: 0.9357 loss_val: 0.7552 acc_val: 0.8067 time: 0.0120s\n",
      "Epoch: 0160 loss_train: 0.4676 acc_train: 0.9429 loss_val: 0.7525 acc_val: 0.8100 time: 0.0120s\n",
      "Epoch: 0161 loss_train: 0.4760 acc_train: 0.9214 loss_val: 0.7498 acc_val: 0.8100 time: 0.0120s\n",
      "Epoch: 0162 loss_train: 0.5062 acc_train: 0.8786 loss_val: 0.7477 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0163 loss_train: 0.4717 acc_train: 0.9214 loss_val: 0.7468 acc_val: 0.8100 time: 0.0120s\n",
      "Epoch: 0164 loss_train: 0.4411 acc_train: 0.9429 loss_val: 0.7463 acc_val: 0.8100 time: 0.0100s\n",
      "Epoch: 0165 loss_train: 0.4964 acc_train: 0.9214 loss_val: 0.7466 acc_val: 0.8100 time: 0.0120s\n",
      "Epoch: 0166 loss_train: 0.5031 acc_train: 0.9357 loss_val: 0.7465 acc_val: 0.8100 time: 0.0100s\n",
      "Epoch: 0167 loss_train: 0.4887 acc_train: 0.9214 loss_val: 0.7460 acc_val: 0.8100 time: 0.0110s\n",
      "Epoch: 0168 loss_train: 0.4847 acc_train: 0.9214 loss_val: 0.7454 acc_val: 0.8067 time: 0.0120s\n",
      "Epoch: 0169 loss_train: 0.4885 acc_train: 0.9214 loss_val: 0.7448 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0170 loss_train: 0.4904 acc_train: 0.9143 loss_val: 0.7430 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0171 loss_train: 0.4451 acc_train: 0.9500 loss_val: 0.7405 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0172 loss_train: 0.4819 acc_train: 0.9429 loss_val: 0.7386 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0173 loss_train: 0.4515 acc_train: 0.9500 loss_val: 0.7368 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0174 loss_train: 0.4662 acc_train: 0.9214 loss_val: 0.7354 acc_val: 0.8033 time: 0.0120s\n",
      "Epoch: 0175 loss_train: 0.4398 acc_train: 0.9500 loss_val: 0.7356 acc_val: 0.8067 time: 0.0100s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0176 loss_train: 0.5099 acc_train: 0.9143 loss_val: 0.7345 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0177 loss_train: 0.5396 acc_train: 0.8929 loss_val: 0.7323 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0178 loss_train: 0.4887 acc_train: 0.9571 loss_val: 0.7303 acc_val: 0.8067 time: 0.0120s\n",
      "Epoch: 0179 loss_train: 0.4548 acc_train: 0.9429 loss_val: 0.7282 acc_val: 0.8033 time: 0.0120s\n",
      "Epoch: 0180 loss_train: 0.4537 acc_train: 0.9500 loss_val: 0.7269 acc_val: 0.8067 time: 0.0120s\n",
      "Epoch: 0181 loss_train: 0.4569 acc_train: 0.9571 loss_val: 0.7258 acc_val: 0.8067 time: 0.0120s\n",
      "Epoch: 0182 loss_train: 0.4692 acc_train: 0.9429 loss_val: 0.7252 acc_val: 0.8100 time: 0.0130s\n",
      "Epoch: 0183 loss_train: 0.4522 acc_train: 0.9286 loss_val: 0.7248 acc_val: 0.8033 time: 0.0120s\n",
      "Epoch: 0184 loss_train: 0.4359 acc_train: 0.9500 loss_val: 0.7248 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0185 loss_train: 0.4356 acc_train: 0.9429 loss_val: 0.7240 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0186 loss_train: 0.4148 acc_train: 0.9286 loss_val: 0.7230 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0187 loss_train: 0.4296 acc_train: 0.9500 loss_val: 0.7230 acc_val: 0.8100 time: 0.0120s\n",
      "Epoch: 0188 loss_train: 0.4450 acc_train: 0.9214 loss_val: 0.7234 acc_val: 0.8000 time: 0.0120s\n",
      "Epoch: 0189 loss_train: 0.4726 acc_train: 0.9571 loss_val: 0.7208 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0190 loss_train: 0.4136 acc_train: 0.8929 loss_val: 0.7179 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0191 loss_train: 0.4550 acc_train: 0.9357 loss_val: 0.7154 acc_val: 0.8067 time: 0.0130s\n",
      "Epoch: 0192 loss_train: 0.4002 acc_train: 0.9571 loss_val: 0.7140 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0193 loss_train: 0.4094 acc_train: 0.9714 loss_val: 0.7131 acc_val: 0.8067 time: 0.0120s\n",
      "Epoch: 0194 loss_train: 0.4192 acc_train: 0.9429 loss_val: 0.7121 acc_val: 0.8000 time: 0.0110s\n",
      "Epoch: 0195 loss_train: 0.4278 acc_train: 0.9500 loss_val: 0.7115 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0196 loss_train: 0.3895 acc_train: 0.9429 loss_val: 0.7109 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0197 loss_train: 0.4439 acc_train: 0.9214 loss_val: 0.7097 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0198 loss_train: 0.4231 acc_train: 0.9500 loss_val: 0.7093 acc_val: 0.8000 time: 0.0110s\n",
      "Epoch: 0199 loss_train: 0.4189 acc_train: 0.9286 loss_val: 0.7071 acc_val: 0.8000 time: 0.0110s\n",
      "Epoch: 0200 loss_train: 0.4057 acc_train: 0.9214 loss_val: 0.7054 acc_val: 0.8000 time: 0.0110s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 2.2819s\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98fc41e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.7389 accuracy= 0.8290\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfcca19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
