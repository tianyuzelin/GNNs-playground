{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44582f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import argparse\n",
    "import scipy.sparse as sp\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75c034c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8910a2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f621b0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4899fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path=\"./data/cora/\", dataset=\"cora\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8156b5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62c2691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "epochs = 200\n",
    "weightDecay = 5e-4\n",
    "learningRate = 0.01\n",
    "hidden = 16\n",
    "dropout = 0.5\n",
    "\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=learningRate, weight_decay=weightDecay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16c9b592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate validation set performance separately,\n",
    "    # deactivates dropout during validation run.\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "256a6311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "008610b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 0.3749 acc_train: 0.9357 loss_val: 0.6792 acc_val: 0.8067 time: 0.0179s\n",
      "Epoch: 0002 loss_train: 0.3804 acc_train: 0.9357 loss_val: 0.6794 acc_val: 0.8100 time: 0.0150s\n",
      "Epoch: 0003 loss_train: 0.3749 acc_train: 0.9571 loss_val: 0.6796 acc_val: 0.8100 time: 0.0120s\n",
      "Epoch: 0004 loss_train: 0.3590 acc_train: 0.9714 loss_val: 0.6810 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0005 loss_train: 0.3646 acc_train: 0.9643 loss_val: 0.6824 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0006 loss_train: 0.3877 acc_train: 0.9571 loss_val: 0.6833 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0007 loss_train: 0.3402 acc_train: 0.9643 loss_val: 0.6833 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0008 loss_train: 0.3571 acc_train: 0.9643 loss_val: 0.6814 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0009 loss_train: 0.3574 acc_train: 0.9643 loss_val: 0.6784 acc_val: 0.8033 time: 0.0099s\n",
      "Epoch: 0010 loss_train: 0.3547 acc_train: 0.9571 loss_val: 0.6750 acc_val: 0.8033 time: 0.0120s\n",
      "Epoch: 0011 loss_train: 0.3403 acc_train: 0.9500 loss_val: 0.6724 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0012 loss_train: 0.3485 acc_train: 0.9357 loss_val: 0.6732 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0013 loss_train: 0.3398 acc_train: 0.9429 loss_val: 0.6751 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0014 loss_train: 0.3448 acc_train: 0.9429 loss_val: 0.6786 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0015 loss_train: 0.3528 acc_train: 0.9643 loss_val: 0.6810 acc_val: 0.8033 time: 0.0120s\n",
      "Epoch: 0016 loss_train: 0.3375 acc_train: 0.9714 loss_val: 0.6828 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0017 loss_train: 0.3615 acc_train: 0.9500 loss_val: 0.6805 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0018 loss_train: 0.3552 acc_train: 0.9571 loss_val: 0.6749 acc_val: 0.8067 time: 0.0150s\n",
      "Epoch: 0019 loss_train: 0.3386 acc_train: 0.9786 loss_val: 0.6702 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0020 loss_train: 0.3179 acc_train: 0.9714 loss_val: 0.6684 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0021 loss_train: 0.3598 acc_train: 0.9357 loss_val: 0.6672 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0022 loss_train: 0.3417 acc_train: 0.9643 loss_val: 0.6673 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0023 loss_train: 0.3353 acc_train: 0.9643 loss_val: 0.6676 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0024 loss_train: 0.3385 acc_train: 0.9500 loss_val: 0.6680 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0025 loss_train: 0.3438 acc_train: 0.9571 loss_val: 0.6685 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0026 loss_train: 0.3711 acc_train: 0.9500 loss_val: 0.6699 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0027 loss_train: 0.3573 acc_train: 0.9143 loss_val: 0.6718 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0028 loss_train: 0.3541 acc_train: 0.9357 loss_val: 0.6743 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0029 loss_train: 0.3207 acc_train: 0.9643 loss_val: 0.6743 acc_val: 0.8067 time: 0.0120s\n",
      "Epoch: 0030 loss_train: 0.3004 acc_train: 0.9714 loss_val: 0.6732 acc_val: 0.8033 time: 0.0120s\n",
      "Epoch: 0031 loss_train: 0.3259 acc_train: 0.9571 loss_val: 0.6719 acc_val: 0.8000 time: 0.0120s\n",
      "Epoch: 0032 loss_train: 0.3287 acc_train: 0.9714 loss_val: 0.6706 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0033 loss_train: 0.3337 acc_train: 0.9643 loss_val: 0.6693 acc_val: 0.8100 time: 0.0110s\n",
      "Epoch: 0034 loss_train: 0.3192 acc_train: 0.9571 loss_val: 0.6667 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0035 loss_train: 0.3076 acc_train: 0.9643 loss_val: 0.6643 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0036 loss_train: 0.3579 acc_train: 0.9500 loss_val: 0.6617 acc_val: 0.8000 time: 0.0130s\n",
      "Epoch: 0037 loss_train: 0.3552 acc_train: 0.9500 loss_val: 0.6611 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0038 loss_train: 0.3220 acc_train: 0.9500 loss_val: 0.6606 acc_val: 0.8033 time: 0.0120s\n",
      "Epoch: 0039 loss_train: 0.3249 acc_train: 0.9571 loss_val: 0.6592 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0040 loss_train: 0.3391 acc_train: 0.9643 loss_val: 0.6568 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0041 loss_train: 0.3358 acc_train: 0.9571 loss_val: 0.6561 acc_val: 0.8033 time: 0.0120s\n",
      "Epoch: 0042 loss_train: 0.3031 acc_train: 0.9571 loss_val: 0.6557 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0043 loss_train: 0.3206 acc_train: 0.9571 loss_val: 0.6550 acc_val: 0.7967 time: 0.0120s\n",
      "Epoch: 0044 loss_train: 0.3032 acc_train: 0.9571 loss_val: 0.6557 acc_val: 0.8000 time: 0.0110s\n",
      "Epoch: 0045 loss_train: 0.2783 acc_train: 0.9714 loss_val: 0.6568 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0046 loss_train: 0.2904 acc_train: 0.9714 loss_val: 0.6568 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0047 loss_train: 0.3042 acc_train: 0.9571 loss_val: 0.6562 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0048 loss_train: 0.3673 acc_train: 0.9571 loss_val: 0.6565 acc_val: 0.8067 time: 0.0120s\n",
      "Epoch: 0049 loss_train: 0.3342 acc_train: 0.9571 loss_val: 0.6568 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0050 loss_train: 0.3057 acc_train: 0.9714 loss_val: 0.6587 acc_val: 0.7967 time: 0.0110s\n",
      "Epoch: 0051 loss_train: 0.3176 acc_train: 0.9643 loss_val: 0.6601 acc_val: 0.8000 time: 0.0120s\n",
      "Epoch: 0052 loss_train: 0.2926 acc_train: 0.9643 loss_val: 0.6603 acc_val: 0.8000 time: 0.0110s\n",
      "Epoch: 0053 loss_train: 0.2988 acc_train: 0.9571 loss_val: 0.6609 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0054 loss_train: 0.3426 acc_train: 0.9500 loss_val: 0.6609 acc_val: 0.8033 time: 0.0129s\n",
      "Epoch: 0055 loss_train: 0.3190 acc_train: 0.9714 loss_val: 0.6606 acc_val: 0.8000 time: 0.0140s\n",
      "Epoch: 0056 loss_train: 0.3053 acc_train: 0.9786 loss_val: 0.6582 acc_val: 0.8033 time: 0.0120s\n",
      "Epoch: 0057 loss_train: 0.2981 acc_train: 0.9643 loss_val: 0.6584 acc_val: 0.8000 time: 0.0110s\n",
      "Epoch: 0058 loss_train: 0.2973 acc_train: 0.9643 loss_val: 0.6582 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0059 loss_train: 0.3211 acc_train: 0.9571 loss_val: 0.6589 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0060 loss_train: 0.2851 acc_train: 0.9786 loss_val: 0.6599 acc_val: 0.8033 time: 0.0120s\n",
      "Epoch: 0061 loss_train: 0.3057 acc_train: 0.9500 loss_val: 0.6581 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0062 loss_train: 0.2977 acc_train: 0.9643 loss_val: 0.6561 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0063 loss_train: 0.2833 acc_train: 0.9857 loss_val: 0.6548 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0064 loss_train: 0.3338 acc_train: 0.9643 loss_val: 0.6544 acc_val: 0.8133 time: 0.0100s\n",
      "Epoch: 0065 loss_train: 0.2740 acc_train: 0.9714 loss_val: 0.6534 acc_val: 0.8100 time: 0.0110s\n",
      "Epoch: 0066 loss_train: 0.3074 acc_train: 0.9500 loss_val: 0.6515 acc_val: 0.8133 time: 0.0100s\n",
      "Epoch: 0067 loss_train: 0.3386 acc_train: 0.9500 loss_val: 0.6494 acc_val: 0.8100 time: 0.0110s\n",
      "Epoch: 0068 loss_train: 0.3116 acc_train: 0.9500 loss_val: 0.6480 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0069 loss_train: 0.3061 acc_train: 0.9714 loss_val: 0.6483 acc_val: 0.8033 time: 0.0090s\n",
      "Epoch: 0070 loss_train: 0.2958 acc_train: 0.9714 loss_val: 0.6490 acc_val: 0.8000 time: 0.0140s\n",
      "Epoch: 0071 loss_train: 0.2830 acc_train: 0.9714 loss_val: 0.6509 acc_val: 0.8000 time: 0.0120s\n",
      "Epoch: 0072 loss_train: 0.3523 acc_train: 0.9286 loss_val: 0.6557 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0073 loss_train: 0.2817 acc_train: 0.9786 loss_val: 0.6623 acc_val: 0.8000 time: 0.0139s\n",
      "Epoch: 0074 loss_train: 0.2899 acc_train: 0.9786 loss_val: 0.6671 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0075 loss_train: 0.3366 acc_train: 0.9571 loss_val: 0.6684 acc_val: 0.8033 time: 0.0109s\n",
      "Epoch: 0076 loss_train: 0.2593 acc_train: 0.9786 loss_val: 0.6654 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0077 loss_train: 0.2651 acc_train: 0.9714 loss_val: 0.6608 acc_val: 0.7967 time: 0.0110s\n",
      "Epoch: 0078 loss_train: 0.2928 acc_train: 0.9571 loss_val: 0.6556 acc_val: 0.7933 time: 0.0110s\n",
      "Epoch: 0079 loss_train: 0.3044 acc_train: 0.9643 loss_val: 0.6503 acc_val: 0.8033 time: 0.0120s\n",
      "Epoch: 0080 loss_train: 0.3201 acc_train: 0.9643 loss_val: 0.6474 acc_val: 0.8100 time: 0.0100s\n",
      "Epoch: 0081 loss_train: 0.2686 acc_train: 0.9643 loss_val: 0.6458 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0082 loss_train: 0.3293 acc_train: 0.9571 loss_val: 0.6468 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0083 loss_train: 0.2733 acc_train: 0.9786 loss_val: 0.6482 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0084 loss_train: 0.2826 acc_train: 0.9571 loss_val: 0.6499 acc_val: 0.8100 time: 0.0110s\n",
      "Epoch: 0085 loss_train: 0.3190 acc_train: 0.9500 loss_val: 0.6502 acc_val: 0.8067 time: 0.0119s\n",
      "Epoch: 0086 loss_train: 0.2801 acc_train: 0.9643 loss_val: 0.6497 acc_val: 0.8100 time: 0.0130s\n",
      "Epoch: 0087 loss_train: 0.3067 acc_train: 0.9643 loss_val: 0.6493 acc_val: 0.8067 time: 0.0140s\n",
      "Epoch: 0088 loss_train: 0.2951 acc_train: 0.9571 loss_val: 0.6507 acc_val: 0.8033 time: 0.0120s\n",
      "Epoch: 0089 loss_train: 0.2578 acc_train: 0.9643 loss_val: 0.6528 acc_val: 0.7967 time: 0.0110s\n",
      "Epoch: 0090 loss_train: 0.2621 acc_train: 0.9643 loss_val: 0.6535 acc_val: 0.8000 time: 0.0120s\n",
      "Epoch: 0091 loss_train: 0.2916 acc_train: 0.9643 loss_val: 0.6513 acc_val: 0.8000 time: 0.0109s\n",
      "Epoch: 0092 loss_train: 0.2743 acc_train: 0.9786 loss_val: 0.6497 acc_val: 0.8000 time: 0.0110s\n",
      "Epoch: 0093 loss_train: 0.3036 acc_train: 0.9643 loss_val: 0.6467 acc_val: 0.8067 time: 0.0120s\n",
      "Epoch: 0094 loss_train: 0.2945 acc_train: 0.9643 loss_val: 0.6455 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0095 loss_train: 0.3066 acc_train: 0.9571 loss_val: 0.6444 acc_val: 0.8067 time: 0.0120s\n",
      "Epoch: 0096 loss_train: 0.3050 acc_train: 0.9571 loss_val: 0.6434 acc_val: 0.8100 time: 0.0120s\n",
      "Epoch: 0097 loss_train: 0.2648 acc_train: 0.9929 loss_val: 0.6422 acc_val: 0.8100 time: 0.0109s\n",
      "Epoch: 0098 loss_train: 0.2493 acc_train: 0.9786 loss_val: 0.6410 acc_val: 0.8100 time: 0.0110s\n",
      "Epoch: 0099 loss_train: 0.2880 acc_train: 0.9714 loss_val: 0.6415 acc_val: 0.8100 time: 0.0110s\n",
      "Epoch: 0100 loss_train: 0.2966 acc_train: 0.9857 loss_val: 0.6394 acc_val: 0.8100 time: 0.0110s\n",
      "Epoch: 0101 loss_train: 0.3350 acc_train: 0.9571 loss_val: 0.6387 acc_val: 0.8100 time: 0.0100s\n",
      "Epoch: 0102 loss_train: 0.3042 acc_train: 0.9643 loss_val: 0.6405 acc_val: 0.8000 time: 0.0100s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0103 loss_train: 0.2922 acc_train: 0.9643 loss_val: 0.6436 acc_val: 0.7967 time: 0.0130s\n",
      "Epoch: 0104 loss_train: 0.2845 acc_train: 0.9429 loss_val: 0.6456 acc_val: 0.7967 time: 0.0140s\n",
      "Epoch: 0105 loss_train: 0.2738 acc_train: 0.9714 loss_val: 0.6461 acc_val: 0.7967 time: 0.0110s\n",
      "Epoch: 0106 loss_train: 0.2799 acc_train: 0.9786 loss_val: 0.6477 acc_val: 0.7967 time: 0.0110s\n",
      "Epoch: 0107 loss_train: 0.2619 acc_train: 0.9714 loss_val: 0.6466 acc_val: 0.8000 time: 0.0120s\n",
      "Epoch: 0108 loss_train: 0.2662 acc_train: 0.9786 loss_val: 0.6460 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0109 loss_train: 0.2680 acc_train: 0.9786 loss_val: 0.6451 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0110 loss_train: 0.2827 acc_train: 0.9786 loss_val: 0.6439 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0111 loss_train: 0.2459 acc_train: 0.9786 loss_val: 0.6436 acc_val: 0.8100 time: 0.0120s\n",
      "Epoch: 0112 loss_train: 0.2876 acc_train: 0.9571 loss_val: 0.6445 acc_val: 0.8100 time: 0.0110s\n",
      "Epoch: 0113 loss_train: 0.2716 acc_train: 0.9786 loss_val: 0.6446 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0114 loss_train: 0.2757 acc_train: 0.9786 loss_val: 0.6462 acc_val: 0.7967 time: 0.0100s\n",
      "Epoch: 0115 loss_train: 0.2712 acc_train: 0.9714 loss_val: 0.6476 acc_val: 0.8000 time: 0.0090s\n",
      "Epoch: 0116 loss_train: 0.2930 acc_train: 0.9429 loss_val: 0.6496 acc_val: 0.7967 time: 0.0110s\n",
      "Epoch: 0117 loss_train: 0.2897 acc_train: 0.9571 loss_val: 0.6491 acc_val: 0.7967 time: 0.0110s\n",
      "Epoch: 0118 loss_train: 0.2776 acc_train: 0.9714 loss_val: 0.6469 acc_val: 0.8033 time: 0.0090s\n",
      "Epoch: 0119 loss_train: 0.2780 acc_train: 0.9429 loss_val: 0.6467 acc_val: 0.7900 time: 0.0130s\n",
      "Epoch: 0120 loss_train: 0.2762 acc_train: 0.9857 loss_val: 0.6462 acc_val: 0.7933 time: 0.0110s\n",
      "Epoch: 0121 loss_train: 0.2478 acc_train: 0.9643 loss_val: 0.6473 acc_val: 0.8067 time: 0.0130s\n",
      "Epoch: 0122 loss_train: 0.2812 acc_train: 0.9429 loss_val: 0.6459 acc_val: 0.8067 time: 0.0130s\n",
      "Epoch: 0123 loss_train: 0.2798 acc_train: 0.9571 loss_val: 0.6438 acc_val: 0.8100 time: 0.0120s\n",
      "Epoch: 0124 loss_train: 0.2564 acc_train: 0.9714 loss_val: 0.6444 acc_val: 0.8133 time: 0.0119s\n",
      "Epoch: 0125 loss_train: 0.2597 acc_train: 0.9714 loss_val: 0.6455 acc_val: 0.8100 time: 0.0110s\n",
      "Epoch: 0126 loss_train: 0.2744 acc_train: 0.9714 loss_val: 0.6445 acc_val: 0.8067 time: 0.0120s\n",
      "Epoch: 0127 loss_train: 0.2595 acc_train: 0.9786 loss_val: 0.6432 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0128 loss_train: 0.2720 acc_train: 0.9714 loss_val: 0.6436 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0129 loss_train: 0.2579 acc_train: 0.9643 loss_val: 0.6441 acc_val: 0.8067 time: 0.0120s\n",
      "Epoch: 0130 loss_train: 0.2754 acc_train: 0.9643 loss_val: 0.6430 acc_val: 0.8000 time: 0.0110s\n",
      "Epoch: 0131 loss_train: 0.2725 acc_train: 0.9786 loss_val: 0.6409 acc_val: 0.7967 time: 0.0110s\n",
      "Epoch: 0132 loss_train: 0.3163 acc_train: 0.9571 loss_val: 0.6392 acc_val: 0.7933 time: 0.0110s\n",
      "Epoch: 0133 loss_train: 0.2704 acc_train: 0.9500 loss_val: 0.6382 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0134 loss_train: 0.2734 acc_train: 0.9857 loss_val: 0.6390 acc_val: 0.8100 time: 0.0110s\n",
      "Epoch: 0135 loss_train: 0.2647 acc_train: 0.9857 loss_val: 0.6426 acc_val: 0.8100 time: 0.0100s\n",
      "Epoch: 0136 loss_train: 0.2446 acc_train: 0.9857 loss_val: 0.6453 acc_val: 0.8133 time: 0.0130s\n",
      "Epoch: 0137 loss_train: 0.2809 acc_train: 0.9714 loss_val: 0.6489 acc_val: 0.8100 time: 0.0130s\n",
      "Epoch: 0138 loss_train: 0.2452 acc_train: 0.9786 loss_val: 0.6503 acc_val: 0.8100 time: 0.0130s\n",
      "Epoch: 0139 loss_train: 0.2662 acc_train: 0.9786 loss_val: 0.6488 acc_val: 0.8067 time: 0.0150s\n",
      "Epoch: 0140 loss_train: 0.2712 acc_train: 0.9714 loss_val: 0.6452 acc_val: 0.7967 time: 0.0140s\n",
      "Epoch: 0141 loss_train: 0.2722 acc_train: 0.9571 loss_val: 0.6402 acc_val: 0.7967 time: 0.0130s\n",
      "Epoch: 0142 loss_train: 0.2898 acc_train: 0.9429 loss_val: 0.6362 acc_val: 0.7967 time: 0.0140s\n",
      "Epoch: 0143 loss_train: 0.2592 acc_train: 0.9714 loss_val: 0.6329 acc_val: 0.7967 time: 0.0140s\n",
      "Epoch: 0144 loss_train: 0.2459 acc_train: 0.9714 loss_val: 0.6317 acc_val: 0.7967 time: 0.0120s\n",
      "Epoch: 0145 loss_train: 0.2507 acc_train: 0.9571 loss_val: 0.6316 acc_val: 0.7967 time: 0.0120s\n",
      "Epoch: 0146 loss_train: 0.3096 acc_train: 0.9643 loss_val: 0.6348 acc_val: 0.7967 time: 0.0130s\n",
      "Epoch: 0147 loss_train: 0.2533 acc_train: 0.9714 loss_val: 0.6380 acc_val: 0.7967 time: 0.0130s\n",
      "Epoch: 0148 loss_train: 0.2474 acc_train: 0.9714 loss_val: 0.6419 acc_val: 0.7967 time: 0.0130s\n",
      "Epoch: 0149 loss_train: 0.2648 acc_train: 0.9357 loss_val: 0.6477 acc_val: 0.8000 time: 0.0140s\n",
      "Epoch: 0150 loss_train: 0.2613 acc_train: 0.9643 loss_val: 0.6514 acc_val: 0.8033 time: 0.0130s\n",
      "Epoch: 0151 loss_train: 0.2636 acc_train: 0.9571 loss_val: 0.6534 acc_val: 0.8033 time: 0.0120s\n",
      "Epoch: 0152 loss_train: 0.2609 acc_train: 0.9714 loss_val: 0.6527 acc_val: 0.8033 time: 0.0120s\n",
      "Epoch: 0153 loss_train: 0.2488 acc_train: 0.9786 loss_val: 0.6495 acc_val: 0.8100 time: 0.0120s\n",
      "Epoch: 0154 loss_train: 0.2518 acc_train: 0.9643 loss_val: 0.6447 acc_val: 0.8133 time: 0.0130s\n",
      "Epoch: 0155 loss_train: 0.2473 acc_train: 0.9786 loss_val: 0.6400 acc_val: 0.8100 time: 0.0120s\n",
      "Epoch: 0156 loss_train: 0.2413 acc_train: 0.9714 loss_val: 0.6359 acc_val: 0.8033 time: 0.0130s\n",
      "Epoch: 0157 loss_train: 0.2810 acc_train: 0.9643 loss_val: 0.6321 acc_val: 0.8000 time: 0.0110s\n",
      "Epoch: 0158 loss_train: 0.2271 acc_train: 0.9857 loss_val: 0.6308 acc_val: 0.8000 time: 0.0120s\n",
      "Epoch: 0159 loss_train: 0.3047 acc_train: 0.9143 loss_val: 0.6327 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0160 loss_train: 0.2541 acc_train: 0.9786 loss_val: 0.6377 acc_val: 0.8000 time: 0.0110s\n",
      "Epoch: 0161 loss_train: 0.2207 acc_train: 0.9714 loss_val: 0.6452 acc_val: 0.8000 time: 0.0119s\n",
      "Epoch: 0162 loss_train: 0.2510 acc_train: 0.9571 loss_val: 0.6524 acc_val: 0.7967 time: 0.0120s\n",
      "Epoch: 0163 loss_train: 0.2573 acc_train: 0.9571 loss_val: 0.6576 acc_val: 0.7967 time: 0.0110s\n",
      "Epoch: 0164 loss_train: 0.2637 acc_train: 0.9571 loss_val: 0.6624 acc_val: 0.7900 time: 0.0120s\n",
      "Epoch: 0165 loss_train: 0.2629 acc_train: 0.9786 loss_val: 0.6626 acc_val: 0.7967 time: 0.0120s\n",
      "Epoch: 0166 loss_train: 0.2470 acc_train: 0.9714 loss_val: 0.6617 acc_val: 0.8000 time: 0.0110s\n",
      "Epoch: 0167 loss_train: 0.2199 acc_train: 0.9857 loss_val: 0.6579 acc_val: 0.8000 time: 0.0120s\n",
      "Epoch: 0168 loss_train: 0.2565 acc_train: 0.9857 loss_val: 0.6502 acc_val: 0.8033 time: 0.0120s\n",
      "Epoch: 0169 loss_train: 0.2670 acc_train: 0.9857 loss_val: 0.6436 acc_val: 0.8033 time: 0.0119s\n",
      "Epoch: 0170 loss_train: 0.2803 acc_train: 0.9500 loss_val: 0.6367 acc_val: 0.7967 time: 0.0170s\n",
      "Epoch: 0171 loss_train: 0.2513 acc_train: 0.9857 loss_val: 0.6328 acc_val: 0.8033 time: 0.0139s\n",
      "Epoch: 0172 loss_train: 0.2386 acc_train: 0.9786 loss_val: 0.6315 acc_val: 0.8000 time: 0.0110s\n",
      "Epoch: 0173 loss_train: 0.2518 acc_train: 0.9571 loss_val: 0.6321 acc_val: 0.8000 time: 0.0120s\n",
      "Epoch: 0174 loss_train: 0.2663 acc_train: 0.9857 loss_val: 0.6327 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0175 loss_train: 0.2494 acc_train: 0.9786 loss_val: 0.6327 acc_val: 0.8033 time: 0.0109s\n",
      "Epoch: 0176 loss_train: 0.2460 acc_train: 0.9786 loss_val: 0.6337 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0177 loss_train: 0.2456 acc_train: 0.9857 loss_val: 0.6372 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0178 loss_train: 0.2721 acc_train: 0.9500 loss_val: 0.6395 acc_val: 0.8100 time: 0.0109s\n",
      "Epoch: 0179 loss_train: 0.2543 acc_train: 0.9571 loss_val: 0.6425 acc_val: 0.8100 time: 0.0120s\n",
      "Epoch: 0180 loss_train: 0.2863 acc_train: 0.9643 loss_val: 0.6429 acc_val: 0.8100 time: 0.0110s\n",
      "Epoch: 0181 loss_train: 0.2563 acc_train: 0.9500 loss_val: 0.6443 acc_val: 0.8100 time: 0.0110s\n",
      "Epoch: 0182 loss_train: 0.2283 acc_train: 0.9786 loss_val: 0.6442 acc_val: 0.8067 time: 0.0120s\n",
      "Epoch: 0183 loss_train: 0.2839 acc_train: 0.9714 loss_val: 0.6408 acc_val: 0.8033 time: 0.0120s\n",
      "Epoch: 0184 loss_train: 0.2287 acc_train: 0.9857 loss_val: 0.6369 acc_val: 0.8033 time: 0.0120s\n",
      "Epoch: 0185 loss_train: 0.2369 acc_train: 0.9714 loss_val: 0.6325 acc_val: 0.8100 time: 0.0110s\n",
      "Epoch: 0186 loss_train: 0.2315 acc_train: 0.9786 loss_val: 0.6305 acc_val: 0.8133 time: 0.0110s\n",
      "Epoch: 0187 loss_train: 0.2588 acc_train: 0.9643 loss_val: 0.6291 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0188 loss_train: 0.2627 acc_train: 0.9643 loss_val: 0.6275 acc_val: 0.8067 time: 0.0120s\n",
      "Epoch: 0189 loss_train: 0.2488 acc_train: 0.9857 loss_val: 0.6278 acc_val: 0.8033 time: 0.0120s\n",
      "Epoch: 0190 loss_train: 0.2370 acc_train: 0.9714 loss_val: 0.6297 acc_val: 0.7967 time: 0.0130s\n",
      "Epoch: 0191 loss_train: 0.2528 acc_train: 0.9571 loss_val: 0.6316 acc_val: 0.8033 time: 0.0109s\n",
      "Epoch: 0192 loss_train: 0.2558 acc_train: 0.9500 loss_val: 0.6307 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0193 loss_train: 0.2447 acc_train: 0.9786 loss_val: 0.6324 acc_val: 0.8067 time: 0.0120s\n",
      "Epoch: 0194 loss_train: 0.2721 acc_train: 0.9786 loss_val: 0.6315 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0195 loss_train: 0.2341 acc_train: 0.9714 loss_val: 0.6303 acc_val: 0.8033 time: 0.0120s\n",
      "Epoch: 0196 loss_train: 0.2399 acc_train: 0.9714 loss_val: 0.6306 acc_val: 0.8000 time: 0.0110s\n",
      "Epoch: 0197 loss_train: 0.2324 acc_train: 0.9714 loss_val: 0.6316 acc_val: 0.8033 time: 0.0130s\n",
      "Epoch: 0198 loss_train: 0.2517 acc_train: 0.9786 loss_val: 0.6315 acc_val: 0.8033 time: 0.0119s\n",
      "Epoch: 0199 loss_train: 0.2628 acc_train: 0.9714 loss_val: 0.6317 acc_val: 0.7933 time: 0.0110s\n",
      "Epoch: 0200 loss_train: 0.2755 acc_train: 0.9643 loss_val: 0.6334 acc_val: 0.8000 time: 0.0129s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 2.3337s\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98fc41e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.6220 accuracy= 0.8310\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfcca19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
