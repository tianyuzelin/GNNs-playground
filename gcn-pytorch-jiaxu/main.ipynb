{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44582f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import argparse\n",
    "import scipy.sparse as sp\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75c034c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8910a2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f621b0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4899fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path=\"../data/cora/\", dataset=\"cora\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "    \n",
    "    print(\"Dataset Loaded!\")\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8156b5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n",
      "Dataset Loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62c2691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "epochs = 200\n",
    "weightDecay = 5e-4\n",
    "learningRate = 0.01\n",
    "hidden = 16\n",
    "dropout = 0.5\n",
    "\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=learningRate, weight_decay=weightDecay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16c9b592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate validation set performance separately,\n",
    "    # deactivates dropout during validation run.\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "256a6311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "008610b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9732 acc_train: 0.0714 loss_val: 1.9581 acc_val: 0.0833 time: 0.0857s\n",
      "Epoch: 0002 loss_train: 1.9633 acc_train: 0.0857 loss_val: 1.9408 acc_val: 0.0833 time: 0.0070s\n",
      "Epoch: 0003 loss_train: 1.9336 acc_train: 0.1214 loss_val: 1.9238 acc_val: 0.0833 time: 0.0070s\n",
      "Epoch: 0004 loss_train: 1.9154 acc_train: 0.1643 loss_val: 1.9069 acc_val: 0.1100 time: 0.0070s\n",
      "Epoch: 0005 loss_train: 1.9152 acc_train: 0.1714 loss_val: 1.8905 acc_val: 0.3533 time: 0.0090s\n",
      "Epoch: 0006 loss_train: 1.8875 acc_train: 0.2571 loss_val: 1.8745 acc_val: 0.3533 time: 0.0100s\n",
      "Epoch: 0007 loss_train: 1.8666 acc_train: 0.3143 loss_val: 1.8591 acc_val: 0.3500 time: 0.0071s\n",
      "Epoch: 0008 loss_train: 1.8520 acc_train: 0.3643 loss_val: 1.8441 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0009 loss_train: 1.8530 acc_train: 0.3071 loss_val: 1.8299 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0010 loss_train: 1.8193 acc_train: 0.3286 loss_val: 1.8160 acc_val: 0.3500 time: 0.0080s\n",
      "Epoch: 0011 loss_train: 1.8212 acc_train: 0.3357 loss_val: 1.8027 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0012 loss_train: 1.7936 acc_train: 0.3357 loss_val: 1.7901 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0013 loss_train: 1.8008 acc_train: 0.3143 loss_val: 1.7782 acc_val: 0.3500 time: 0.0080s\n",
      "Epoch: 0014 loss_train: 1.7858 acc_train: 0.3000 loss_val: 1.7671 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0015 loss_train: 1.7479 acc_train: 0.3214 loss_val: 1.7567 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0016 loss_train: 1.7594 acc_train: 0.3214 loss_val: 1.7470 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0017 loss_train: 1.7615 acc_train: 0.3071 loss_val: 1.7381 acc_val: 0.3500 time: 0.0080s\n",
      "Epoch: 0018 loss_train: 1.7413 acc_train: 0.3143 loss_val: 1.7300 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0019 loss_train: 1.7330 acc_train: 0.3000 loss_val: 1.7224 acc_val: 0.3500 time: 0.0080s\n",
      "Epoch: 0020 loss_train: 1.7300 acc_train: 0.3214 loss_val: 1.7153 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0021 loss_train: 1.7177 acc_train: 0.3143 loss_val: 1.7085 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0022 loss_train: 1.6945 acc_train: 0.3429 loss_val: 1.7017 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0023 loss_train: 1.6883 acc_train: 0.3214 loss_val: 1.6952 acc_val: 0.3533 time: 0.0090s\n",
      "Epoch: 0024 loss_train: 1.6621 acc_train: 0.3357 loss_val: 1.6887 acc_val: 0.3600 time: 0.0080s\n",
      "Epoch: 0025 loss_train: 1.6462 acc_train: 0.3571 loss_val: 1.6820 acc_val: 0.3633 time: 0.0090s\n",
      "Epoch: 0026 loss_train: 1.6467 acc_train: 0.3429 loss_val: 1.6751 acc_val: 0.3633 time: 0.0090s\n",
      "Epoch: 0027 loss_train: 1.6565 acc_train: 0.3714 loss_val: 1.6681 acc_val: 0.3700 time: 0.0090s\n",
      "Epoch: 0028 loss_train: 1.6138 acc_train: 0.4143 loss_val: 1.6609 acc_val: 0.3767 time: 0.0080s\n",
      "Epoch: 0029 loss_train: 1.6301 acc_train: 0.3857 loss_val: 1.6532 acc_val: 0.3867 time: 0.0070s\n",
      "Epoch: 0030 loss_train: 1.5833 acc_train: 0.4143 loss_val: 1.6452 acc_val: 0.3967 time: 0.0070s\n",
      "Epoch: 0031 loss_train: 1.5918 acc_train: 0.4143 loss_val: 1.6366 acc_val: 0.4000 time: 0.0080s\n",
      "Epoch: 0032 loss_train: 1.5957 acc_train: 0.4000 loss_val: 1.6275 acc_val: 0.4033 time: 0.0080s\n",
      "Epoch: 0033 loss_train: 1.5557 acc_train: 0.4643 loss_val: 1.6182 acc_val: 0.4133 time: 0.0080s\n",
      "Epoch: 0034 loss_train: 1.5581 acc_train: 0.4143 loss_val: 1.6086 acc_val: 0.4133 time: 0.0090s\n",
      "Epoch: 0035 loss_train: 1.5503 acc_train: 0.4429 loss_val: 1.5987 acc_val: 0.4200 time: 0.0080s\n",
      "Epoch: 0036 loss_train: 1.4905 acc_train: 0.4643 loss_val: 1.5884 acc_val: 0.4233 time: 0.0080s\n",
      "Epoch: 0037 loss_train: 1.5296 acc_train: 0.4500 loss_val: 1.5775 acc_val: 0.4300 time: 0.0090s\n",
      "Epoch: 0038 loss_train: 1.4879 acc_train: 0.4571 loss_val: 1.5662 acc_val: 0.4300 time: 0.0100s\n",
      "Epoch: 0039 loss_train: 1.4838 acc_train: 0.4714 loss_val: 1.5544 acc_val: 0.4300 time: 0.0100s\n",
      "Epoch: 0040 loss_train: 1.4531 acc_train: 0.4786 loss_val: 1.5423 acc_val: 0.4300 time: 0.0070s\n",
      "Epoch: 0041 loss_train: 1.4464 acc_train: 0.4929 loss_val: 1.5298 acc_val: 0.4333 time: 0.0080s\n",
      "Epoch: 0042 loss_train: 1.4159 acc_train: 0.4714 loss_val: 1.5174 acc_val: 0.4433 time: 0.0090s\n",
      "Epoch: 0043 loss_train: 1.4078 acc_train: 0.5000 loss_val: 1.5048 acc_val: 0.4467 time: 0.0090s\n",
      "Epoch: 0044 loss_train: 1.3741 acc_train: 0.5286 loss_val: 1.4920 acc_val: 0.4500 time: 0.0070s\n",
      "Epoch: 0045 loss_train: 1.3750 acc_train: 0.5357 loss_val: 1.4790 acc_val: 0.4533 time: 0.0080s\n",
      "Epoch: 0046 loss_train: 1.3710 acc_train: 0.4714 loss_val: 1.4661 acc_val: 0.4600 time: 0.0070s\n",
      "Epoch: 0047 loss_train: 1.3396 acc_train: 0.5143 loss_val: 1.4531 acc_val: 0.4633 time: 0.0080s\n",
      "Epoch: 0048 loss_train: 1.3304 acc_train: 0.5071 loss_val: 1.4402 acc_val: 0.4733 time: 0.0070s\n",
      "Epoch: 0049 loss_train: 1.3115 acc_train: 0.5071 loss_val: 1.4273 acc_val: 0.4800 time: 0.0070s\n",
      "Epoch: 0050 loss_train: 1.3314 acc_train: 0.4857 loss_val: 1.4146 acc_val: 0.5133 time: 0.0080s\n",
      "Epoch: 0051 loss_train: 1.2631 acc_train: 0.5857 loss_val: 1.4016 acc_val: 0.5200 time: 0.0080s\n",
      "Epoch: 0052 loss_train: 1.2698 acc_train: 0.5571 loss_val: 1.3886 acc_val: 0.5367 time: 0.0070s\n",
      "Epoch: 0053 loss_train: 1.2928 acc_train: 0.5286 loss_val: 1.3760 acc_val: 0.5500 time: 0.0070s\n",
      "Epoch: 0054 loss_train: 1.2347 acc_train: 0.6071 loss_val: 1.3633 acc_val: 0.5600 time: 0.0090s\n",
      "Epoch: 0055 loss_train: 1.1991 acc_train: 0.6214 loss_val: 1.3503 acc_val: 0.5633 time: 0.0080s\n",
      "Epoch: 0056 loss_train: 1.2135 acc_train: 0.6357 loss_val: 1.3370 acc_val: 0.5767 time: 0.0090s\n",
      "Epoch: 0057 loss_train: 1.2295 acc_train: 0.5786 loss_val: 1.3238 acc_val: 0.5833 time: 0.0080s\n",
      "Epoch: 0058 loss_train: 1.1811 acc_train: 0.6286 loss_val: 1.3107 acc_val: 0.6033 time: 0.0080s\n",
      "Epoch: 0059 loss_train: 1.1502 acc_train: 0.7071 loss_val: 1.2974 acc_val: 0.6067 time: 0.0090s\n",
      "Epoch: 0060 loss_train: 1.1518 acc_train: 0.6429 loss_val: 1.2845 acc_val: 0.6133 time: 0.0080s\n",
      "Epoch: 0061 loss_train: 1.1261 acc_train: 0.6714 loss_val: 1.2718 acc_val: 0.6300 time: 0.0090s\n",
      "Epoch: 0062 loss_train: 1.0864 acc_train: 0.6786 loss_val: 1.2589 acc_val: 0.6367 time: 0.0070s\n",
      "Epoch: 0063 loss_train: 1.1070 acc_train: 0.6929 loss_val: 1.2458 acc_val: 0.6433 time: 0.0100s\n",
      "Epoch: 0064 loss_train: 1.0776 acc_train: 0.6857 loss_val: 1.2330 acc_val: 0.6567 time: 0.0080s\n",
      "Epoch: 0065 loss_train: 1.0770 acc_train: 0.6929 loss_val: 1.2208 acc_val: 0.6700 time: 0.0090s\n",
      "Epoch: 0066 loss_train: 1.0447 acc_train: 0.6857 loss_val: 1.2090 acc_val: 0.6867 time: 0.0090s\n",
      "Epoch: 0067 loss_train: 1.0633 acc_train: 0.7214 loss_val: 1.1975 acc_val: 0.6967 time: 0.0080s\n",
      "Epoch: 0068 loss_train: 0.9901 acc_train: 0.7786 loss_val: 1.1863 acc_val: 0.6967 time: 0.0080s\n",
      "Epoch: 0069 loss_train: 0.9841 acc_train: 0.7929 loss_val: 1.1750 acc_val: 0.7033 time: 0.0070s\n",
      "Epoch: 0070 loss_train: 1.0018 acc_train: 0.7357 loss_val: 1.1639 acc_val: 0.7067 time: 0.0070s\n",
      "Epoch: 0071 loss_train: 0.9939 acc_train: 0.7500 loss_val: 1.1534 acc_val: 0.7167 time: 0.0090s\n",
      "Epoch: 0072 loss_train: 0.9527 acc_train: 0.7714 loss_val: 1.1432 acc_val: 0.7200 time: 0.0090s\n",
      "Epoch: 0073 loss_train: 0.9241 acc_train: 0.8143 loss_val: 1.1331 acc_val: 0.7300 time: 0.0080s\n",
      "Epoch: 0074 loss_train: 0.9143 acc_train: 0.8214 loss_val: 1.1226 acc_val: 0.7400 time: 0.0070s\n",
      "Epoch: 0075 loss_train: 0.9445 acc_train: 0.7571 loss_val: 1.1122 acc_val: 0.7467 time: 0.0070s\n",
      "Epoch: 0076 loss_train: 0.9250 acc_train: 0.7857 loss_val: 1.1025 acc_val: 0.7500 time: 0.0080s\n",
      "Epoch: 0077 loss_train: 0.9025 acc_train: 0.7929 loss_val: 1.0933 acc_val: 0.7500 time: 0.0080s\n",
      "Epoch: 0078 loss_train: 0.8959 acc_train: 0.8000 loss_val: 1.0842 acc_val: 0.7567 time: 0.0070s\n",
      "Epoch: 0079 loss_train: 0.8679 acc_train: 0.8000 loss_val: 1.0749 acc_val: 0.7567 time: 0.0070s\n",
      "Epoch: 0080 loss_train: 0.8646 acc_train: 0.8143 loss_val: 1.0656 acc_val: 0.7567 time: 0.0080s\n",
      "Epoch: 0081 loss_train: 0.8518 acc_train: 0.8000 loss_val: 1.0568 acc_val: 0.7600 time: 0.0070s\n",
      "Epoch: 0082 loss_train: 0.8388 acc_train: 0.8214 loss_val: 1.0482 acc_val: 0.7567 time: 0.0080s\n",
      "Epoch: 0083 loss_train: 0.8489 acc_train: 0.8357 loss_val: 1.0392 acc_val: 0.7667 time: 0.0070s\n",
      "Epoch: 0084 loss_train: 0.8596 acc_train: 0.8071 loss_val: 1.0307 acc_val: 0.7667 time: 0.0100s\n",
      "Epoch: 0085 loss_train: 0.8714 acc_train: 0.8429 loss_val: 1.0220 acc_val: 0.7700 time: 0.0091s\n",
      "Epoch: 0086 loss_train: 0.8416 acc_train: 0.8357 loss_val: 1.0137 acc_val: 0.7733 time: 0.0089s\n",
      "Epoch: 0087 loss_train: 0.8268 acc_train: 0.8214 loss_val: 1.0052 acc_val: 0.7733 time: 0.0070s\n",
      "Epoch: 0088 loss_train: 0.7904 acc_train: 0.8643 loss_val: 0.9966 acc_val: 0.7767 time: 0.0080s\n",
      "Epoch: 0089 loss_train: 0.7337 acc_train: 0.8643 loss_val: 0.9886 acc_val: 0.7900 time: 0.0070s\n",
      "Epoch: 0090 loss_train: 0.7988 acc_train: 0.8214 loss_val: 0.9812 acc_val: 0.8000 time: 0.0080s\n",
      "Epoch: 0091 loss_train: 0.7100 acc_train: 0.8714 loss_val: 0.9737 acc_val: 0.8067 time: 0.0080s\n",
      "Epoch: 0092 loss_train: 0.7683 acc_train: 0.8571 loss_val: 0.9670 acc_val: 0.8100 time: 0.0070s\n",
      "Epoch: 0093 loss_train: 0.7529 acc_train: 0.8214 loss_val: 0.9609 acc_val: 0.8033 time: 0.0070s\n",
      "Epoch: 0094 loss_train: 0.7720 acc_train: 0.8214 loss_val: 0.9554 acc_val: 0.8033 time: 0.0080s\n",
      "Epoch: 0095 loss_train: 0.7166 acc_train: 0.8643 loss_val: 0.9490 acc_val: 0.8033 time: 0.0070s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0096 loss_train: 0.7372 acc_train: 0.8357 loss_val: 0.9413 acc_val: 0.8000 time: 0.0090s\n",
      "Epoch: 0097 loss_train: 0.7262 acc_train: 0.8857 loss_val: 0.9335 acc_val: 0.8000 time: 0.0080s\n",
      "Epoch: 0098 loss_train: 0.7088 acc_train: 0.8500 loss_val: 0.9261 acc_val: 0.8100 time: 0.0089s\n",
      "Epoch: 0099 loss_train: 0.6808 acc_train: 0.8714 loss_val: 0.9194 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0100 loss_train: 0.7171 acc_train: 0.8429 loss_val: 0.9136 acc_val: 0.8167 time: 0.0100s\n",
      "Epoch: 0101 loss_train: 0.6831 acc_train: 0.9000 loss_val: 0.9076 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0102 loss_train: 0.6726 acc_train: 0.8643 loss_val: 0.9029 acc_val: 0.8200 time: 0.0090s\n",
      "Epoch: 0103 loss_train: 0.6647 acc_train: 0.9000 loss_val: 0.8981 acc_val: 0.8267 time: 0.0070s\n",
      "Epoch: 0104 loss_train: 0.6352 acc_train: 0.8857 loss_val: 0.8937 acc_val: 0.8267 time: 0.0080s\n",
      "Epoch: 0105 loss_train: 0.6449 acc_train: 0.9143 loss_val: 0.8894 acc_val: 0.8267 time: 0.0080s\n",
      "Epoch: 0106 loss_train: 0.6490 acc_train: 0.9143 loss_val: 0.8853 acc_val: 0.8267 time: 0.0080s\n",
      "Epoch: 0107 loss_train: 0.6707 acc_train: 0.9000 loss_val: 0.8808 acc_val: 0.8267 time: 0.0070s\n",
      "Epoch: 0108 loss_train: 0.6457 acc_train: 0.9000 loss_val: 0.8751 acc_val: 0.8300 time: 0.0080s\n",
      "Epoch: 0109 loss_train: 0.6230 acc_train: 0.8929 loss_val: 0.8700 acc_val: 0.8333 time: 0.0070s\n",
      "Epoch: 0110 loss_train: 0.6155 acc_train: 0.8929 loss_val: 0.8645 acc_val: 0.8333 time: 0.0090s\n",
      "Epoch: 0111 loss_train: 0.6500 acc_train: 0.9143 loss_val: 0.8593 acc_val: 0.8333 time: 0.0090s\n",
      "Epoch: 0112 loss_train: 0.6046 acc_train: 0.8929 loss_val: 0.8546 acc_val: 0.8300 time: 0.0080s\n",
      "Epoch: 0113 loss_train: 0.6308 acc_train: 0.9071 loss_val: 0.8506 acc_val: 0.8333 time: 0.0070s\n",
      "Epoch: 0114 loss_train: 0.6313 acc_train: 0.8714 loss_val: 0.8471 acc_val: 0.8333 time: 0.0080s\n",
      "Epoch: 0115 loss_train: 0.6162 acc_train: 0.9214 loss_val: 0.8431 acc_val: 0.8333 time: 0.0080s\n",
      "Epoch: 0116 loss_train: 0.6096 acc_train: 0.8786 loss_val: 0.8402 acc_val: 0.8267 time: 0.0090s\n",
      "Epoch: 0117 loss_train: 0.5889 acc_train: 0.9143 loss_val: 0.8364 acc_val: 0.8267 time: 0.0070s\n",
      "Epoch: 0118 loss_train: 0.5917 acc_train: 0.9286 loss_val: 0.8332 acc_val: 0.8267 time: 0.0080s\n",
      "Epoch: 0119 loss_train: 0.5953 acc_train: 0.9143 loss_val: 0.8296 acc_val: 0.8267 time: 0.0070s\n",
      "Epoch: 0120 loss_train: 0.6191 acc_train: 0.9071 loss_val: 0.8249 acc_val: 0.8267 time: 0.0070s\n",
      "Epoch: 0121 loss_train: 0.6062 acc_train: 0.8857 loss_val: 0.8201 acc_val: 0.8300 time: 0.0080s\n",
      "Epoch: 0122 loss_train: 0.5905 acc_train: 0.9071 loss_val: 0.8161 acc_val: 0.8300 time: 0.0070s\n",
      "Epoch: 0123 loss_train: 0.5830 acc_train: 0.9000 loss_val: 0.8120 acc_val: 0.8267 time: 0.0070s\n",
      "Epoch: 0124 loss_train: 0.5708 acc_train: 0.9000 loss_val: 0.8085 acc_val: 0.8233 time: 0.0070s\n",
      "Epoch: 0125 loss_train: 0.5259 acc_train: 0.9143 loss_val: 0.8054 acc_val: 0.8267 time: 0.0070s\n",
      "Epoch: 0126 loss_train: 0.5624 acc_train: 0.9214 loss_val: 0.8032 acc_val: 0.8233 time: 0.0070s\n",
      "Epoch: 0127 loss_train: 0.5101 acc_train: 0.9357 loss_val: 0.8005 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0128 loss_train: 0.5332 acc_train: 0.9286 loss_val: 0.7973 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0129 loss_train: 0.5946 acc_train: 0.8857 loss_val: 0.7933 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0130 loss_train: 0.5381 acc_train: 0.9143 loss_val: 0.7885 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0131 loss_train: 0.5078 acc_train: 0.9357 loss_val: 0.7835 acc_val: 0.8267 time: 0.0060s\n",
      "Epoch: 0132 loss_train: 0.5359 acc_train: 0.9214 loss_val: 0.7793 acc_val: 0.8300 time: 0.0080s\n",
      "Epoch: 0133 loss_train: 0.5372 acc_train: 0.9143 loss_val: 0.7758 acc_val: 0.8233 time: 0.0070s\n",
      "Epoch: 0134 loss_train: 0.5373 acc_train: 0.9071 loss_val: 0.7742 acc_val: 0.8267 time: 0.0070s\n",
      "Epoch: 0135 loss_train: 0.4999 acc_train: 0.9286 loss_val: 0.7735 acc_val: 0.8300 time: 0.0080s\n",
      "Epoch: 0136 loss_train: 0.5270 acc_train: 0.9357 loss_val: 0.7735 acc_val: 0.8300 time: 0.0070s\n",
      "Epoch: 0137 loss_train: 0.4973 acc_train: 0.9143 loss_val: 0.7730 acc_val: 0.8300 time: 0.0080s\n",
      "Epoch: 0138 loss_train: 0.4837 acc_train: 0.9357 loss_val: 0.7721 acc_val: 0.8267 time: 0.0070s\n",
      "Epoch: 0139 loss_train: 0.5387 acc_train: 0.9143 loss_val: 0.7700 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0140 loss_train: 0.4852 acc_train: 0.9357 loss_val: 0.7689 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0141 loss_train: 0.4812 acc_train: 0.9429 loss_val: 0.7673 acc_val: 0.8167 time: 0.0080s\n",
      "Epoch: 0142 loss_train: 0.5158 acc_train: 0.9357 loss_val: 0.7652 acc_val: 0.8100 time: 0.0070s\n",
      "Epoch: 0143 loss_train: 0.5143 acc_train: 0.8857 loss_val: 0.7625 acc_val: 0.8133 time: 0.0080s\n",
      "Epoch: 0144 loss_train: 0.4745 acc_train: 0.9429 loss_val: 0.7597 acc_val: 0.8167 time: 0.0090s\n",
      "Epoch: 0145 loss_train: 0.5085 acc_train: 0.9357 loss_val: 0.7564 acc_val: 0.8167 time: 0.0080s\n",
      "Epoch: 0146 loss_train: 0.4966 acc_train: 0.9214 loss_val: 0.7533 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0147 loss_train: 0.5260 acc_train: 0.9000 loss_val: 0.7512 acc_val: 0.8133 time: 0.0090s\n",
      "Epoch: 0148 loss_train: 0.5350 acc_train: 0.9000 loss_val: 0.7495 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0149 loss_train: 0.4489 acc_train: 0.9286 loss_val: 0.7475 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0150 loss_train: 0.4515 acc_train: 0.9286 loss_val: 0.7464 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0151 loss_train: 0.4856 acc_train: 0.9071 loss_val: 0.7448 acc_val: 0.8367 time: 0.0070s\n",
      "Epoch: 0152 loss_train: 0.4826 acc_train: 0.9429 loss_val: 0.7436 acc_val: 0.8367 time: 0.0080s\n",
      "Epoch: 0153 loss_train: 0.4610 acc_train: 0.9214 loss_val: 0.7421 acc_val: 0.8300 time: 0.0080s\n",
      "Epoch: 0154 loss_train: 0.4436 acc_train: 0.9500 loss_val: 0.7410 acc_val: 0.8267 time: 0.0080s\n",
      "Epoch: 0155 loss_train: 0.4626 acc_train: 0.9143 loss_val: 0.7415 acc_val: 0.8300 time: 0.0080s\n",
      "Epoch: 0156 loss_train: 0.4672 acc_train: 0.9214 loss_val: 0.7414 acc_val: 0.8267 time: 0.0070s\n",
      "Epoch: 0157 loss_train: 0.4737 acc_train: 0.9214 loss_val: 0.7378 acc_val: 0.8233 time: 0.0090s\n",
      "Epoch: 0158 loss_train: 0.5056 acc_train: 0.9214 loss_val: 0.7334 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0159 loss_train: 0.4299 acc_train: 0.9571 loss_val: 0.7295 acc_val: 0.8100 time: 0.0070s\n",
      "Epoch: 0160 loss_train: 0.4686 acc_train: 0.9500 loss_val: 0.7266 acc_val: 0.8167 time: 0.0080s\n",
      "Epoch: 0161 loss_train: 0.4113 acc_train: 0.9500 loss_val: 0.7241 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0162 loss_train: 0.4375 acc_train: 0.9357 loss_val: 0.7220 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0163 loss_train: 0.4275 acc_train: 0.9286 loss_val: 0.7206 acc_val: 0.8200 time: 0.0110s\n",
      "Epoch: 0164 loss_train: 0.4759 acc_train: 0.9214 loss_val: 0.7193 acc_val: 0.8267 time: 0.0120s\n",
      "Epoch: 0165 loss_train: 0.4105 acc_train: 0.9571 loss_val: 0.7192 acc_val: 0.8233 time: 0.0100s\n",
      "Epoch: 0166 loss_train: 0.4946 acc_train: 0.9357 loss_val: 0.7196 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0167 loss_train: 0.4535 acc_train: 0.9500 loss_val: 0.7201 acc_val: 0.8267 time: 0.0090s\n",
      "Epoch: 0168 loss_train: 0.4366 acc_train: 0.9571 loss_val: 0.7194 acc_val: 0.8233 time: 0.0100s\n",
      "Epoch: 0169 loss_train: 0.4578 acc_train: 0.9500 loss_val: 0.7178 acc_val: 0.8267 time: 0.0100s\n",
      "Epoch: 0170 loss_train: 0.4355 acc_train: 0.9571 loss_val: 0.7162 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0171 loss_train: 0.4058 acc_train: 0.9500 loss_val: 0.7141 acc_val: 0.8300 time: 0.0080s\n",
      "Epoch: 0172 loss_train: 0.4160 acc_train: 0.9571 loss_val: 0.7115 acc_val: 0.8300 time: 0.0070s\n",
      "Epoch: 0173 loss_train: 0.4482 acc_train: 0.9357 loss_val: 0.7085 acc_val: 0.8267 time: 0.0080s\n",
      "Epoch: 0174 loss_train: 0.4175 acc_train: 0.9714 loss_val: 0.7058 acc_val: 0.8300 time: 0.0060s\n",
      "Epoch: 0175 loss_train: 0.4019 acc_train: 0.9786 loss_val: 0.7034 acc_val: 0.8267 time: 0.0080s\n",
      "Epoch: 0176 loss_train: 0.4330 acc_train: 0.9500 loss_val: 0.7014 acc_val: 0.8300 time: 0.0070s\n",
      "Epoch: 0177 loss_train: 0.4070 acc_train: 0.9500 loss_val: 0.7001 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0178 loss_train: 0.4074 acc_train: 0.9429 loss_val: 0.6991 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0179 loss_train: 0.4114 acc_train: 0.9500 loss_val: 0.6981 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0180 loss_train: 0.4021 acc_train: 0.9357 loss_val: 0.6970 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0181 loss_train: 0.4377 acc_train: 0.9286 loss_val: 0.6957 acc_val: 0.8267 time: 0.0080s\n",
      "Epoch: 0182 loss_train: 0.4283 acc_train: 0.9214 loss_val: 0.6951 acc_val: 0.8333 time: 0.0090s\n",
      "Epoch: 0183 loss_train: 0.3788 acc_train: 0.9500 loss_val: 0.6956 acc_val: 0.8367 time: 0.0070s\n",
      "Epoch: 0184 loss_train: 0.4443 acc_train: 0.9214 loss_val: 0.6964 acc_val: 0.8367 time: 0.0080s\n",
      "Epoch: 0185 loss_train: 0.4073 acc_train: 0.9286 loss_val: 0.6978 acc_val: 0.8333 time: 0.0080s\n",
      "Epoch: 0186 loss_train: 0.3926 acc_train: 0.9571 loss_val: 0.6977 acc_val: 0.8300 time: 0.0070s\n",
      "Epoch: 0187 loss_train: 0.3950 acc_train: 0.9571 loss_val: 0.6965 acc_val: 0.8267 time: 0.0080s\n",
      "Epoch: 0188 loss_train: 0.3805 acc_train: 0.9500 loss_val: 0.6942 acc_val: 0.8233 time: 0.0070s\n",
      "Epoch: 0189 loss_train: 0.4136 acc_train: 0.9714 loss_val: 0.6921 acc_val: 0.8233 time: 0.0079s\n",
      "Epoch: 0190 loss_train: 0.3853 acc_train: 0.9500 loss_val: 0.6905 acc_val: 0.8200 time: 0.0070s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0191 loss_train: 0.4229 acc_train: 0.9500 loss_val: 0.6895 acc_val: 0.8100 time: 0.0100s\n",
      "Epoch: 0192 loss_train: 0.3951 acc_train: 0.9429 loss_val: 0.6880 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0193 loss_train: 0.4056 acc_train: 0.9429 loss_val: 0.6864 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0194 loss_train: 0.4048 acc_train: 0.9500 loss_val: 0.6854 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0195 loss_train: 0.3778 acc_train: 0.9500 loss_val: 0.6851 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0196 loss_train: 0.4353 acc_train: 0.9571 loss_val: 0.6862 acc_val: 0.8300 time: 0.0070s\n",
      "Epoch: 0197 loss_train: 0.3880 acc_train: 0.9357 loss_val: 0.6881 acc_val: 0.8267 time: 0.0090s\n",
      "Epoch: 0198 loss_train: 0.4111 acc_train: 0.9214 loss_val: 0.6881 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0199 loss_train: 0.4121 acc_train: 0.9357 loss_val: 0.6882 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0200 loss_train: 0.3764 acc_train: 0.9500 loss_val: 0.6881 acc_val: 0.8200 time: 0.0070s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 1.6785s\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98fc41e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.7164 accuracy= 0.8360\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfcca19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
