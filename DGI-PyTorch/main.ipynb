{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "907c7959-33a3-4547-9cf2-9fb67ad36d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model is currently unusable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72f55cbb-676d-4c64-9653-64a1ce92f350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import argparse\n",
    "import scipy.sparse as sp\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "082c6237-fbcc-4aad-a437-e46e0fb8fc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, n_h):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.f_k = nn.Bilinear(n_h, n_h, 1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            self.weights_init(m)\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        if isinstance(m, nn.Bilinear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, c, h_pl, h_mi, s_bias1=None, s_bias2=None):\n",
    "        c_x = torch.unsqueeze(c, 1)\n",
    "        c_x = c_x.expand_as(h_pl)\n",
    "\n",
    "        sc_1 = torch.squeeze(self.f_k(h_pl, c_x), 2)\n",
    "        sc_2 = torch.squeeze(self.f_k(h_mi, c_x), 2)\n",
    "\n",
    "        if s_bias1 is not None:\n",
    "            sc_1 += s_bias1\n",
    "        if s_bias2 is not None:\n",
    "            sc_2 += s_bias2\n",
    "\n",
    "        logits = torch.cat((sc_1, sc_2), 1)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "715f89d8-e49e-435c-99ed-400b167799ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_ft, out_ft, act, bias=True):\n",
    "        super(GCN, self).__init__()\n",
    "        self.fc = nn.Linear(in_ft, out_ft, bias=False)\n",
    "        self.act = nn.PReLU() if act == 'prelu' else act\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_ft))\n",
    "            self.bias.data.fill_(0.0)\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        for m in self.modules():\n",
    "            self.weights_init(m)\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(0.0)\n",
    "\n",
    "    # Shape of seq: (batch, nodes, features)\n",
    "    def forward(self, seq, adj, sparse=False):\n",
    "        seq_fts = self.fc(seq)\n",
    "        if sparse:\n",
    "            out = torch.unsqueeze(torch.spmm(adj, torch.squeeze(seq_fts, 0)), 0)\n",
    "        else:\n",
    "            out = torch.bmm(adj, seq_fts)\n",
    "        if self.bias is not None:\n",
    "            out += self.bias\n",
    "        \n",
    "        return self.act(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca6038b6-d900-47f9-8b6c-3880877c74f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgReadout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AvgReadout, self).__init__()\n",
    "\n",
    "    def forward(self, seq, msk):\n",
    "        if msk is None:\n",
    "            return torch.mean(seq, 1)\n",
    "        else:\n",
    "            msk = torch.unsqueeze(msk, -1)\n",
    "            return torch.sum(seq * msk, 1) / torch.sum(msk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1e4d4c6-64a6-4112-b5bb-ad6c26e094d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg(nn.Module):\n",
    "    def __init__(self, ft_in, nb_classes):\n",
    "        super(LogReg, self).__init__()\n",
    "        self.fc = nn.Linear(ft_in, nb_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            self.weights_init(m)\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        ret = self.fc(seq)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a95c2f6-12d6-49e8-8edb-55f567a48e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGI(nn.Module):\n",
    "    def __init__(self, n_in, n_h, activation):\n",
    "        super(DGI, self).__init__()\n",
    "        self.gcn = GCN(n_in, n_h, activation)\n",
    "        self.read = AvgReadout()\n",
    "\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "        self.disc = Discriminator(n_h)\n",
    "\n",
    "    def forward(self, seq1, seq2, adj, sparse, msk, samp_bias1, samp_bias2):\n",
    "        h_1 = self.gcn(seq1, adj, sparse)\n",
    "\n",
    "        c = self.read(h_1, msk)\n",
    "        c = self.sigm(c)\n",
    "\n",
    "        h_2 = self.gcn(seq2, adj, sparse)\n",
    "\n",
    "        ret = self.disc(c, h_1, h_2, samp_bias1, samp_bias2)\n",
    "\n",
    "        return ret\n",
    "\n",
    "    # Detach the return variables\n",
    "    def embed(self, seq, adj, sparse, msk):\n",
    "        h_1 = self.gcn(seq, adj, sparse)\n",
    "        c = self.read(h_1, msk)\n",
    "\n",
    "        return h_1.detach(), c.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "643237f9-e528-463f-a289-316420808529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    }
   ],
   "source": [
    "dataset = 'cora'\n",
    "\n",
    "# training params\n",
    "batch_size = 1\n",
    "nb_epochs = 10000\n",
    "patience = 20\n",
    "lr = 0.001\n",
    "l2_coef = 0.0\n",
    "drop_prob = 0.0\n",
    "hid_units = 512\n",
    "sparse = True\n",
    "nonlinearity = 'prelu' # special name to separate parameters\n",
    "\n",
    "adj, features, labels, idx_train, idx_val, idx_test = process.load_data(dataset)\n",
    "features, _ = process.preprocess_features(features)\n",
    "\n",
    "nb_nodes = features.shape[0]\n",
    "ft_size = features.shape[1]\n",
    "nb_classes = labels.shape[1]\n",
    "\n",
    "adj = process.normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "if sparse:\n",
    "    sp_adj = process.sparse_mx_to_torch_sparse_tensor(adj)\n",
    "else:\n",
    "    adj = (adj + sp.eye(adj.shape[0])).todense()\n",
    "\n",
    "features = torch.FloatTensor(features[np.newaxis])\n",
    "if not sparse:\n",
    "    adj = torch.FloatTensor(adj[np.newaxis])\n",
    "labels = torch.FloatTensor(labels[np.newaxis])\n",
    "idx_train = torch.LongTensor(idx_train)\n",
    "idx_val = torch.LongTensor(idx_val)\n",
    "idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "model = DGI(ft_size, hid_units, nonlinearity)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_coef)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('Using CUDA')\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    if sparse:\n",
    "        sp_adj = sp_adj.cuda()\n",
    "    else:\n",
    "        adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "\n",
    "b_xent = nn.BCEWithLogitsLoss()\n",
    "xent = nn.CrossEntropyLoss()\n",
    "cnt_wait = 0\n",
    "best = 1e9\n",
    "best_t = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36812e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.6932, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.6920, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.6871, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.6857, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.6803, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.6764, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.6710, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.6644, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.6578, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.6478, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.6427, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.6318, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.6249, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.6087, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.6003, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.5847, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.5756, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.5587, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.5497, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.5384, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.5212, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.5138, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.4898, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.4783, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.4661, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.4501, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.4361, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.4264, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.4124, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.3964, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.3849, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.3721, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.3675, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.3503, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.3382, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.3312, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.3250, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.3053, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.3099, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.2918, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.2921, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.2756, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.2652, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.2646, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.2534, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.2536, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.2474, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.2361, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.2268, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.2365, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.2249, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.2159, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.2126, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.2164, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.2212, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1986, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.2004, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1994, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1806, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1908, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1879, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1869, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1941, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1818, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1961, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1684, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1745, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1758, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1757, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1665, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1712, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1665, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1523, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1542, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1535, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1423, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1494, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1479, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1537, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1443, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1430, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1478, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1305, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1415, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1333, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1389, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1264, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1330, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1293, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1315, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1272, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1241, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1198, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1261, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1235, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1378, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1233, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1249, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1177, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1351, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1195, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1160, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1087, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1172, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1084, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1122, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1048, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1054, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1040, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1110, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1093, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1058, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0987, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1038, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1066, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1084, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1016, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1080, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0968, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0909, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1032, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0988, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.1023, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0978, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0930, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0928, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0865, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0951, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0893, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0903, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0981, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0899, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0821, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0888, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0922, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0899, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0745, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0920, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0991, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0760, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0864, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0831, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0982, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0807, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0927, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0898, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0866, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0805, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0850, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0898, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0839, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0965, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0763, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0827, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0739, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0785, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0755, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0852, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0852, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0692, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0795, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0894, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0707, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0636, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0836, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0778, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0758, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0657, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0795, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0779, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0740, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0845, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0750, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0706, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0685, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0664, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0776, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0705, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0736, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0724, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0683, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0638, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0674, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0629, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0681, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0619, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0750, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0658, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0596, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0790, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0732, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0744, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0670, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0710, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0742, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0624, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0718, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0664, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0747, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0587, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0715, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0619, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0658, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0593, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0692, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0738, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0551, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0572, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0627, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0704, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0601, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0519, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0523, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0633, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0569, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0682, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0562, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0583, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0545, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0535, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0581, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0515, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0506, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0566, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0535, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0548, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0522, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0691, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0492, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0538, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0559, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0539, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0536, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0570, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0559, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0482, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0498, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0580, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0591, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0554, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0488, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0562, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0542, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0576, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0510, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0477, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0484, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0540, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0541, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0470, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0448, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0625, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0523, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0528, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0495, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0468, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0441, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0545, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0461, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0563, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0558, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0500, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0402, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0487, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0462, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0510, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0521, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0486, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0414, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0575, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0475, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0469, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0447, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0511, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0429, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0431, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0432, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0415, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0488, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0382, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0491, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0422, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0422, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0470, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0429, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0429, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0454, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0490, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0425, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0413, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0437, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0386, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0446, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0343, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0432, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0380, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0401, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0439, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0495, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0454, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0469, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0446, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0441, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0432, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0393, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0406, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0454, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0449, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0359, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0375, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0348, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0398, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0383, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Loss: tensor(0.0432, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Early stopping!\n",
      "Loading 293th epoch\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(nb_epochs):\n",
    "    model.train()\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    idx = np.random.permutation(nb_nodes)\n",
    "    shuf_fts = features[:, idx, :]\n",
    "\n",
    "    lbl_1 = torch.ones(batch_size, nb_nodes)\n",
    "    lbl_2 = torch.zeros(batch_size, nb_nodes)\n",
    "    lbl = torch.cat((lbl_1, lbl_2), 1)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        shuf_fts = shuf_fts.cuda()\n",
    "        lbl = lbl.cuda()\n",
    "    \n",
    "    logits = model(features, shuf_fts, sp_adj if sparse else adj, sparse, None, None, None) \n",
    "\n",
    "    loss = b_xent(logits, lbl)\n",
    "\n",
    "    print('Loss:', loss)\n",
    "\n",
    "    if loss < best:\n",
    "        best = loss\n",
    "        best_t = epoch\n",
    "        cnt_wait = 0\n",
    "        torch.save(model.state_dict(), 'best_dgi.pkl')\n",
    "    else:\n",
    "        cnt_wait += 1\n",
    "\n",
    "    if cnt_wait == patience:\n",
    "        print('Early stopping!')\n",
    "        break\n",
    "\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "print('Loading {}th epoch'.format(best_t))\n",
    "model.load_state_dict(torch.load('best_dgi.pkl'))\n",
    "\n",
    "embeds, _ = model.embed(features, sp_adj if sparse else adj, sparse, None)\n",
    "train_embs = embeds[0, idx_train]\n",
    "val_embs = embeds[0, idx_val]\n",
    "test_embs = embeds[0, idx_test]\n",
    "\n",
    "train_lbls = torch.argmax(labels[0, idx_train], dim=1)\n",
    "val_lbls = torch.argmax(labels[0, idx_val], dim=1)\n",
    "test_lbls = torch.argmax(labels[0, idx_test], dim=1)\n",
    "\n",
    "tot = torch.zeros(1)\n",
    "tot = tot.cuda()\n",
    "\n",
    "accs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a6c3c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8140, device='cuda:0')\n",
      "tensor(0.8170, device='cuda:0')\n",
      "tensor(0.8140, device='cuda:0')\n",
      "tensor(0.8130, device='cuda:0')\n",
      "tensor(0.8160, device='cuda:0')\n",
      "tensor(0.8150, device='cuda:0')\n",
      "tensor(0.8150, device='cuda:0')\n",
      "tensor(0.8130, device='cuda:0')\n",
      "tensor(0.8140, device='cuda:0')\n",
      "tensor(0.8150, device='cuda:0')\n",
      "tensor(0.8140, device='cuda:0')\n",
      "tensor(0.8140, device='cuda:0')\n",
      "tensor(0.8130, device='cuda:0')\n",
      "tensor(0.8160, device='cuda:0')\n",
      "tensor(0.8160, device='cuda:0')\n",
      "tensor(0.8130, device='cuda:0')\n",
      "tensor(0.8150, device='cuda:0')\n",
      "tensor(0.8140, device='cuda:0')\n",
      "tensor(0.8110, device='cuda:0')\n",
      "tensor(0.8160, device='cuda:0')\n",
      "tensor(0.8140, device='cuda:0')\n",
      "tensor(0.8120, device='cuda:0')\n",
      "tensor(0.8130, device='cuda:0')\n",
      "tensor(0.8160, device='cuda:0')\n",
      "tensor(0.8150, device='cuda:0')\n",
      "tensor(0.8130, device='cuda:0')\n",
      "tensor(0.8160, device='cuda:0')\n",
      "tensor(0.8130, device='cuda:0')\n",
      "tensor(0.8140, device='cuda:0')\n",
      "tensor(0.8140, device='cuda:0')\n",
      "tensor(0.8160, device='cuda:0')\n",
      "tensor(0.8140, device='cuda:0')\n",
      "tensor(0.8140, device='cuda:0')\n",
      "tensor(0.8120, device='cuda:0')\n",
      "tensor(0.8150, device='cuda:0')\n",
      "tensor(0.8130, device='cuda:0')\n",
      "tensor(0.8160, device='cuda:0')\n",
      "tensor(0.8150, device='cuda:0')\n",
      "tensor(0.8160, device='cuda:0')\n",
      "tensor(0.8150, device='cuda:0')\n",
      "tensor(0.8160, device='cuda:0')\n",
      "tensor(0.8150, device='cuda:0')\n",
      "tensor(0.8150, device='cuda:0')\n",
      "tensor(0.8130, device='cuda:0')\n",
      "tensor(0.8140, device='cuda:0')\n",
      "tensor(0.8130, device='cuda:0')\n",
      "tensor(0.8150, device='cuda:0')\n",
      "tensor(0.8140, device='cuda:0')\n",
      "tensor(0.8120, device='cuda:0')\n",
      "tensor(0.8180, device='cuda:0')\n",
      "Average accuracy: tensor([0.8144], device='cuda:0')\n",
      "tensor(81.4380, device='cuda:0')\n",
      "tensor(0.1427, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for _ in range(50):\n",
    "    log = LogReg(hid_units, nb_classes)\n",
    "    opt = torch.optim.Adam(log.parameters(), lr=0.01, weight_decay=0.0)\n",
    "    log.cuda()\n",
    "\n",
    "    pat_steps = 0\n",
    "    best_acc = torch.zeros(1)\n",
    "    best_acc = best_acc.cuda()\n",
    "    for _ in range(100):\n",
    "        log.train()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        logits = log(train_embs)\n",
    "        loss = xent(logits, train_lbls)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    logits = log(test_embs)\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    acc = torch.sum(preds == test_lbls).float() / test_lbls.shape[0]\n",
    "    accs.append(acc * 100)\n",
    "    print(acc)\n",
    "    tot += acc\n",
    "\n",
    "print('Average accuracy:', tot / 50)\n",
    "\n",
    "accs = torch.stack(accs)\n",
    "print(accs.mean())\n",
    "print(accs.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85438ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d5fd77f783dbdc8f4bffff806f3f9a50f5e2c598183462bcffdfc1ca39dade52"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('research': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
